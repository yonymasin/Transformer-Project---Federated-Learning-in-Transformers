PostId,PostCreationDate,OwnerUserId,OwnerCreationDate,ReputationAtPostCreation,OwnerUndeletedAnswerCountAtPostTime,Title,BodyMarkdown,Tag1,Tag2,Tag3,Tag4,Tag5,PostClosedDate,OpenStatus,OpenStatusInt,BodyLength,TitleLength,TitleConcatWithBody,NumberOfTags
11712353,07/29/2012 19:41:17,170352,09/08/2009 18:23:21,6703,189,Downloading Live Olympic Medal Data into R,"It looks like the website is blocking direct access from Curl. 

    library(XML) 
    library(RCurl) 
    theurl <- ""http://www.london2012.com/medals/medal-count/""
    page <- getURL(theurl)
    
    page # fail
    [1] ""<HTML><HEAD>\n<TITLE>Access Denied</TITLE>\n</HEAD><BODY>\n<H1>Access Denied</H1>\n \nYou don't have permission to access \""http&#58;&#47;&#47;www&#46;london2012&#46;com&#47;medals&#47;medal&#45;count&#47;\"" on this server.<P>\nReference&#32;&#35;18&#46;358a503f&#46;1343590091&#46;c056ae2\n</BODY>\n</HTML>\n""

Let's try to see if we can access it directly from the Table. 

    page <- readHTMLTable(theurl)

No luck there `Error in htmlParse(doc) : error in creating parser for http://www.london2012.com/medals/medal-count/`

How would you go about getting this table into R? ",r,,,,,,open,0,102,7,"Downloading Live Olympic Medal Data into R It looks like the website is blocking direct access from Curl. 

    library(XML) 
    library(RCurl) 
    theurl <- ""http://www.london2012.com/medals/medal-count/""
    page <- getURL(theurl)
    
    page # fail
    [1] ""<HTML><HEAD>\n<TITLE>Access Denied</TITLE>\n</HEAD><BODY>\n<H1>Access Denied</H1>\n \nYou don't have permission to access \""http&#58;&#47;&#47;www&#46;london2012&#46;com&#47;medals&#47;medal&#45;count&#47;\"" on this server.<P>\nReference&#32;&#35;18&#46;358a503f&#46;1343590091&#46;c056ae2\n</BODY>\n</HTML>\n""

Let's try to see if we can access it directly from the Table. 

    page <- readHTMLTable(theurl)

No luck there `Error in htmlParse(doc) : error in creating parser for http://www.london2012.com/medals/medal-count/`

How would you go about getting this table into R? ",1
4454435,12/15/2010 20:15:40,543835,12/15/2010 19:50:02,1,0,Calculating Start for stl(),"I'm reading weekly data from a .csv data file.   A sample of the data is:    

    Date,Demand    
    ""Feb 08, 1991"",6621    
    ""Feb 15, 1991"",6433    
    ""Feb 22, 1991"",6582   
    ""Mar 01, 1991"",7224   
    ""Mar 08, 1991"",6875   
    ""Mar 15, 1991"",6947   
    ""Mar 22, 1991"",7328   
    ""Mar 29, 1991"",6777   
    ""Apr 05, 1991"",7503
    .....  
   

My code is:

    > temp<-read.table(file=""E:\\Data\\Demand_00.csv"",header=TRUE, sep="","")
    > stadat<-strptime(as.character(temp[,1]),""%b %d, %Y"")[1]
    > statim<-as.numeric(strftime(stadat,""%Y""))+(as.numeric(strftime(stadat,""%j""))/366)
    > temdat<-ts(temp[,2],start=statim,frequency=52)
    > plot(temp2<- stl(log(temdat), ""per""))

My question is:  Is there a better/cleaner way to build statim (the start required in the above ts object)?   Notice that this is weekly data that may or may not start at the first week of the year.   


Thanks,    
Bill",r,time-series,,,,,open,0,197,4,"Calculating Start for stl() I'm reading weekly data from a .csv data file.   A sample of the data is:    

    Date,Demand    
    ""Feb 08, 1991"",6621    
    ""Feb 15, 1991"",6433    
    ""Feb 22, 1991"",6582   
    ""Mar 01, 1991"",7224   
    ""Mar 08, 1991"",6875   
    ""Mar 15, 1991"",6947   
    ""Mar 22, 1991"",7328   
    ""Mar 29, 1991"",6777   
    ""Apr 05, 1991"",7503
    .....  
   

My code is:

    > temp<-read.table(file=""E:\\Data\\Demand_00.csv"",header=TRUE, sep="","")
    > stadat<-strptime(as.character(temp[,1]),""%b %d, %Y"")[1]
    > statim<-as.numeric(strftime(stadat,""%Y""))+(as.numeric(strftime(stadat,""%j""))/366)
    > temdat<-ts(temp[,2],start=statim,frequency=52)
    > plot(temp2<- stl(log(temdat), ""per""))

My question is:  Is there a better/cleaner way to build statim (the start required in the above ts object)?   Notice that this is weekly data that may or may not start at the first week of the year.   


Thanks,    
Bill",2
9801658,03/21/2012 09:22:35,1282955,03/21/2012 09:19:57,1,0,R - delete a line drawn by abline,"How to delete a line drawn by function abline without changing the color to match background? 
Btw, I am using chart_Series for charting. It would be great to have xy lines (1 vertical and 1 horizontal) following the mouse movement. 
Pls help
",r,,,,,03/21/2012 22:53:02,not a real question,1,42,8,"R - delete a line drawn by abline How to delete a line drawn by function abline without changing the color to match background? 
Btw, I am using chart_Series for charting. It would be great to have xy lines (1 vertical and 1 horizontal) following the mouse movement. 
Pls help
",1
11370001,07/06/2012 21:39:51,1373633,05/03/2012 21:23:44,12,1,Evaluate/Compare multiple numerical dataset for consensus or difference in R,"I have a couple of stacked histograms which I need to compare/evaluate for similarity or difference.![Stacked histogram generated from one dataset][1]


  [1]: http://i.stack.imgur.com/JK2GG.png

I believe rather than evaluating histograms is will be east to work with dataset used to plot these stacked histograms, which is in format: 

RED                PURPLE       BLUE        GREY       YELLOW

22.0640569395	16.9483985765	0	60.987544484	0

8.1850533808	8.8523131673	0	82.962633452	0

6.8505338078	6.8950177936	0.756227758	85.4982206406	0.5338078292

6.7615658363	5.2491103203	1.6459074733	86.3434163701	0.6672597865

5.8274021352	7.384341637	2.1352313167	84.653024911	1.1565836299

7.8736654804	6.628113879	1.5569395018	83.9412811388	1.2010676157

7.1619217082	8.1850533808	1.2455516014	83.4074733096	1.3790035587

5.5604982206	10.2758007117	1.0676156584	83.0960854093	1.0231316726

7.1174377224	7.6067615658	0.7117437722	84.5640569395	0.756227758

7.8736654804	3.9590747331	0.6672597865	87.5	0.3113879004

7.6512455516	7.8736654804	0.5338078292	83.9412811388	0.5338078292

7.6067615658	8.9857651246	1.4679715302	81.9395017794	0.3558718861

8.9412811388	8.0071174377	1.3790035587	81.6725978648	0.5782918149

19.0836298932	9.2081850534	2.1352313167	69.5729537367	1.3790035587

14.9911032028	11.0765124555	3.2028469751	70.7295373665	1.0676156584

15.3914590747	10.8985765125	3.024911032	70.6850533808	1.2900355872

17.4822064057	12.5444839858	2.4911032028	67.4822064057	1.334519573

15.8362989324	13.0338078292	2.0017793594	69.128113879	1.334519573

17.037366548	10.4537366548	2.4021352313	70.1067615658	1.2010676157

20.2846975089	10.0088967972	0	69.706405694	1.0676156584

28.7366548043	12.6334519573	0	58.6298932384	0


Is there any possible way I can compare such dataset from multiple experiments (n=8) and visually show (plot) that these datasets are in consensus or differ from each other? Can QQplot be used to evaluate such datasets. 

Awaiting reply, 

Atul


",r,dataset,plot,evaluate,consensus,07/10/2012 14:13:51,off topic,1,125,10,"Evaluate/Compare multiple numerical dataset for consensus or difference in R I have a couple of stacked histograms which I need to compare/evaluate for similarity or difference.![Stacked histogram generated from one dataset][1]


  [1]: http://i.stack.imgur.com/JK2GG.png

I believe rather than evaluating histograms is will be east to work with dataset used to plot these stacked histograms, which is in format: 

RED                PURPLE       BLUE        GREY       YELLOW

22.0640569395	16.9483985765	0	60.987544484	0

8.1850533808	8.8523131673	0	82.962633452	0

6.8505338078	6.8950177936	0.756227758	85.4982206406	0.5338078292

6.7615658363	5.2491103203	1.6459074733	86.3434163701	0.6672597865

5.8274021352	7.384341637	2.1352313167	84.653024911	1.1565836299

7.8736654804	6.628113879	1.5569395018	83.9412811388	1.2010676157

7.1619217082	8.1850533808	1.2455516014	83.4074733096	1.3790035587

5.5604982206	10.2758007117	1.0676156584	83.0960854093	1.0231316726

7.1174377224	7.6067615658	0.7117437722	84.5640569395	0.756227758

7.8736654804	3.9590747331	0.6672597865	87.5	0.3113879004

7.6512455516	7.8736654804	0.5338078292	83.9412811388	0.5338078292

7.6067615658	8.9857651246	1.4679715302	81.9395017794	0.3558718861

8.9412811388	8.0071174377	1.3790035587	81.6725978648	0.5782918149

19.0836298932	9.2081850534	2.1352313167	69.5729537367	1.3790035587

14.9911032028	11.0765124555	3.2028469751	70.7295373665	1.0676156584

15.3914590747	10.8985765125	3.024911032	70.6850533808	1.2900355872

17.4822064057	12.5444839858	2.4911032028	67.4822064057	1.334519573

15.8362989324	13.0338078292	2.0017793594	69.128113879	1.334519573

17.037366548	10.4537366548	2.4021352313	70.1067615658	1.2010676157

20.2846975089	10.0088967972	0	69.706405694	1.0676156584

28.7366548043	12.6334519573	0	58.6298932384	0


Is there any possible way I can compare such dataset from multiple experiments (n=8) and visually show (plot) that these datasets are in consensus or differ from each other? Can QQplot be used to evaluate such datasets. 

Awaiting reply, 

Atul


",5
10999725,06/12/2012 15:20:34,1403232,05/18/2012 11:26:39,21,0,Fitting a harmonic function into a time series in R,"I got a time series that shows various cyclical components.
After looking at the residuals of the linear regression, there
is a month trend and a week trend.
I would like to fit first a harmonic functions for the monthly cyle
and then for the residuals another harmonic trend.

I really appreciate your ideas about which function to use for estimating the right parameters!

Thanks! Best, Fabian


P.S. I tried to use the months as factors for a multivariate regression. However,
this will produce strange forecast vales. So that's the reason why I am now looking for a harmonic function.

",r,cycle,fit,sin,cos,07/04/2012 16:28:44,off topic,1,92,10,"Fitting a harmonic function into a time series in R I got a time series that shows various cyclical components.
After looking at the residuals of the linear regression, there
is a month trend and a week trend.
I would like to fit first a harmonic functions for the monthly cyle
and then for the residuals another harmonic trend.

I really appreciate your ideas about which function to use for estimating the right parameters!

Thanks! Best, Fabian


P.S. I tried to use the months as factors for a multivariate regression. However,
this will produce strange forecast vales. So that's the reason why I am now looking for a harmonic function.

",5
7212490,08/27/2011 04:09:25,669551,03/21/2011 13:54:09,31,0,Extracting values in table,"I know this seems to be over dependent on SO, but i am a beginner and worked on this for a couple of days but was not successful. I am able to do it for an individual table, but when it comes to list the code doesn`t work. I hope everyone understands the situation.

1. we have a list of elements in a list say: 

ls<-list(""N"",""E"",""E"",""N"",""P"",""E"",""M"",""Q"",""E"",""M"") 

2.We have an another list of tables in a list say: 

n <- list(""M"", ""N"",""E"",""P"",""Q"",""M"",""N"",""E"",""Q"",""N"") 

tb <- lapply(1:10, function(i)matrix(sample(4), 2, 2, dimnames=list(n[sample(10,2)], n[sample(2,2)])))
 
3.we need to extract values from the table in the list where colname is always ""M"" , wherein the rowname should be the 1st element in the list ls for table 1 in the list tb and 2nd element in table 2 and so on...
 
for ex:   
 M N 
 
N 4 1

P 3 2 

In table 1 , we need to extract value 4. 


Thanks to all in advance. 
",r,,,,,08/28/2011 10:01:33,too localized,1,162,4,"Extracting values in table I know this seems to be over dependent on SO, but i am a beginner and worked on this for a couple of days but was not successful. I am able to do it for an individual table, but when it comes to list the code doesn`t work. I hope everyone understands the situation.

1. we have a list of elements in a list say: 

ls<-list(""N"",""E"",""E"",""N"",""P"",""E"",""M"",""Q"",""E"",""M"") 

2.We have an another list of tables in a list say: 

n <- list(""M"", ""N"",""E"",""P"",""Q"",""M"",""N"",""E"",""Q"",""N"") 

tb <- lapply(1:10, function(i)matrix(sample(4), 2, 2, dimnames=list(n[sample(10,2)], n[sample(2,2)])))
 
3.we need to extract values from the table in the list where colname is always ""M"" , wherein the rowname should be the 1st element in the list ls for table 1 in the list tb and 2nd element in table 2 and so on...
 
for ex:   
 M N 
 
N 4 1

P 3 2 

In table 1 , we need to extract value 4. 


Thanks to all in advance. 
",1
7728462,10/11/2011 15:23:13,989691,10/11/2011 14:08:18,1,0,Identify records in data frame A not contained in data frame B,"This is my first time posting here, so please be kind ;-)

I was wondering what would be the most efficient way to identify records in one data frame that `(x.1)` are not contained in a second data frame `(x.2)`. Kind of like the ""inverse"" of 

    merge(x.1, x.2, by.x=TRUE)

I thought that if `merge()` does the merging job, it must also be able to do the ""reverse/inverse"" out-of-the-box, which seems not to be true (maybe I am missing out on something, though) So I've looked for a while and below are the solutions that I found (a bit lengthy, but maybe a reference for other people).

Yet I still wonder if there's nothing more ""built-in"" ;-)

Thanks for any advice!


----------


PROBLEM
-----------

Identify records in `x.1` that are *not* contained in `x.2` (both 'data.frame') based on *all* attributes available (i.e. columns).

**DATA**
--------

    > x.1 <- data.frame(a=c(1,2,3,4,5), b=c(1,2,3,4,5))
    > x.1
      a b
    1 1 1
    2 2 2
    3 3 3
    4 4 4
    5 5 5

    > x.2 <- data.frame(a=c(1,1,2,3,4), b=c(1,1,99,3,4))
    > x.2
      a  b
    1 1  1
    2 1  1
    3 2 99
    4 3  3
    5 4  4

**BENCHMARK SETTINGS**
--------

    > require(microbenchmark)
    > to.sec <- 1000000000

**SOLUTION 1**
--------
by Chase

    > subset(x.1, !(a %in% x.2$a))
      a b
    5 5 5

    ### DISCUSSION ###
    
    # 1) Effectiveness
    # Does not match x.1 = (2,2) as only column 'a' is used. Yet, extendable to 
    # multiple column check, but this would get pretty verbose:
    > subset(x.1, !(a %in% x.2$a) | !(b %in% x.2$b))
      a b
    2 2 2
    5 5 5
    
    # 2) Efficiency
    > sol.1 <- microbenchmark(subset(x.1, !(a %in% x.2$a)))
    > sol.1 <- median(sol.1$time)/to.sec
    > sol.1
    [1] 0.000214137

**SOLUTION 2** 
--------
by Ramnath

    > setdiff(x.1$a, x.2$a)   # elements in x.1$a NOT in x.2$a
    [1] 5
    > setdiff(x.2$a, x.1$a)   # elements in x.2$a NOT in x.1$a
    
    ### DISCUSSION ###
    
    # 1) Effectiveness
    # Not easily extendable to multiple column check.
    
    # 2) Efficiency
    > sol.2 <- microbenchmark(setdiff(x.1$a, x.2$a))
    > sol.2 <- median(sol.2$time)/to.sec
    > sol.2
    [1] 3.59595e-05

**SOLUTION 3** 
--------
by Brian Ripley

    > fun.3 <- function(x.1, x.2, ...){
    +    x.1.id <- do.call(""paste"", c(x.1, sep = ""\r""))
    +    x.2.id <- do.call(""paste"", c(x.2, sep = ""\r""))
    +    x.1[match(setdiff(x.1.id, x.2.id),x.1.id), ]
    + }
    > fun.3(x.1,x.2)
      a b
    2 2 2
    5 5 5
    
    ### DISCUSSION ###
    
    # 1) Effectiveness
    # Matches desired records
    
    # 2) Efficiency
    > sol.3 <- microbenchmark(fun.3(x.1,x.2))
    > sol.3 <- median(sol.3$time)/to.sec
    > sol.3
    [1] 0.000244114

**SOLUTION 4**
--------
by myself

    > fun.4 <- function(x.1, x.2, ...){
    +     # Combine
    +     df <- rbind(x.1,x.2)
    +     df <- df[order(df[,1]),]
    +     # Find duplicates
    +     idx.1 <- duplicated(df, all=TRUE)
    +     idx.2 <- duplicated(df, fromLast=TRUE)
    +     idx <- cbind(idx.1, idx.2)
    +     idx <- apply(idx, MARGIN=1, any)
    +     # Index records
    +     df[-which(idx),]
    + }
    > fun.4(x.1,x.2)
      a  b
    2 2  2
    8 2 99
    5 5  5
    
    ### DISCUSSION ###
    
    # 1) Effectiveness
    # Matches record x.2 = (2,99) which was not contained in x.1 as duplicate 
    # check is performed on combined df and result is not verified against x.1

    # 2) Efficiency
    > sol.4 <- microbenchmark(fun.4(x.1,x.2))
    > sol.4 <- median(sol.4$time)/to.sec
    > sol.4
    > [1] 0.001057671

**SOLUTION 5**
--------
by Gabor Grothendiek

    > library(sqldf)
    > fun.5 <- function(x1,x2,...){
    +     out <- sqldf(
    +         ""SELECT * FROM x1 
    +             WHERE 
    +                 x1.a NOT IN (SELECT x2.a FROM x2) OR 
    +                 x1.b NOT IN (SELECT x2.b FROM x2)""
    +     )
    +     out
    + }
    > fun.5(x1=x.1,x2=x.2)
      a b
    1 2 2
    2 5 5
     
    ### DISCUSSION ###
     
    # 1) Effectiveness
    # Matches desired records, but will get equally verbose as solution 1 when trying
    # to extend matching criteria.

    # 2) Efficiency
    > sol.5 <- microbenchmark(fun.5(x1=x.1,x2=x.2))
    > sol.5 <- median(sol.5$time)/to.sec
    > sol.5
    > [1] 0.02260869

**SOLUTION 6**
--------
by Tal Galili

    > fun.6 <- function(x.1,x.2){
    +     x.1.vec <- apply(x.1, 1, paste, collapse = """")
    +     x.2.vec <- apply(x.2, 1, paste, collapse = """")
    +     x.1.without.x.2.rows <- x.1[!x.1.vec %in% x.2.vec,]
    +     return(x.1.without.x.2.rows)
    + }
    > fun.6(x.1,x.2)
      a b
    2 2 2
    5 5 5
     
    ### DISCUSSION ###
     
    # 1) Effectiveness
    # Matches desired records.

    # 2) Efficiency
    > sol.6 <- microbenchmark(fun.6(x.1,x.2))
    > sol.6 <- median(sol.6$time)/to.sec
    > sol.6
    > [1] 0.0007865175

**SOLUTION 7**
--------
by nullglob

    > fun.7 <- function(x.1,x.2,...){
    +     library(compare)
    +     comparison <- compare(x.1,x.2,allowAll=TRUE)
    +     comparison$tM
    +     
    +     difference <- data.frame(
    +         lapply(1:ncol(x.1), function(i){
    +             setdiff(x.1[,i], comparison$tM[,i])
    +         })
    +     )
    +     colnames(difference) <- colnames(x.1)
    +     difference
    + }
    > fun.7(x.1,x.2)
    [1] a b
    <0 rows> (or 0-length row.names)
    > # Original example by nullglob:
    > fun.7 <- function(x.1,x.2,...){
    +     a1 <- data.frame(a = 1:5, b = letters[1:5])
    +     a2 <- data.frame(a = 1:3, b = letters[1:3])
    +     comparison <- compare(a1,a2,allowAll=TRUE)
    +     comparison$tM
    +     difference <-
    +         data.frame(lapply(1:ncol(a1),function(i)setdiff(a1[,i],comparison$tM[,i])))
    +     colnames(difference) <- colnames(a1)
    +     difference
    + }
    > fun.7(x.1,x.2)
      a b
    1 4 d
    2 5 e
     
    ### DISCUSSION ###
     
    # 1) Effectiveness
    # Could not reproduce results with my data frames.

    # 2) Efficiency
    > sol.7 <- microbenchmark(fun.7(x.1,x.2)) 
    > sol.7 <- median(sol.7$time)/to.sec
    > sol.7
    > [1] 0.01010778

**SOLUTION 8**
--------
by Henrico

    #  Derived from src/library/base/R/merge.R
    #  Part of the R package, http://www.R-project.org
    #
    #  This program is free software; you can redistribute it and/or modify
    #  it under the terms of the GNU General Public License as published by
    #  the Free Software Foundation; either version 2 of the License, or
    #  (at your option) any later version.
    #
    #  This program is distributed in the hope that it will be useful,
    #  but WITHOUT ANY WARRANTY; without even the implied warranty of
    #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    #  GNU General Public License for more details.
    #
    #  A copy of the GNU General Public License is available at
    #  http://www.r-project.org/Licenses/
    > XinY <-
    +     function(x, y, by = intersect(names(x), names(y)), by.x = by, by.y = by,
    +              notin = FALSE, incomparables = NULL,
    +              ...)
    + {
    +     fix.by <- function(by, df)
    +     {
    +         ## fix up 'by' to be a valid set of cols by number: 0 is row.names
    +         if(is.null(by)) by <- numeric(0L)
    +         by <- as.vector(by)
    +         nc <- ncol(df)
    +         if(is.character(by))
    +             by <- match(by, c(""row.names"", names(df))) - 1L
    +         else if(is.numeric(by)) {
    +             if(any(by < 0L) || any(by > nc))
    +                 stop(""'by' must match numbers of columns"")
    +         } else if(is.logical(by)) {
    +             if(length(by) != nc) stop(""'by' must match number of columns"")
    +             by <- seq_along(by)[by]
    +         } else stop(""'by' must specify column(s) as numbers, names or logical"")
    +         if(any(is.na(by))) stop(""'by' must specify valid column(s)"")
    +         unique(by)
    +     }
    + 
    +     nx <- nrow(x <- as.data.frame(x)); ny <- nrow(y <- as.data.frame(y))
    +     by.x <- fix.by(by.x, x)
    +     by.y <- fix.by(by.y, y)
    +     if((l.b <- length(by.x)) != length(by.y))
    +         stop(""'by.x' and 'by.y' specify different numbers of columns"")
    +     if(l.b == 0L) {
    +         ## was: stop(""no columns to match on"")
    +         ## returns x
    +         x
    +     }
    +     else {
    +         if(any(by.x == 0L)) {
    +             x <- cbind(Row.names = I(row.names(x)), x)
    +             by.x <- by.x + 1L
    +         }
    +         if(any(by.y == 0L)) {
    +             y <- cbind(Row.names = I(row.names(y)), y)
    +             by.y <- by.y + 1L
    +         }
    +         ## create keys from 'by' columns:
    +         if(l.b == 1L) {                  # (be faster)
    +             bx <- x[, by.x]; if(is.factor(bx)) bx <- as.character(bx)
    +             by <- y[, by.y]; if(is.factor(by)) by <- as.character(by)
    +         } else {
    +             ## Do these together for consistency in as.character.
    +             ## Use same set of names.
    +             bx <- x[, by.x, drop=FALSE]; by <- y[, by.y, drop=FALSE]
    +             names(bx) <- names(by) <- paste(""V"", seq_len(ncol(bx)), sep="""")
    +             bz <- do.call(""paste"", c(rbind(bx, by), sep = ""\r""))
    +             bx <- bz[seq_len(nx)]
    +             by <- bz[nx + seq_len(ny)]
    +         }
    +         comm <- match(bx, by, 0L)
    +         if (notin) {
    +             res <- x[comm == 0,]
    +         } else {
    +             res <- x[comm > 0,]
    +         }
    +     }
    +     ## avoid a copy
    +     ## row.names(res) <- NULL
    +     attr(res, ""row.names"") <- .set_row_names(nrow(res))
    +     res
    + }
    > 
    > XnotinY <-
    +     function(x, y, by = intersect(names(x), names(y)), by.x = by, by.y = by,
    +              notin = TRUE, incomparables = NULL,
    +              ...)
    + {
    +     XinY(x,y,by,by.x,by.y,notin,incomparables)
    + }
    > fun.8 <- XnotinY 
    > fun.8(x.1,x.2)
      a b
    1 2 2
    2 5 5
     
    ### DISCUSSION ###
    
    # 1) Effectiveness
    # Matches desired records.

    # 2) Efficiency
    > sol.8 <- microbenchmark(fun.8(x.1,x.2))
    > sol.8 <- median(sol.8$time)/to.sec
    > sol.8
    > [1] 0.001030573

**EFFICIENCY COMPARISON**
--------

    > comp <- data.frame(lapply(1:8, function(x){
    +     eval(substitute(get(SOL), list(SOL=paste(""sol."", x, sep=""""))))        
    + }))
    > names(comp) <- paste(""solution"", 1:8)
    > comp <- comp[order(comp[1,])]
    > comp
       solution 2  solution 1  solution 3   solution 6  solution 8  solution 4
    1 3.59595e-05 0.000214137 0.000244114 0.0007865175 0.001030573 0.001057671
      solution 7 solution 5
    1 0.01010778 0.02260869",r,join,merge,match,data.frame,10/12/2011 01:12:28,not a real question,1,3247,12,"Identify records in data frame A not contained in data frame B This is my first time posting here, so please be kind ;-)

I was wondering what would be the most efficient way to identify records in one data frame that `(x.1)` are not contained in a second data frame `(x.2)`. Kind of like the ""inverse"" of 

    merge(x.1, x.2, by.x=TRUE)

I thought that if `merge()` does the merging job, it must also be able to do the ""reverse/inverse"" out-of-the-box, which seems not to be true (maybe I am missing out on something, though) So I've looked for a while and below are the solutions that I found (a bit lengthy, but maybe a reference for other people).

Yet I still wonder if there's nothing more ""built-in"" ;-)

Thanks for any advice!


----------


PROBLEM
-----------

Identify records in `x.1` that are *not* contained in `x.2` (both 'data.frame') based on *all* attributes available (i.e. columns).

**DATA**
--------

    > x.1 <- data.frame(a=c(1,2,3,4,5), b=c(1,2,3,4,5))
    > x.1
      a b
    1 1 1
    2 2 2
    3 3 3
    4 4 4
    5 5 5

    > x.2 <- data.frame(a=c(1,1,2,3,4), b=c(1,1,99,3,4))
    > x.2
      a  b
    1 1  1
    2 1  1
    3 2 99
    4 3  3
    5 4  4

**BENCHMARK SETTINGS**
--------

    > require(microbenchmark)
    > to.sec <- 1000000000

**SOLUTION 1**
--------
by Chase

    > subset(x.1, !(a %in% x.2$a))
      a b
    5 5 5

    ### DISCUSSION ###
    
    # 1) Effectiveness
    # Does not match x.1 = (2,2) as only column 'a' is used. Yet, extendable to 
    # multiple column check, but this would get pretty verbose:
    > subset(x.1, !(a %in% x.2$a) | !(b %in% x.2$b))
      a b
    2 2 2
    5 5 5
    
    # 2) Efficiency
    > sol.1 <- microbenchmark(subset(x.1, !(a %in% x.2$a)))
    > sol.1 <- median(sol.1$time)/to.sec
    > sol.1
    [1] 0.000214137

**SOLUTION 2** 
--------
by Ramnath

    > setdiff(x.1$a, x.2$a)   # elements in x.1$a NOT in x.2$a
    [1] 5
    > setdiff(x.2$a, x.1$a)   # elements in x.2$a NOT in x.1$a
    
    ### DISCUSSION ###
    
    # 1) Effectiveness
    # Not easily extendable to multiple column check.
    
    # 2) Efficiency
    > sol.2 <- microbenchmark(setdiff(x.1$a, x.2$a))
    > sol.2 <- median(sol.2$time)/to.sec
    > sol.2
    [1] 3.59595e-05

**SOLUTION 3** 
--------
by Brian Ripley

    > fun.3 <- function(x.1, x.2, ...){
    +    x.1.id <- do.call(""paste"", c(x.1, sep = ""\r""))
    +    x.2.id <- do.call(""paste"", c(x.2, sep = ""\r""))
    +    x.1[match(setdiff(x.1.id, x.2.id),x.1.id), ]
    + }
    > fun.3(x.1,x.2)
      a b
    2 2 2
    5 5 5
    
    ### DISCUSSION ###
    
    # 1) Effectiveness
    # Matches desired records
    
    # 2) Efficiency
    > sol.3 <- microbenchmark(fun.3(x.1,x.2))
    > sol.3 <- median(sol.3$time)/to.sec
    > sol.3
    [1] 0.000244114

**SOLUTION 4**
--------
by myself

    > fun.4 <- function(x.1, x.2, ...){
    +     # Combine
    +     df <- rbind(x.1,x.2)
    +     df <- df[order(df[,1]),]
    +     # Find duplicates
    +     idx.1 <- duplicated(df, all=TRUE)
    +     idx.2 <- duplicated(df, fromLast=TRUE)
    +     idx <- cbind(idx.1, idx.2)
    +     idx <- apply(idx, MARGIN=1, any)
    +     # Index records
    +     df[-which(idx),]
    + }
    > fun.4(x.1,x.2)
      a  b
    2 2  2
    8 2 99
    5 5  5
    
    ### DISCUSSION ###
    
    # 1) Effectiveness
    # Matches record x.2 = (2,99) which was not contained in x.1 as duplicate 
    # check is performed on combined df and result is not verified against x.1

    # 2) Efficiency
    > sol.4 <- microbenchmark(fun.4(x.1,x.2))
    > sol.4 <- median(sol.4$time)/to.sec
    > sol.4
    > [1] 0.001057671

**SOLUTION 5**
--------
by Gabor Grothendiek

    > library(sqldf)
    > fun.5 <- function(x1,x2,...){
    +     out <- sqldf(
    +         ""SELECT * FROM x1 
    +             WHERE 
    +                 x1.a NOT IN (SELECT x2.a FROM x2) OR 
    +                 x1.b NOT IN (SELECT x2.b FROM x2)""
    +     )
    +     out
    + }
    > fun.5(x1=x.1,x2=x.2)
      a b
    1 2 2
    2 5 5
     
    ### DISCUSSION ###
     
    # 1) Effectiveness
    # Matches desired records, but will get equally verbose as solution 1 when trying
    # to extend matching criteria.

    # 2) Efficiency
    > sol.5 <- microbenchmark(fun.5(x1=x.1,x2=x.2))
    > sol.5 <- median(sol.5$time)/to.sec
    > sol.5
    > [1] 0.02260869

**SOLUTION 6**
--------
by Tal Galili

    > fun.6 <- function(x.1,x.2){
    +     x.1.vec <- apply(x.1, 1, paste, collapse = """")
    +     x.2.vec <- apply(x.2, 1, paste, collapse = """")
    +     x.1.without.x.2.rows <- x.1[!x.1.vec %in% x.2.vec,]
    +     return(x.1.without.x.2.rows)
    + }
    > fun.6(x.1,x.2)
      a b
    2 2 2
    5 5 5
     
    ### DISCUSSION ###
     
    # 1) Effectiveness
    # Matches desired records.

    # 2) Efficiency
    > sol.6 <- microbenchmark(fun.6(x.1,x.2))
    > sol.6 <- median(sol.6$time)/to.sec
    > sol.6
    > [1] 0.0007865175

**SOLUTION 7**
--------
by nullglob

    > fun.7 <- function(x.1,x.2,...){
    +     library(compare)
    +     comparison <- compare(x.1,x.2,allowAll=TRUE)
    +     comparison$tM
    +     
    +     difference <- data.frame(
    +         lapply(1:ncol(x.1), function(i){
    +             setdiff(x.1[,i], comparison$tM[,i])
    +         })
    +     )
    +     colnames(difference) <- colnames(x.1)
    +     difference
    + }
    > fun.7(x.1,x.2)
    [1] a b
    <0 rows> (or 0-length row.names)
    > # Original example by nullglob:
    > fun.7 <- function(x.1,x.2,...){
    +     a1 <- data.frame(a = 1:5, b = letters[1:5])
    +     a2 <- data.frame(a = 1:3, b = letters[1:3])
    +     comparison <- compare(a1,a2,allowAll=TRUE)
    +     comparison$tM
    +     difference <-
    +         data.frame(lapply(1:ncol(a1),function(i)setdiff(a1[,i],comparison$tM[,i])))
    +     colnames(difference) <- colnames(a1)
    +     difference
    + }
    > fun.7(x.1,x.2)
      a b
    1 4 d
    2 5 e
     
    ### DISCUSSION ###
     
    # 1) Effectiveness
    # Could not reproduce results with my data frames.

    # 2) Efficiency
    > sol.7 <- microbenchmark(fun.7(x.1,x.2)) 
    > sol.7 <- median(sol.7$time)/to.sec
    > sol.7
    > [1] 0.01010778

**SOLUTION 8**
--------
by Henrico

    #  Derived from src/library/base/R/merge.R
    #  Part of the R package, http://www.R-project.org
    #
    #  This program is free software; you can redistribute it and/or modify
    #  it under the terms of the GNU General Public License as published by
    #  the Free Software Foundation; either version 2 of the License, or
    #  (at your option) any later version.
    #
    #  This program is distributed in the hope that it will be useful,
    #  but WITHOUT ANY WARRANTY; without even the implied warranty of
    #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    #  GNU General Public License for more details.
    #
    #  A copy of the GNU General Public License is available at
    #  http://www.r-project.org/Licenses/
    > XinY <-
    +     function(x, y, by = intersect(names(x), names(y)), by.x = by, by.y = by,
    +              notin = FALSE, incomparables = NULL,
    +              ...)
    + {
    +     fix.by <- function(by, df)
    +     {
    +         ## fix up 'by' to be a valid set of cols by number: 0 is row.names
    +         if(is.null(by)) by <- numeric(0L)
    +         by <- as.vector(by)
    +         nc <- ncol(df)
    +         if(is.character(by))
    +             by <- match(by, c(""row.names"", names(df))) - 1L
    +         else if(is.numeric(by)) {
    +             if(any(by < 0L) || any(by > nc))
    +                 stop(""'by' must match numbers of columns"")
    +         } else if(is.logical(by)) {
    +             if(length(by) != nc) stop(""'by' must match number of columns"")
    +             by <- seq_along(by)[by]
    +         } else stop(""'by' must specify column(s) as numbers, names or logical"")
    +         if(any(is.na(by))) stop(""'by' must specify valid column(s)"")
    +         unique(by)
    +     }
    + 
    +     nx <- nrow(x <- as.data.frame(x)); ny <- nrow(y <- as.data.frame(y))
    +     by.x <- fix.by(by.x, x)
    +     by.y <- fix.by(by.y, y)
    +     if((l.b <- length(by.x)) != length(by.y))
    +         stop(""'by.x' and 'by.y' specify different numbers of columns"")
    +     if(l.b == 0L) {
    +         ## was: stop(""no columns to match on"")
    +         ## returns x
    +         x
    +     }
    +     else {
    +         if(any(by.x == 0L)) {
    +             x <- cbind(Row.names = I(row.names(x)), x)
    +             by.x <- by.x + 1L
    +         }
    +         if(any(by.y == 0L)) {
    +             y <- cbind(Row.names = I(row.names(y)), y)
    +             by.y <- by.y + 1L
    +         }
    +         ## create keys from 'by' columns:
    +         if(l.b == 1L) {                  # (be faster)
    +             bx <- x[, by.x]; if(is.factor(bx)) bx <- as.character(bx)
    +             by <- y[, by.y]; if(is.factor(by)) by <- as.character(by)
    +         } else {
    +             ## Do these together for consistency in as.character.
    +             ## Use same set of names.
    +             bx <- x[, by.x, drop=FALSE]; by <- y[, by.y, drop=FALSE]
    +             names(bx) <- names(by) <- paste(""V"", seq_len(ncol(bx)), sep="""")
    +             bz <- do.call(""paste"", c(rbind(bx, by), sep = ""\r""))
    +             bx <- bz[seq_len(nx)]
    +             by <- bz[nx + seq_len(ny)]
    +         }
    +         comm <- match(bx, by, 0L)
    +         if (notin) {
    +             res <- x[comm == 0,]
    +         } else {
    +             res <- x[comm > 0,]
    +         }
    +     }
    +     ## avoid a copy
    +     ## row.names(res) <- NULL
    +     attr(res, ""row.names"") <- .set_row_names(nrow(res))
    +     res
    + }
    > 
    > XnotinY <-
    +     function(x, y, by = intersect(names(x), names(y)), by.x = by, by.y = by,
    +              notin = TRUE, incomparables = NULL,
    +              ...)
    + {
    +     XinY(x,y,by,by.x,by.y,notin,incomparables)
    + }
    > fun.8 <- XnotinY 
    > fun.8(x.1,x.2)
      a b
    1 2 2
    2 5 5
     
    ### DISCUSSION ###
    
    # 1) Effectiveness
    # Matches desired records.

    # 2) Efficiency
    > sol.8 <- microbenchmark(fun.8(x.1,x.2))
    > sol.8 <- median(sol.8$time)/to.sec
    > sol.8
    > [1] 0.001030573

**EFFICIENCY COMPARISON**
--------

    > comp <- data.frame(lapply(1:8, function(x){
    +     eval(substitute(get(SOL), list(SOL=paste(""sol."", x, sep=""""))))        
    + }))
    > names(comp) <- paste(""solution"", 1:8)
    > comp <- comp[order(comp[1,])]
    > comp
       solution 2  solution 1  solution 3   solution 6  solution 8  solution 4
    1 3.59595e-05 0.000214137 0.000244114 0.0007865175 0.001030573 0.001057671
      solution 7 solution 5
    1 0.01010778 0.02260869",5
2599824,04/08/2010 12:36:10,223265,12/02/2009 21:26:48,346,15,What's the most comprehensive and comprehensible overview of statistics for programmers?,"I'm looking for a book (or other media) which provides an overview of statistics that is both comprehensive (covering all the basic/intermediate concepts) and comprehensible (which, for me, means not being weighed down with unnecessary and especially un-introduced mathematical symbology).

Can anyone offer suggestions?",r,statistics,self-improvement,books,,10/24/2011 02:26:38,not constructive,1,43,11,"What's the most comprehensive and comprehensible overview of statistics for programmers? I'm looking for a book (or other media) which provides an overview of statistics that is both comprehensive (covering all the basic/intermediate concepts) and comprehensible (which, for me, means not being weighed down with unnecessary and especially un-introduced mathematical symbology).

Can anyone offer suggestions?",4
9296004,02/15/2012 15:19:10,1148098,01/13/2012 16:35:24,6,1,Re: installing R on Linux,"Steps to install:

1. ./configure --enable-R-shlib

I get this error:

     configure: error: --with-x=yes (default) and X11 headers/libs are not available
in config.log file
I see this entry:

     #define X_DISPLAY_MISSING 1

any ideas?",r,,,,,02/15/2012 16:36:36,off topic,1,35,5,"Re: installing R on Linux Steps to install:

1. ./configure --enable-R-shlib

I get this error:

     configure: error: --with-x=yes (default) and X11 headers/libs are not available
in config.log file
I see this entry:

     #define X_DISPLAY_MISSING 1

any ideas?",1
1204043,07/30/2009 01:39:21,136862,07/11/2009 21:36:42,228,6,Good intro books for R,"I have a slew of Springer books on specific packages, but my library is lacking a good solid book on R the language. While a language spec book would be great, it would be nice if I could find a book that has a solid founding in the language itself but couched in the examples that typify the day to day work of R users - statistics, visualization, analysis & modelling. 

What is the one R book that you would take to a desert island?",r,books,,,,09/26/2011 14:34:31,not constructive,1,85,5,"Good intro books for R I have a slew of Springer books on specific packages, but my library is lacking a good solid book on R the language. While a language spec book would be great, it would be nice if I could find a book that has a solid founding in the language itself but couched in the examples that typify the day to day work of R users - statistics, visualization, analysis & modelling. 

What is the one R book that you would take to a desert island?",2
2551921,03/31/2010 10:00:59,457898,01/29/2010 20:24:01,221,20,Show frequencies along with barplot in ggplot2,"I'm trying to display frequencies within barplot ... well, I want them **somewhere** in the graph: under the bars, within bars, above bars or in the legend area. And I recall (I may be wrong) that it can be done in `ggplot2`. This is probably an easy one... at least it seems easy. Here's the code:

    p <- ggplot(mtcars)
    p + aes(factor(cyl)) + geom_bar()

Is there any chance that I can get frequencies embedded in the graph?",r,ggplot2,frequency,,,,open,0,82,7,"Show frequencies along with barplot in ggplot2 I'm trying to display frequencies within barplot ... well, I want them **somewhere** in the graph: under the bars, within bars, above bars or in the legend area. And I recall (I may be wrong) that it can be done in `ggplot2`. This is probably an easy one... at least it seems easy. Here's the code:

    p <- ggplot(mtcars)
    p + aes(factor(cyl)) + geom_bar()

Is there any chance that I can get frequencies embedded in the graph?",3
9639024,03/09/2012 18:21:36,1259867,03/09/2012 17:35:36,1,0,Multiply column values in one data.frame by column in another data.frame on a condition in R,"I have two data frames in r that I am trying to combine based on the values in a column for each.

    df1=data.frame(comp=c(""comp1"", ""comp2"", ""comp3"", ""comp1""),state1=c(1,0,0,1),state2=c(1,1,0,1),state3=c(0,1,1,0),state4=c(0,0,1,0),year=c(1,1,1,2))

which df1 is:

             state1 state2 state3 state4 year
        1 comp1  1      1      0      0     1
        2 comp2  0      1      1      0     1
        3 comp3  0      0      1      1     1
        4 comp1  1      1      0      0     2

    
and

    df2=data.frame(state=c(""state1"",""state2"", ""state3"", ""state4"", ""state1"",""state2"", ""state3"", ""state4""), var1=c(1,0,0,1,0,0,1,1), var2=c(0,1,0,0,0,1,1,0), year=c(1,1,1,1,2,2,2,2))

df2

        state var1 var2 year
    1 state1    1    0    1
    2 state2    0    1    1
    3 state3    0    0    1
    4 state4    1    0    1
    5 state1    0    0    2
    6 state2    0    1    2
    7 state3    1    1    2
    8 state4    1    0    2


I'd like to append columns to df1 that are var1, var2 which is the mean of all states for that comp. 

so, var1 for comp1 should be 1*1+1*0+0*0+0*1/(1+1) or state*var/sum(state for comp) by year.

Is this possible?  I tried to use ddply with mean of var1, by comp and year, but that doesn't work.

Thanks in advance.
This one is the  most similar to my problem, but it doesn't show a conditional in the second data set. http://stackoverflow.com/questions/6878899/r-multiply-various-subsets-of-a-data-frame-by-different-vectors

Please advise.",r,data.frame,,,,,open,0,424,16,"Multiply column values in one data.frame by column in another data.frame on a condition in R I have two data frames in r that I am trying to combine based on the values in a column for each.

    df1=data.frame(comp=c(""comp1"", ""comp2"", ""comp3"", ""comp1""),state1=c(1,0,0,1),state2=c(1,1,0,1),state3=c(0,1,1,0),state4=c(0,0,1,0),year=c(1,1,1,2))

which df1 is:

             state1 state2 state3 state4 year
        1 comp1  1      1      0      0     1
        2 comp2  0      1      1      0     1
        3 comp3  0      0      1      1     1
        4 comp1  1      1      0      0     2

    
and

    df2=data.frame(state=c(""state1"",""state2"", ""state3"", ""state4"", ""state1"",""state2"", ""state3"", ""state4""), var1=c(1,0,0,1,0,0,1,1), var2=c(0,1,0,0,0,1,1,0), year=c(1,1,1,1,2,2,2,2))

df2

        state var1 var2 year
    1 state1    1    0    1
    2 state2    0    1    1
    3 state3    0    0    1
    4 state4    1    0    1
    5 state1    0    0    2
    6 state2    0    1    2
    7 state3    1    1    2
    8 state4    1    0    2


I'd like to append columns to df1 that are var1, var2 which is the mean of all states for that comp. 

so, var1 for comp1 should be 1*1+1*0+0*0+0*1/(1+1) or state*var/sum(state for comp) by year.

Is this possible?  I tried to use ddply with mean of var1, by comp and year, but that doesn't work.

Thanks in advance.
This one is the  most similar to my problem, but it doesn't show a conditional in the second data set. http://stackoverflow.com/questions/6878899/r-multiply-various-subsets-of-a-data-frame-by-different-vectors

Please advise.",2
9554507,03/04/2012 11:29:52,1227490,02/23/2012 04:55:18,11,0,I am unable to specify the correct date format to subset a dataframe in R,"This question is related to my previous one, http://stackoverflow.com/questions/9407622/subsetting-a-dataframe-for-a-specified-month-and-year

I use the command

sales <- read.csv(""mysales.csv"", colClasses=""character"")       

to obtain a dataframe which looks like this:

        date            pieces          income
1       21/11/2011      49              220.5
2       22/11/2011      58	 	261
3	23/11/2011	23	 	103.5
4	24/11/2011	57	 	256.5
5	25/11/2011	117	 	445.5
6	26/11/2011	107	 	0
7	27/11/2011	98	 	396
8	28/11/2011	0	 	0
9	29/11/2011	47	 	166.5
10	30/11/2011	48	 	198
11	01/12/2011	40	 	180
12	02/12/2011	34	 	153
13	03/12/2011	55	 	247.5
14	04/12/2011	77	 	346.5

I want to create a subset for November 2011 using the code provided in my previous question, but various attempts have failed. So for a check I wrote in the console:

format.Date(sales[1,1], ""%Y"")==""2011""

and the answer was:

[1] FALSE

Moreover:

format(as.Date(sales[1,1]), ""%d/%m/%Y"")
[1] ""20/11/21""

How can I, at least, know what is happening with date format?

What should I do to subset the dataframe using code like:

subset(sales, format.Date(date, ""%m"")==""11"" & format.Date(date, ""%Y"")==""2011"")

Sorry if my question is not clear, but the problem I am facing is not clear to me either.
",r,date,format,,,,open,0,193,15,"I am unable to specify the correct date format to subset a dataframe in R This question is related to my previous one, http://stackoverflow.com/questions/9407622/subsetting-a-dataframe-for-a-specified-month-and-year

I use the command

sales <- read.csv(""mysales.csv"", colClasses=""character"")       

to obtain a dataframe which looks like this:

        date            pieces          income
1       21/11/2011      49              220.5
2       22/11/2011      58	 	261
3	23/11/2011	23	 	103.5
4	24/11/2011	57	 	256.5
5	25/11/2011	117	 	445.5
6	26/11/2011	107	 	0
7	27/11/2011	98	 	396
8	28/11/2011	0	 	0
9	29/11/2011	47	 	166.5
10	30/11/2011	48	 	198
11	01/12/2011	40	 	180
12	02/12/2011	34	 	153
13	03/12/2011	55	 	247.5
14	04/12/2011	77	 	346.5

I want to create a subset for November 2011 using the code provided in my previous question, but various attempts have failed. So for a check I wrote in the console:

format.Date(sales[1,1], ""%Y"")==""2011""

and the answer was:

[1] FALSE

Moreover:

format(as.Date(sales[1,1]), ""%d/%m/%Y"")
[1] ""20/11/21""

How can I, at least, know what is happening with date format?

What should I do to subset the dataframe using code like:

subset(sales, format.Date(date, ""%m"")==""11"" & format.Date(date, ""%Y"")==""2011"")

Sorry if my question is not clear, but the problem I am facing is not clear to me either.
",3
5437400,03/25/2011 19:48:29,675740,03/24/2011 21:26:35,1,0,Need to access variables from a parent apply within a child apply without globally scoping,"Let me try this again, I'm going to leave out the exact data/example and just walk through what I need to accomplish.

I need to apply a function over the rows of a data.frame, that is easy. Then I need to derive some variables within that function using the data.frame that was passed to it.  Finally, I'd like to apply a new function over a subset of the data.frame and use the derived variables in the new function.

Can someone please tell me the best practice way to do this rather than globally scoping each of my variables (var1, var2)?

    cpt <- a.data.frame
    
    query.db <- function(another.data.frame){
    	    var1 <- some.values
            var2 <- some.other.values
            apply(cpt[var1,], 1, calc.enrichment) #calc.enrichment needs to access var1, var2!
    }

I tried writing the calc.enrichment function as a user-defined function rather than outside of the scope, but my list of arguments (var1, var2) weren't being recognized. Thanks for any help.",r,data.frame,,,,,open,0,192,15,"Need to access variables from a parent apply within a child apply without globally scoping Let me try this again, I'm going to leave out the exact data/example and just walk through what I need to accomplish.

I need to apply a function over the rows of a data.frame, that is easy. Then I need to derive some variables within that function using the data.frame that was passed to it.  Finally, I'd like to apply a new function over a subset of the data.frame and use the derived variables in the new function.

Can someone please tell me the best practice way to do this rather than globally scoping each of my variables (var1, var2)?

    cpt <- a.data.frame
    
    query.db <- function(another.data.frame){
    	    var1 <- some.values
            var2 <- some.other.values
            apply(cpt[var1,], 1, calc.enrichment) #calc.enrichment needs to access var1, var2!
    }

I tried writing the calc.enrichment function as a user-defined function rather than outside of the scope, but my list of arguments (var1, var2) weren't being recognized. Thanks for any help.",2
11403919,07/09/2012 22:07:35,1009730,10/23/2011 16:45:34,63,3,how to find if all elements in a subset of a data.frame row are TRUE,"I have a data.frame with a block of columns that are logicals, e.g.

    > tmp <- data.frame(a=c(13, 23, 52),
    +                   b=c(TRUE,FALSE,TRUE),
    +                   c=c(TRUE,TRUE,FALSE),
    +                   d=c(TRUE,TRUE,TRUE))
    > tmp
       a     b     c    d
    1 13  TRUE  TRUE TRUE
    2 23 FALSE  TRUE TRUE
    3 52  TRUE FALSE TRUE

I'd like to compute a summary column (say: e) that is a  logical `AND` over the whole range of logical columns. In other words, for a given row, if all b:d are `TRUE`, then e would be `TRUE`; if any b:d are `FALSE`, then e would be `FALSE`.

My expected result is:

    > tmp
       a     b     c    d     e
    1 13  TRUE  TRUE TRUE  TRUE
    2 23 FALSE  TRUE TRUE FALSE
    3 52  TRUE FALSE TRUE FALSE

I want to indicate the range of columns by indices, as I have a bunch of columns, and the names are cumbersome. The following code works, but i'd rather use a vectorized approach to improve performance.

    > tmp$e <- NA
    > for(i in 1:nrow(tmp)){
    +     tmp[i,""e""] <- all(tmp[i,2:(ncol(tmp)-1)]==TRUE)
    + }
    > tmp
       a     b     c    d     e
    1 13  TRUE  TRUE TRUE  TRUE
    2 23 FALSE  TRUE TRUE FALSE
    3 52  TRUE FALSE TRUE FALSE


Any way to do this without using a `for` loop to step through the rows of the data.frame?",r,,,,,,open,0,404,15,"how to find if all elements in a subset of a data.frame row are TRUE I have a data.frame with a block of columns that are logicals, e.g.

    > tmp <- data.frame(a=c(13, 23, 52),
    +                   b=c(TRUE,FALSE,TRUE),
    +                   c=c(TRUE,TRUE,FALSE),
    +                   d=c(TRUE,TRUE,TRUE))
    > tmp
       a     b     c    d
    1 13  TRUE  TRUE TRUE
    2 23 FALSE  TRUE TRUE
    3 52  TRUE FALSE TRUE

I'd like to compute a summary column (say: e) that is a  logical `AND` over the whole range of logical columns. In other words, for a given row, if all b:d are `TRUE`, then e would be `TRUE`; if any b:d are `FALSE`, then e would be `FALSE`.

My expected result is:

    > tmp
       a     b     c    d     e
    1 13  TRUE  TRUE TRUE  TRUE
    2 23 FALSE  TRUE TRUE FALSE
    3 52  TRUE FALSE TRUE FALSE

I want to indicate the range of columns by indices, as I have a bunch of columns, and the names are cumbersome. The following code works, but i'd rather use a vectorized approach to improve performance.

    > tmp$e <- NA
    > for(i in 1:nrow(tmp)){
    +     tmp[i,""e""] <- all(tmp[i,2:(ncol(tmp)-1)]==TRUE)
    + }
    > tmp
       a     b     c    d     e
    1 13  TRUE  TRUE TRUE  TRUE
    2 23 FALSE  TRUE TRUE FALSE
    3 52  TRUE FALSE TRUE FALSE


Any way to do this without using a `for` loop to step through the rows of the data.frame?",1
8944057,01/20/2012 15:49:20,1160927,01/20/2012 15:36:58,1,0,How to cluster a column with R,"I have a dataframe 

    numbers = 1:4
    letters = factor(c(""a"", ""b"", ""c"", ""d""))
    df <- data.frame(numbers, letters)
    numbers letters
    1       1       a
    2       2       b
    3       3       c
    4       4       d

And I want to create a new column clustering the last column:

    numbers letters
    1       1       a      c1
    2       2       b      c2
    3       3       c      c1
    4       4       d      c1

Is there a way to do that with R?
",r,,,,,,open,0,224,7,"How to cluster a column with R I have a dataframe 

    numbers = 1:4
    letters = factor(c(""a"", ""b"", ""c"", ""d""))
    df <- data.frame(numbers, letters)
    numbers letters
    1       1       a
    2       2       b
    3       3       c
    4       4       d

And I want to create a new column clustering the last column:

    numbers letters
    1       1       a      c1
    2       2       b      c2
    3       3       c      c1
    4       4       d      c1

Is there a way to do that with R?
",1
11529297,07/17/2012 19:15:41,728345,04/28/2011 00:48:39,47,3,How do I clear an NA flag for a posix value?,"sample data in csv format. Save in a file broken_posix.csv

    Date
    3/10/2012 23:00
    3/11/2012 0:00
    3/11/2012 1:00
    3/11/2012 2:00
    3/11/2012 3:00
    3/11/2012 4:00
    3/11/2012 5:00
    3/11/2012 6:00
    3/11/2012 7:00
    3/11/2012 8:00
    3/11/2012 9:00
    3/11/2012 10:00
    3/11/2012 11:00
    3/11/2012 12:00
    3/11/2012 13:00
    3/11/2012 14:00
    3/11/2012 15:00
    3/11/2012 16:00
    3/11/2012 17:00
    3/11/2012 18:00
    3/11/2012 19:00
    3/11/2012 20:00
    3/11/2012 21:00
    3/11/2012 22:00
    3/11/2012 23:00
    3/12/2012 0:00
    3/12/2012 1:00
    3/12/2012 2:00
    3/12/2012 3:00
    3/12/2012 4:00
    3/12/2012 5:00
    3/12/2012 6:00
    3/12/2012 7:00
    3/12/2012 8:00
    3/12/2012 9:00
    3/12/2012 10:00
    3/12/2012 11:00


Hi guys, so I have this file broken_posix.csv
I can read the file just fine with 

a_var <- read.csv(""broken_posix.csv"")

Then I can convert it to posix using

    a_var_posixct = as.POSIXct(strptime( as.character( a_var$Date) , '%m/%d/%Y %H:%M'))

or with 

    a_var_posixlt = strptime(as.character( a_var$Date) , '%m/%d/%Y %H:%M')

The problem occurs now though because when I use posixct, then I get 4 NA values in my string every year. When I use posixlt I get one NA value on March 11,2012 at 2 (daylight savings time)

You'll see what I mean when you run
    
    which(is.na(a_var_posixct))
    which(is.na(a_var_posixlt))

    a_var_posixct[4]
    a_var_posixlt[4]

The fourth value is always a ""NA"" value whenever you apply an operation even though it is clearly a date value for posixlt.

I've tried omitting the value only to end up messing up the rest of the posix string.
I've tried setting the posix string as itself, in an attempt to clear the NA flag, to no effect.
I've even tried setting it as a character value only to lose the hour and minute formatting.

I think that this situation occurs because of daylight savings time. It's very frustrating to deal with because when I try to run other functions on the dates I have to try to avoid the NA values since I can't change them. I could aggregate the data by day, and or just use date objects but that doesn't seem like the right method.",r,,,,,,open,0,444,11,"How do I clear an NA flag for a posix value? sample data in csv format. Save in a file broken_posix.csv

    Date
    3/10/2012 23:00
    3/11/2012 0:00
    3/11/2012 1:00
    3/11/2012 2:00
    3/11/2012 3:00
    3/11/2012 4:00
    3/11/2012 5:00
    3/11/2012 6:00
    3/11/2012 7:00
    3/11/2012 8:00
    3/11/2012 9:00
    3/11/2012 10:00
    3/11/2012 11:00
    3/11/2012 12:00
    3/11/2012 13:00
    3/11/2012 14:00
    3/11/2012 15:00
    3/11/2012 16:00
    3/11/2012 17:00
    3/11/2012 18:00
    3/11/2012 19:00
    3/11/2012 20:00
    3/11/2012 21:00
    3/11/2012 22:00
    3/11/2012 23:00
    3/12/2012 0:00
    3/12/2012 1:00
    3/12/2012 2:00
    3/12/2012 3:00
    3/12/2012 4:00
    3/12/2012 5:00
    3/12/2012 6:00
    3/12/2012 7:00
    3/12/2012 8:00
    3/12/2012 9:00
    3/12/2012 10:00
    3/12/2012 11:00


Hi guys, so I have this file broken_posix.csv
I can read the file just fine with 

a_var <- read.csv(""broken_posix.csv"")

Then I can convert it to posix using

    a_var_posixct = as.POSIXct(strptime( as.character( a_var$Date) , '%m/%d/%Y %H:%M'))

or with 

    a_var_posixlt = strptime(as.character( a_var$Date) , '%m/%d/%Y %H:%M')

The problem occurs now though because when I use posixct, then I get 4 NA values in my string every year. When I use posixlt I get one NA value on March 11,2012 at 2 (daylight savings time)

You'll see what I mean when you run
    
    which(is.na(a_var_posixct))
    which(is.na(a_var_posixlt))

    a_var_posixct[4]
    a_var_posixlt[4]

The fourth value is always a ""NA"" value whenever you apply an operation even though it is clearly a date value for posixlt.

I've tried omitting the value only to end up messing up the rest of the posix string.
I've tried setting the posix string as itself, in an attempt to clear the NA flag, to no effect.
I've even tried setting it as a character value only to lose the hour and minute formatting.

I think that this situation occurs because of daylight savings time. It's very frustrating to deal with because when I try to run other functions on the dates I have to try to avoid the NA values since I can't change them. I could aggregate the data by day, and or just use date objects but that doesn't seem like the right method.",1
8264530,11/25/2011 03:19:54,1048757,11/16/2011 01:34:43,35,0,How do I calculate amplitude and phase angle of fft() output from real-valued input?,"I have 24 samples from a real-valued signal. I perform the fft() function on the sample and get the complex output. I want to obtain the amplitude and phase angle of each of the non-redundant harmonics. I know my calculation must account for aliasing since I have real-valued data. How do I:

(1) convert from the two-sided to a one-sided Fourier transform,

I've heard several things here. For example, do I multiply the first 12 harmonics (i.e., 2nd through 13th elements of fft() output) by two and drop the rest of the harmonics (i.e., keep 1st through 13th elements of fft() output)?

(2) calculate the amplitude of the one-sided Fourier transform,

I know I can use the Mod() function, but when do I do this? Before or after I convert from two- to one-sided?

(3) calculate the phase angle of the one-sided Fourier transform.

I know I can use the atan() function on the ratio of imaginary to real parts of the fft() output, but again, when do I do this? Before or after two- to one-sided conversion? Also, what if atan is undefined?

Thanks.",r,fft,,,,11/25/2011 15:07:09,off topic,1,178,14,"How do I calculate amplitude and phase angle of fft() output from real-valued input? I have 24 samples from a real-valued signal. I perform the fft() function on the sample and get the complex output. I want to obtain the amplitude and phase angle of each of the non-redundant harmonics. I know my calculation must account for aliasing since I have real-valued data. How do I:

(1) convert from the two-sided to a one-sided Fourier transform,

I've heard several things here. For example, do I multiply the first 12 harmonics (i.e., 2nd through 13th elements of fft() output) by two and drop the rest of the harmonics (i.e., keep 1st through 13th elements of fft() output)?

(2) calculate the amplitude of the one-sided Fourier transform,

I know I can use the Mod() function, but when do I do this? Before or after I convert from two- to one-sided?

(3) calculate the phase angle of the one-sided Fourier transform.

I know I can use the atan() function on the ratio of imaginary to real parts of the fft() output, but again, when do I do this? Before or after two- to one-sided conversion? Also, what if atan is undefined?

Thanks.",2
9317428,02/16/2012 19:03:22,906592,08/22/2011 20:13:19,451,2,Mixed model with lme4. Is the effect significant?,"I use lme4 in R to fit the mixed model

    lmer(value~status+(1|experiment)))

where value is continuous, status and experiment are factors, and I get

    Linear mixed model fit by REML 
    Formula: value ~ status + (1 | experiment) 
      AIC   BIC logLik deviance REMLdev
     29.1 46.98 -9.548    5.911    19.1
    Random effects:
     Groups     Name        Variance Std.Dev.
     experiment (Intercept) 0.065526 0.25598 
     Residual               0.053029 0.23028 
    Number of obs: 264, groups: experiment, 10
    
    Fixed effects:
                Estimate Std. Error t value
    (Intercept)  2.78004    0.08448   32.91
    statusD      0.20493    0.03389    6.05
    statusR      0.88690    0.03583   24.76
    
    Correlation of Fixed Effects:
            (Intr) statsD
    statusD -0.204       
    statusR -0.193  0.476


How can I know that the effect of status is significant? R reports only t-values and not p-values

Thanks a lot",r,,,,,02/16/2012 21:27:20,off topic,1,278,8,"Mixed model with lme4. Is the effect significant? I use lme4 in R to fit the mixed model

    lmer(value~status+(1|experiment)))

where value is continuous, status and experiment are factors, and I get

    Linear mixed model fit by REML 
    Formula: value ~ status + (1 | experiment) 
      AIC   BIC logLik deviance REMLdev
     29.1 46.98 -9.548    5.911    19.1
    Random effects:
     Groups     Name        Variance Std.Dev.
     experiment (Intercept) 0.065526 0.25598 
     Residual               0.053029 0.23028 
    Number of obs: 264, groups: experiment, 10
    
    Fixed effects:
                Estimate Std. Error t value
    (Intercept)  2.78004    0.08448   32.91
    statusD      0.20493    0.03389    6.05
    statusR      0.88690    0.03583   24.76
    
    Correlation of Fixed Effects:
            (Intr) statsD
    statusD -0.204       
    statusR -0.193  0.476


How can I know that the effect of status is significant? R reports only t-values and not p-values

Thanks a lot",1
4056919,10/30/2010 01:59:23,170352,09/08/2009 18:23:21,1176,50,R Quality Assurance Techniques,"Could you provide some insight into the techniques that you use to ensure the quality of your solutions. For example, sometimes, I like to test my result using stopif() to ensure I'm not receiving ridiculous results. Are there any other techniques or functions that you use in data processing to ensure that you're receiving the solution you meant to?

Note: I realize that this is a broad question and perhaps a candidate for community wiki or even closure, but rather than voting to close, perhaps assist me by adding comments to direct the conversation.  ",r,,,,,,open,0,95,4,"R Quality Assurance Techniques Could you provide some insight into the techniques that you use to ensure the quality of your solutions. For example, sometimes, I like to test my result using stopif() to ensure I'm not receiving ridiculous results. Are there any other techniques or functions that you use in data processing to ensure that you're receiving the solution you meant to?

Note: I realize that this is a broad question and perhaps a candidate for community wiki or even closure, but rather than voting to close, perhaps assist me by adding comments to direct the conversation.  ",1
8129485,11/14/2011 23:05:43,633251,02/24/2011 22:43:02,214,8,Using inst/extdata with vignette during package checking R 2.14.0,"I have a package which contains a csv file which I put in inst/extdata per R-exts.  This file is needed for the vignette.  If I Sweave the vignette directly, all works well.  When I run R --vanilla CMD check however, the check process can't find the file.  I know it has been moved into an .Rcheck directory during checking and this is probably part of the problem.  But I don't know how to set it up so both direct Sweave and vignette building/checking works.

The vignette contains a line like this:

    EC1 <- dot2HPD(file = ""../inst/extdata/E_coli/ecoli.dot"",
	node.inst = ""../inst/extdata/E_coli/NodeInst.csv"",

and the function dot2HPD accesses the file via:

    	ni <- read.csv(node.inst)


Here's the error message:

        > tab <- read.csv(""../inst/extdata/E_coli/NodeInst.csv"")
    Warning in file(file, ""rt"") :
      cannot open file '../inst/extdata/E_coli/NodeInst.csv': No such file or directory
    
      When sourcing ‘HiveR.R’:
    Error: cannot open the connection
    Execution halted

By the way, this is related to this [question][1] but that info seems outdated and doesn't quite cover this territory.

I'm on a Mac.

  [1]: http://stackoverflow.com/questions/1886644/how-to-point-to-a-directory-in-an-r-package/1886891#1886891
",r,package,packages,,,,open,0,204,9,"Using inst/extdata with vignette during package checking R 2.14.0 I have a package which contains a csv file which I put in inst/extdata per R-exts.  This file is needed for the vignette.  If I Sweave the vignette directly, all works well.  When I run R --vanilla CMD check however, the check process can't find the file.  I know it has been moved into an .Rcheck directory during checking and this is probably part of the problem.  But I don't know how to set it up so both direct Sweave and vignette building/checking works.

The vignette contains a line like this:

    EC1 <- dot2HPD(file = ""../inst/extdata/E_coli/ecoli.dot"",
	node.inst = ""../inst/extdata/E_coli/NodeInst.csv"",

and the function dot2HPD accesses the file via:

    	ni <- read.csv(node.inst)


Here's the error message:

        > tab <- read.csv(""../inst/extdata/E_coli/NodeInst.csv"")
    Warning in file(file, ""rt"") :
      cannot open file '../inst/extdata/E_coli/NodeInst.csv': No such file or directory
    
      When sourcing ‘HiveR.R’:
    Error: cannot open the connection
    Execution halted

By the way, this is related to this [question][1] but that info seems outdated and doesn't quite cover this territory.

I'm on a Mac.

  [1]: http://stackoverflow.com/questions/1886644/how-to-point-to-a-directory-in-an-r-package/1886891#1886891
",3
5596661,04/08/2011 14:33:48,691390,04/04/2011 15:43:25,33,0,"R: strsplit symbol ""|"" lead to non-character error message","I want to use R's strsplit command to separate the cells based on the symbol ""|"".

However, an error message appears:

Error in strsplit(df[x, 3], ""|"") : non-character argument.

What does this error message means?  

How can I correct this error?

I was using the command line listed in previous a question and answer message in this website:

    > write.csv(df, file=""3columns.csv"")
    > as.data.frame(   
    + t(     
    + do.call(cbind,       
    + lapply(1:nrow(df),function(x){         
    + sapply(unlist(strsplit(df[x,3],""|"")),c,df[x,1:2],USE.NAMES=FALSE)       
    + })     
    + )   
    + ) 
    + )


Best regards & thanks,

Catherine",r,error-message,strsplit,,,04/09/2011 01:34:45,not a real question,1,149,9,"R: strsplit symbol ""|"" lead to non-character error message I want to use R's strsplit command to separate the cells based on the symbol ""|"".

However, an error message appears:

Error in strsplit(df[x, 3], ""|"") : non-character argument.

What does this error message means?  

How can I correct this error?

I was using the command line listed in previous a question and answer message in this website:

    > write.csv(df, file=""3columns.csv"")
    > as.data.frame(   
    + t(     
    + do.call(cbind,       
    + lapply(1:nrow(df),function(x){         
    + sapply(unlist(strsplit(df[x,3],""|"")),c,df[x,1:2],USE.NAMES=FALSE)       
    + })     
    + )   
    + ) 
    + )


Best regards & thanks,

Catherine",3
9551266,03/04/2012 00:43:33,1247522,03/04/2012 00:15:16,1,0,How Can I Create Ascetically Pleasing Graphs from R data?,"I am using R in a business setting and I am looking for a way to generate ascetically pleasing graphs from the data. The graphing packages for R, such as ggplot2, are very powerful but their output looks a bit too... functional. For my purposes the graphs need to be pretty while still communicating information. All I really need are a few tweaks like rounding the corners of the bars in a graph or shading the bar to create the illusion of depth. 

Basically, does anyone know of a software package that can progamatically create good looking charts and graphs?

Thanks!",r,graphics,graph,statistics,,03/04/2012 07:31:30,not constructive,1,100,10,"How Can I Create Ascetically Pleasing Graphs from R data? I am using R in a business setting and I am looking for a way to generate ascetically pleasing graphs from the data. The graphing packages for R, such as ggplot2, are very powerful but their output looks a bit too... functional. For my purposes the graphs need to be pretty while still communicating information. All I really need are a few tweaks like rounding the corners of the bars in a graph or shading the bar to create the illusion of depth. 

Basically, does anyone know of a software package that can progamatically create good looking charts and graphs?

Thanks!",4
6046793,05/18/2011 14:47:27,410823,08/04/2010 13:22:28,919,70,How to install RMysql on Windows 7 with proxy settings,"I am fighting with the installation of RMysql on R2.13.0 on Windows 7 64bit.  The version of my mysql 5.5. Here are the steps, which led me to an error:

 1. Install RTools
 2. Set up proxy by calling: `setInternet2(TRUE)` after [FAQ@CRAN][1]
 3. Set MYSQL_HOME: `Sys.setenv(""MYSQL_HOME""=""C:\\Program Files\\MySQL\\MySQL Server 5.5\\"")` after [this post on SO][2]
 4. Call in R Console: `install.packages(""RMySQL"", type=""source"")`

After 4, I got the following error:

    Installing package(s) into ‘C:/Users/Foo/Documents/R/win-library/2.13’
    (as ‘lib’ is unspecified)
    Warning: unable to access index for repository http://artfiles.org/cran/src/contrib
    Warning message:
    In getDependencies(pkgs, dependencies, available, lib) :
      package ‘RMySQL’ is not available (for R version 2.13.0)

Alternative to 4 is just to install the package: `install.packages(""RMySQL"")`. In this case I got:

    Installing package(s) into ‘C:/Users/foo/Documents/R/win-library/2.13’
    (as ‘lib’ is unspecified)
    Warning: unable to access index for repository http://artfiles.org/cran/bin/windows/contrib/2.13
    Warning message:
    In getDependencies(pkgs, dependencies, available, lib) :
      package ‘RMySQL’ is not available (for R version 2.13.0)

I am sure that I do not have problems with connection, because I can call R update. 
Could anybody help me to find out how I can access mysql db from R?

ps. Here is output from sessionInfo()

    R version 2.13.0 (2011-04-13)
    Platform: i386-pc-mingw32/i386 (32-bit)
    
    locale:
    [1] LC_COLLATE=German_Germany.1252  LC_CTYPE=German_Germany.1252   
    [3] LC_MONETARY=German_Germany.1252 LC_NUMERIC=C                   
    [5] LC_TIME=German_Germany.1252    
    
    attached base packages:
    [1] stats     graphics  grDevices utils     datasets  methods   base     
    
    loaded via a namespace (and not attached):
    [1] tools_2.13.0


  [1]: http://cran.r-project.org/bin/windows/base/rw-FAQ.html#The-Internet-download-functions-fail_002e
  [2]: http://stackoverflow.com/questions/4785933/adding-rmysql-package-to-r-fails/4792884#4792884",r,,,,,05/20/2011 05:40:29,off topic,1,352,10,"How to install RMysql on Windows 7 with proxy settings I am fighting with the installation of RMysql on R2.13.0 on Windows 7 64bit.  The version of my mysql 5.5. Here are the steps, which led me to an error:

 1. Install RTools
 2. Set up proxy by calling: `setInternet2(TRUE)` after [FAQ@CRAN][1]
 3. Set MYSQL_HOME: `Sys.setenv(""MYSQL_HOME""=""C:\\Program Files\\MySQL\\MySQL Server 5.5\\"")` after [this post on SO][2]
 4. Call in R Console: `install.packages(""RMySQL"", type=""source"")`

After 4, I got the following error:

    Installing package(s) into ‘C:/Users/Foo/Documents/R/win-library/2.13’
    (as ‘lib’ is unspecified)
    Warning: unable to access index for repository http://artfiles.org/cran/src/contrib
    Warning message:
    In getDependencies(pkgs, dependencies, available, lib) :
      package ‘RMySQL’ is not available (for R version 2.13.0)

Alternative to 4 is just to install the package: `install.packages(""RMySQL"")`. In this case I got:

    Installing package(s) into ‘C:/Users/foo/Documents/R/win-library/2.13’
    (as ‘lib’ is unspecified)
    Warning: unable to access index for repository http://artfiles.org/cran/bin/windows/contrib/2.13
    Warning message:
    In getDependencies(pkgs, dependencies, available, lib) :
      package ‘RMySQL’ is not available (for R version 2.13.0)

I am sure that I do not have problems with connection, because I can call R update. 
Could anybody help me to find out how I can access mysql db from R?

ps. Here is output from sessionInfo()

    R version 2.13.0 (2011-04-13)
    Platform: i386-pc-mingw32/i386 (32-bit)
    
    locale:
    [1] LC_COLLATE=German_Germany.1252  LC_CTYPE=German_Germany.1252   
    [3] LC_MONETARY=German_Germany.1252 LC_NUMERIC=C                   
    [5] LC_TIME=German_Germany.1252    
    
    attached base packages:
    [1] stats     graphics  grDevices utils     datasets  methods   base     
    
    loaded via a namespace (and not attached):
    [1] tools_2.13.0


  [1]: http://cran.r-project.org/bin/windows/base/rw-FAQ.html#The-Internet-download-functions-fail_002e
  [2]: http://stackoverflow.com/questions/4785933/adding-rmysql-package-to-r-fails/4792884#4792884",1
11494188,07/15/2012 17:46:58,1527206,07/15/2012 17:18:00,9,3,zoo objects and millisecond timestamps,"quick question on tick data. I have tons of data under this format which I believe is perfect for what I'm trying to achieve. I want to keep some granularity in order to be able to trigger buy/sell signal under a second.

> data

           SYMBOL  TIMESTAMP            STAMP   PRICE  SIZE EXCHANGE   BID BIDEX BIDSIZE   ASK ASKEX ASKSIZE
    1        SPXU 1330938005 1330938005000000      NA    NA       9.99   PSE       5 10.10   PSE       6
    2        SPXU 1330938221 1330938221000000      NA    NA       9.99   PSE       5 10.19   PSE       1
    3        SPXU 1330938221 1330938221000001 10.1000   600      PSE    NA        NA    NA        NA
    4        SPXU 1330938392 1330938392000000      NA    NA      10.00   PSE     174 10.19   PSE       1
    5        SPXU 1330938431 1330938431000000      NA    NA      10.00   PSE     175 10.19   PSE       1
    6        SPXU 1330938468 1330938468000000      NA    NA      10.00   PSE       1 10.19   PSE       1
    7        SPXU 1330938736 1330938736000000      NA    NA      10.04   PSE      46 10.19   PSE       1
    8        SPXU 1330938843 1330938843000000      NA    NA      10.04   PSE      47 10.19   PSE       1
    9        SPXU 1330939576 1330939576000000      NA    NA      10.04   PSE       1 10.19   PSE       1
    10       SPXU 1330939615 1330939615000000      NA    NA      10.05   PSE     100 10.19   PSE       1
    11       SPXU 1330939615 1330939615000001      NA    NA      10.05   PSE     100 10.19   PSE     101
    12       SPXU 1330939621 1330939621000000      NA    NA      10.04   PSE       1 10.19   PSE     101
    13       SPXU 1330939621 1330939621000001      NA    NA      10.04   PSE       1 10.19   PSE       1
    14       SPXU 1330939623 1330939623000000      NA    NA      10.05   PSE      46 10.19   PSE       1
    15       SPXU 1330939623 1330939623000001      NA    NA      10.05   PSE      46 10.18   PSE      46
    16       SPXU 1330939638 1330939638000000      NA    NA      10.04   PSE       1 10.18   PSE      46
    17       SPXU 1330939686 1330939686000000      NA    NA      10.04   PSE       1 10.19   PSE       1
    18       SPXU 1330939825 1330939825000000      NA    NA      10.05   PSE     100 10.19   PSE       1
    19       SPXU 1330939825 1330939825000001      NA    NA      10.05   PSE     100 10.19   PSE     101
    20       SPXU 1330939833 1330939833000000      NA    NA      10.04   PSE       1 10.19   PSE     101
    21       SPXU 1330939833 1330939833000001      NA    NA      10.04   PSE       1 10.19   PSE       1
    22       SPXU 1330939833 1330939833000002      NA    NA      10.04   PSE     101 10.19   PSE       1
    23       SPXU 1330939833 1330939833000003      NA    NA      10.04   PSE     101 10.19   PSE     101
    24       SPXU 1330939941 1330939941000000      NA    NA      10.04   PSE     101 10.19   PSE     102
    25       SPXU 1330940041 1330940041000000      NA    NA      10.04   PSE       1 10.19   PSE     102

I want to be able to have zoo objects created keeping the millisecond granularity. I'm unable to transform the ""data$STAMP"" into a date. How can I do this?

working:

    > as.POSIXlt(data2$TIMESTAMP[3], origin=""1970-01-01"", tz=""EST"")
    [1] ""2012-03-05 04:01:36 EST""

not working:

    > as.POSIXlt(data2$STAMP[3], origin=""1970-01-01"", tz=""EST"")
    [1] ""))0'-06-03 15:45:52 EST""",r,,,,,,open,0,1360,5,"zoo objects and millisecond timestamps quick question on tick data. I have tons of data under this format which I believe is perfect for what I'm trying to achieve. I want to keep some granularity in order to be able to trigger buy/sell signal under a second.

> data

           SYMBOL  TIMESTAMP            STAMP   PRICE  SIZE EXCHANGE   BID BIDEX BIDSIZE   ASK ASKEX ASKSIZE
    1        SPXU 1330938005 1330938005000000      NA    NA       9.99   PSE       5 10.10   PSE       6
    2        SPXU 1330938221 1330938221000000      NA    NA       9.99   PSE       5 10.19   PSE       1
    3        SPXU 1330938221 1330938221000001 10.1000   600      PSE    NA        NA    NA        NA
    4        SPXU 1330938392 1330938392000000      NA    NA      10.00   PSE     174 10.19   PSE       1
    5        SPXU 1330938431 1330938431000000      NA    NA      10.00   PSE     175 10.19   PSE       1
    6        SPXU 1330938468 1330938468000000      NA    NA      10.00   PSE       1 10.19   PSE       1
    7        SPXU 1330938736 1330938736000000      NA    NA      10.04   PSE      46 10.19   PSE       1
    8        SPXU 1330938843 1330938843000000      NA    NA      10.04   PSE      47 10.19   PSE       1
    9        SPXU 1330939576 1330939576000000      NA    NA      10.04   PSE       1 10.19   PSE       1
    10       SPXU 1330939615 1330939615000000      NA    NA      10.05   PSE     100 10.19   PSE       1
    11       SPXU 1330939615 1330939615000001      NA    NA      10.05   PSE     100 10.19   PSE     101
    12       SPXU 1330939621 1330939621000000      NA    NA      10.04   PSE       1 10.19   PSE     101
    13       SPXU 1330939621 1330939621000001      NA    NA      10.04   PSE       1 10.19   PSE       1
    14       SPXU 1330939623 1330939623000000      NA    NA      10.05   PSE      46 10.19   PSE       1
    15       SPXU 1330939623 1330939623000001      NA    NA      10.05   PSE      46 10.18   PSE      46
    16       SPXU 1330939638 1330939638000000      NA    NA      10.04   PSE       1 10.18   PSE      46
    17       SPXU 1330939686 1330939686000000      NA    NA      10.04   PSE       1 10.19   PSE       1
    18       SPXU 1330939825 1330939825000000      NA    NA      10.05   PSE     100 10.19   PSE       1
    19       SPXU 1330939825 1330939825000001      NA    NA      10.05   PSE     100 10.19   PSE     101
    20       SPXU 1330939833 1330939833000000      NA    NA      10.04   PSE       1 10.19   PSE     101
    21       SPXU 1330939833 1330939833000001      NA    NA      10.04   PSE       1 10.19   PSE       1
    22       SPXU 1330939833 1330939833000002      NA    NA      10.04   PSE     101 10.19   PSE       1
    23       SPXU 1330939833 1330939833000003      NA    NA      10.04   PSE     101 10.19   PSE     101
    24       SPXU 1330939941 1330939941000000      NA    NA      10.04   PSE     101 10.19   PSE     102
    25       SPXU 1330940041 1330940041000000      NA    NA      10.04   PSE       1 10.19   PSE     102

I want to be able to have zoo objects created keeping the millisecond granularity. I'm unable to transform the ""data$STAMP"" into a date. How can I do this?

working:

    > as.POSIXlt(data2$TIMESTAMP[3], origin=""1970-01-01"", tz=""EST"")
    [1] ""2012-03-05 04:01:36 EST""

not working:

    > as.POSIXlt(data2$STAMP[3], origin=""1970-01-01"", tz=""EST"")
    [1] ""))0'-06-03 15:45:52 EST""",1
5267955,03/11/2011 01:13:47,375267,06/24/2010 12:54:14,57,2,Simple apply question,"I couldn't find a solution for this problem online, as simple as it seems. Here's it is: 

    #Construct test dataframe 
    tf <- data.frame(1:3,4:6,c(""A"",""A"",""A"")) 

    #Try the apply function I'm trying to use
    test <- apply(tf,2,function(x) if(is.numeric(x)) mean(x) else unique(x)[1]) 

    #Look at the output--all columns treated as character columns...
    test

    #Look at the format of the original data--the first two columns are integers. 
    str(tf) 


In general terms, I want to differentiate what function I apply over a row/column based on what type of data that row/column contains. Here I want a simple mean if the column is numeric and the first unique value if the column is a character column. As you can see, 'apply' treats all columns as characters the way I've written this function. 

Any thoughts? Many thanks in advance, 

Aaron 
",r,,,,,,open,0,161,3,"Simple apply question I couldn't find a solution for this problem online, as simple as it seems. Here's it is: 

    #Construct test dataframe 
    tf <- data.frame(1:3,4:6,c(""A"",""A"",""A"")) 

    #Try the apply function I'm trying to use
    test <- apply(tf,2,function(x) if(is.numeric(x)) mean(x) else unique(x)[1]) 

    #Look at the output--all columns treated as character columns...
    test

    #Look at the format of the original data--the first two columns are integers. 
    str(tf) 


In general terms, I want to differentiate what function I apply over a row/column based on what type of data that row/column contains. Here I want a simple mean if the column is numeric and the first unique value if the column is a character column. As you can see, 'apply' treats all columns as characters the way I've written this function. 

Any thoughts? Many thanks in advance, 

Aaron 
",1
7652185,10/04/2011 18:07:38,851043,07/19/2011 00:35:07,72,1,'x' must be a numeric vector: Error from data.frame of numbers,"I am running a cor.test on two columns within a file/table.  

    tmp <- read.table(files_to_test[i], header=TRUE, sep=""\t"")
    ## Obtain Columns To Compare ##
    colA <-tmp[compareA]
    colB <-tmp[compareB]
    # sctr = 'spearman cor.test result'
    sctr <- cor.test(colA, colB, alternative=""two.sided"", method=""spearman"")

But I am getting this confounding error...

    Error in cor.test.default(colA, colB, alternative = ""two.sided"", method = ""spearman"") : 
    'x' must be a numeric vector

the values in the columns ARE numbers but

    is.numeric(colA) = FALSE 
    class (colA) = data.frame

What have I missed?",r,data.frame,,,,,open,0,112,11,"'x' must be a numeric vector: Error from data.frame of numbers I am running a cor.test on two columns within a file/table.  

    tmp <- read.table(files_to_test[i], header=TRUE, sep=""\t"")
    ## Obtain Columns To Compare ##
    colA <-tmp[compareA]
    colB <-tmp[compareB]
    # sctr = 'spearman cor.test result'
    sctr <- cor.test(colA, colB, alternative=""two.sided"", method=""spearman"")

But I am getting this confounding error...

    Error in cor.test.default(colA, colB, alternative = ""two.sided"", method = ""spearman"") : 
    'x' must be a numeric vector

the values in the columns ARE numbers but

    is.numeric(colA) = FALSE 
    class (colA) = data.frame

What have I missed?",2
11499909,07/16/2012 07:33:54,525563,11/30/2010 18:32:21,279,3,R flip XY axis on a plot,"This seems like a trivial `R` question, but I didn't find any convincing solution. I would like to flip my plot where the X axis become Y, and vice-versa. In boxplot there is an `horiz=""T""` option, but not in `plot()`.  

This is what I plot :

    plot(rm, type=""l"", main=""CpG - running window 100"")

    > str(rm)
     num [1:43631] 0.667 0.673 0.679 0.685 0.691 ...

![This is what I have][1]


And I would like to obtain this : 

![enter image description here][2]


  [1]: http://i.stack.imgur.com/wXdVz.png
  [2]: http://i.stack.imgur.com/hyEKv.png

Thanks for the feedback.",r,plot,axis,flip,,,open,0,98,7,"R flip XY axis on a plot This seems like a trivial `R` question, but I didn't find any convincing solution. I would like to flip my plot where the X axis become Y, and vice-versa. In boxplot there is an `horiz=""T""` option, but not in `plot()`.  

This is what I plot :

    plot(rm, type=""l"", main=""CpG - running window 100"")

    > str(rm)
     num [1:43631] 0.667 0.673 0.679 0.685 0.691 ...

![This is what I have][1]


And I would like to obtain this : 

![enter image description here][2]


  [1]: http://i.stack.imgur.com/wXdVz.png
  [2]: http://i.stack.imgur.com/hyEKv.png

Thanks for the feedback.",4
6181653,05/30/2011 23:44:16,615304,02/13/2011 18:36:32,33,0,"creating a more continuous color palette in r, ggplot2, lattice, or latticeExtra","Warning.... very novice question follows:

I am trying to plot a fairly regular distribution of several thousand (X,Y) points each associated with a value, let's call Z, which varies very irregularly between, say, -20 to +20. I am not interested in smoothing; I want the point Z values to plot according to a smoothly varying color palette much like Gnuplot can do with the proper smooth color palette. I've tried base R, ggplot2, and latticeExtra, and as best I can, I can come up with the following which does almost what I want:

library(lattice)
library(latticeExtra)
library(colorRamps)
df = read.table(file""whatever"", header=T)
levelplot(Z~X*Y, df, panel=panel.levelplot.points, cex=0.2, col.regions=colorRampPalette(c(""red"",""white"",""blue""))(50))

One data point looks like:  1302525 225167 -3.5

When I plot my dataframe with the ""50"" in the last code line as 3, I get the predictable R recycle behavior of the red, white, and blue colors repeating five times with the 16th color bar segment white. Changing the 3 to a 7 causes more shades of red and blue creating 2 repeat color range segments with two reddish colors left over as the color range tries to recycle. This suggests making this number larger causes a finer graduation of colors. But if I put in a number greater than 16, that's all I get, 16 colored segments, evenly changing from red, to white, to blue. But I'd like the color scale even finer, and in a perfect world, force a Z of zero to be the white color. 

My experience so far with R is when I can't do something as simple as this, I'm missing a very fundamental concept. What is it?


",r,ggplot2,lattice,,,,open,0,262,12,"creating a more continuous color palette in r, ggplot2, lattice, or latticeExtra Warning.... very novice question follows:

I am trying to plot a fairly regular distribution of several thousand (X,Y) points each associated with a value, let's call Z, which varies very irregularly between, say, -20 to +20. I am not interested in smoothing; I want the point Z values to plot according to a smoothly varying color palette much like Gnuplot can do with the proper smooth color palette. I've tried base R, ggplot2, and latticeExtra, and as best I can, I can come up with the following which does almost what I want:

library(lattice)
library(latticeExtra)
library(colorRamps)
df = read.table(file""whatever"", header=T)
levelplot(Z~X*Y, df, panel=panel.levelplot.points, cex=0.2, col.regions=colorRampPalette(c(""red"",""white"",""blue""))(50))

One data point looks like:  1302525 225167 -3.5

When I plot my dataframe with the ""50"" in the last code line as 3, I get the predictable R recycle behavior of the red, white, and blue colors repeating five times with the 16th color bar segment white. Changing the 3 to a 7 causes more shades of red and blue creating 2 repeat color range segments with two reddish colors left over as the color range tries to recycle. This suggests making this number larger causes a finer graduation of colors. But if I put in a number greater than 16, that's all I get, 16 colored segments, evenly changing from red, to white, to blue. But I'd like the color scale even finer, and in a perfect world, force a Z of zero to be the white color. 

My experience so far with R is when I can't do something as simple as this, I'm missing a very fundamental concept. What is it?


",3
8896778,01/17/2012 14:57:07,976991,10/03/2011 15:47:15,29,0,heatmaps with huge data,"I'm trying to cluster data  and then plotting a heatmap using heatmap.2 from ggplot, the script works perfectly with matrices up to 30000 rows, the problems is that I'm using matrices up to 500000 rows (data_sel), and when I try to cluster I get this error:
    
    heatmap.2(as.matrix(data_sel),col=greenred(10), trace=""none"",cexRow=0.3, cexCol=0.3,  ColSideColors=fenot.colour, margins=c(20,1), labCol="""", labRow="""",distfun=function(x) dist(x,method=""manhattan""))
    Error in vector(""double"", length) : vector size specified is too large
    
Is there any approximation using R to plot heatmpas with his big data?

Thanks in advance",r,heatmap,ggplot,,,,open,0,95,4,"heatmaps with huge data I'm trying to cluster data  and then plotting a heatmap using heatmap.2 from ggplot, the script works perfectly with matrices up to 30000 rows, the problems is that I'm using matrices up to 500000 rows (data_sel), and when I try to cluster I get this error:
    
    heatmap.2(as.matrix(data_sel),col=greenred(10), trace=""none"",cexRow=0.3, cexCol=0.3,  ColSideColors=fenot.colour, margins=c(20,1), labCol="""", labRow="""",distfun=function(x) dist(x,method=""manhattan""))
    Error in vector(""double"", length) : vector size specified is too large
    
Is there any approximation using R to plot heatmpas with his big data?

Thanks in advance",3
4972105,02/11/2011 17:26:50,511548,11/18/2010 01:37:27,13,1,Specifying xlim and ylim when using log-scale in R,"I'm trying to specify the lower and upper range for the x- and y-axis for a log-scale plot. I thought I could use xlim and ylim, but I receive a warning message and no plot:

1: In plot.window(...) :
  nonfinite axis limits [GScale(-inf,3.30103,1, .); log=1]

Here is my code.

<code>
plot(FAS_set$ConcCalc~ZCS_set$ConcCalc,pch=21,bg=""black"",log=""xy"",xlim=c(0,2000),ylim=c(0,100000))
</code>

Any help would be appreciated.

Cheers.
",r,plot,,,,,open,0,52,9,"Specifying xlim and ylim when using log-scale in R I'm trying to specify the lower and upper range for the x- and y-axis for a log-scale plot. I thought I could use xlim and ylim, but I receive a warning message and no plot:

1: In plot.window(...) :
  nonfinite axis limits [GScale(-inf,3.30103,1, .); log=1]

Here is my code.

<code>
plot(FAS_set$ConcCalc~ZCS_set$ConcCalc,pch=21,bg=""black"",log=""xy"",xlim=c(0,2000),ylim=c(0,100000))
</code>

Any help would be appreciated.

Cheers.
",2
11513953,07/16/2012 23:15:20,1515626,07/10/2012 17:46:23,28,2,Creating contingency tables from recoded variables (atomic vectors),"Another boneheaded request. I am trying to create contingency tables using recoded variables where any answer is coded as ""1"" and non-answers are coded as ""0."" 

My original data might have looked like this: some variables are recoded from character strings, whereas others are recoded from numbers.

    id   var1       recode    var2    recode2 
    1    ""hello""     1         1         1      
    2    ""hi""        1         <NA>      0      
    3                0         <NA>      0      
    4     ""hola""     1         1         1       

I have written a bit of code to do this recoding of strings, which I check using a contingency table.

    data$recode <- ifelse((as.numeric(data$var1)!=1), 1, 0) #RECODES STRINGS
    table(data$recode)
        0     1
        1     3

But then, I also need to recode the NA's in all of my other variables to be 0.  I tried to do this with another ifelse statement:

     data <- ifelse(is.na(data), 0, 1)

The values seem to change, but now when I try to run the same contingency table, I get the following error message:

      Error in data$recode : $ operator is invalid for atomic vectors


I actually need to be able to produce contingency tables for all of my variables (i.e. report percentages and frequencies), so help on how to correctly recode all of my NA's (within a range of columns) into 0 so would be very helpful. Thanks!",r,,,,,,open,0,399,8,"Creating contingency tables from recoded variables (atomic vectors) Another boneheaded request. I am trying to create contingency tables using recoded variables where any answer is coded as ""1"" and non-answers are coded as ""0."" 

My original data might have looked like this: some variables are recoded from character strings, whereas others are recoded from numbers.

    id   var1       recode    var2    recode2 
    1    ""hello""     1         1         1      
    2    ""hi""        1         <NA>      0      
    3                0         <NA>      0      
    4     ""hola""     1         1         1       

I have written a bit of code to do this recoding of strings, which I check using a contingency table.

    data$recode <- ifelse((as.numeric(data$var1)!=1), 1, 0) #RECODES STRINGS
    table(data$recode)
        0     1
        1     3

But then, I also need to recode the NA's in all of my other variables to be 0.  I tried to do this with another ifelse statement:

     data <- ifelse(is.na(data), 0, 1)

The values seem to change, but now when I try to run the same contingency table, I get the following error message:

      Error in data$recode : $ operator is invalid for atomic vectors


I actually need to be able to produce contingency tables for all of my variables (i.e. report percentages and frequencies), so help on how to correctly recode all of my NA's (within a range of columns) into 0 so would be very helpful. Thanks!",1
11709539,07/29/2012 13:16:06,1442363,06/07/2012 13:40:16,10,0,Should Categorical predictors within a linear model be normally distributed?,"I am running simple linear models(Y~X) in R where my predictor is a categorical variable (0-10). However, this variable is not normally distributed and none of the transformation techniques available are healpful (e.g. log, sq etc.) as the data is not negatively/positively skewed but rather all over the place. I am aware that for lm the outcome variable (Y) has to be normally distributed but is this also required for predictors? If yes, any suggestions of how to do this would be more than welcome.

Also, as the data I am looking at has two groups, patients vs controls (I am interested in group differences, as you can guess), do I have to look at whether the data is normally distributed within the two groups or overall across the two groups?  
Thanks.



  [1]: http://i.stack.imgur.com/RRGNU.png",r,normal-distribution,lm,,,07/31/2012 07:36:26,off topic,1,135,10,"Should Categorical predictors within a linear model be normally distributed? I am running simple linear models(Y~X) in R where my predictor is a categorical variable (0-10). However, this variable is not normally distributed and none of the transformation techniques available are healpful (e.g. log, sq etc.) as the data is not negatively/positively skewed but rather all over the place. I am aware that for lm the outcome variable (Y) has to be normally distributed but is this also required for predictors? If yes, any suggestions of how to do this would be more than welcome.

Also, as the data I am looking at has two groups, patients vs controls (I am interested in group differences, as you can guess), do I have to look at whether the data is normally distributed within the two groups or overall across the two groups?  
Thanks.



  [1]: http://i.stack.imgur.com/RRGNU.png",3
8898521,01/17/2012 16:53:33,1142145,01/11/2012 01:42:10,11,0,Finding 2 & 3 word Phrases Using R TM Package,"I am trying to find a code that actually works to find the most frequently used two and three word phrases in R text mining package (maybe there is another package for it that I do not know). I have been trying to use the tokenizer, but seem to have no luck. 

If you worked on a similar situation in the past, could you post a code that is tested and actually works? Thank you so much!
",r,data-mining,text-mining,,,,open,0,77,10,"Finding 2 & 3 word Phrases Using R TM Package I am trying to find a code that actually works to find the most frequently used two and three word phrases in R text mining package (maybe there is another package for it that I do not know). I have been trying to use the tokenizer, but seem to have no luck. 

If you worked on a similar situation in the past, could you post a code that is tested and actually works? Thank you so much!
",3
10149571,04/13/2012 23:28:44,903061,08/19/2011 20:07:36,924,24,A way to always dodge a histogram?,"Using ggplot2 I'm creating a histogram with a factor on the horizontal axis and another factor for the fill color, using a dodged position. My problem is that the fill factor sometimes takes only one value for a value of the horizontal factor, and with nothing to dodge the bar takes up the full width. Is there a way to make it dodge nothing so that all bar widths are the same? Or equivalently to plot the 0's?

For example

    ggplot(data = mtcars, aes(x = factor(carb), fill = factor(gear)) +
    geom_histogram(position = ""dodge"")

![enter image description here][1]

[This answer](http://stackoverflow.com/a/9101710/903061) has a couple ideas. It was also asked before the new version was released, so maybe something changed? Using facets (also shown [here](http://stackoverflow.com/a/7104847/903061)) I don't like for my situation, though I suppose editing the data and using `geom_bar` could work, but it feels inelegant. Moreover, when I tried facetting anyway

    ggplot(mtcars, aes(x = factor(carb), fill = factor(gear))) +
        geom_bar() + facet_grid(~factor(carb))

I get the error ""Error in layout_base(data, cols, drop = drop): 
  At least one layer must contain all variables used for facetting""

I suppose I could generate a data frame of counts and then use `geom_bar`,

    mtcounts <- ddply(subset(mtcars, select = c(""carb"", ""gear"")),
        .fun = count, .variables = c(""carb"", ""gear""))

filling out the levels that aren't present with 0's. Does anyone know if that would work or if there's a better way?

  [1]: http://i.stack.imgur.com/ezqc7.png",r,ggplot2,,,,,open,0,257,7,"A way to always dodge a histogram? Using ggplot2 I'm creating a histogram with a factor on the horizontal axis and another factor for the fill color, using a dodged position. My problem is that the fill factor sometimes takes only one value for a value of the horizontal factor, and with nothing to dodge the bar takes up the full width. Is there a way to make it dodge nothing so that all bar widths are the same? Or equivalently to plot the 0's?

For example

    ggplot(data = mtcars, aes(x = factor(carb), fill = factor(gear)) +
    geom_histogram(position = ""dodge"")

![enter image description here][1]

[This answer](http://stackoverflow.com/a/9101710/903061) has a couple ideas. It was also asked before the new version was released, so maybe something changed? Using facets (also shown [here](http://stackoverflow.com/a/7104847/903061)) I don't like for my situation, though I suppose editing the data and using `geom_bar` could work, but it feels inelegant. Moreover, when I tried facetting anyway

    ggplot(mtcars, aes(x = factor(carb), fill = factor(gear))) +
        geom_bar() + facet_grid(~factor(carb))

I get the error ""Error in layout_base(data, cols, drop = drop): 
  At least one layer must contain all variables used for facetting""

I suppose I could generate a data frame of counts and then use `geom_bar`,

    mtcounts <- ddply(subset(mtcars, select = c(""carb"", ""gear"")),
        .fun = count, .variables = c(""carb"", ""gear""))

filling out the levels that aren't present with 0's. Does anyone know if that would work or if there's a better way?

  [1]: http://i.stack.imgur.com/ezqc7.png",2
7009711,08/10/2011 10:59:10,366256,06/14/2010 11:03:44,1806,61,How to get years from out of a ts index when the underlying time series is of monthly freq?,"I´d like to extract years from a ts index (the underlying ts is of monthly frequency). The reason I want to do it is creating a yearly axis, e.g.

    plot(myts)
    axis(1, at = year(time(myts)), labels = FALSE)
    # note I know 'year()' does not work :)

because if I just plot it, R arbitrarily(?) creates a time axis. Often it's a two or even 5 year axis which makes is inappropriate sometimes. ",r,plot,time-series,,,,open,0,81,19,"How to get years from out of a ts index when the underlying time series is of monthly freq? I´d like to extract years from a ts index (the underlying ts is of monthly frequency). The reason I want to do it is creating a yearly axis, e.g.

    plot(myts)
    axis(1, at = year(time(myts)), labels = FALSE)
    # note I know 'year()' does not work :)

because if I just plot it, R arbitrarily(?) creates a time axis. Often it's a two or even 5 year axis which makes is inappropriate sometimes. ",3
520810,02/06/2009 15:49:48,63372,02/06/2009 15:45:33,1,0,Does R have quote-like operators like Perl's q()?,Anyone know if R has quote-like operators like Perl's q() for generating character vectors? ,r,perl,,,,,open,0,15,8,Does R have quote-like operators like Perl's q()? Anyone know if R has quote-like operators like Perl's q() for generating character vectors? ,2
10112149,04/11/2012 18:54:25,1327367,04/11/2012 18:34:45,1,0,"error with zoo package, data too long?","I want to generate a new column with a running mean of a specified bin width and for that I used the zoo package (rollmean function). My datasheet consists of 1 million rows, and the function applies to the first 500,000, after that it is all NAs produced, any ideas what I may be doing wrong?


    library(""zoo"")

    HB<-bin/2
    n<-length(dataraw$S)
    data<-dataraw[HB:(n-HB),]
    sax<- rollmean (dataraw$S, bin)
    data2<-cbind(data,sax)



I would aappreciate any help you could give me, I've been stuck with this for far too long. Thanks in advance.",r,zoo,na,,,,open,0,103,7,"error with zoo package, data too long? I want to generate a new column with a running mean of a specified bin width and for that I used the zoo package (rollmean function). My datasheet consists of 1 million rows, and the function applies to the first 500,000, after that it is all NAs produced, any ideas what I may be doing wrong?


    library(""zoo"")

    HB<-bin/2
    n<-length(dataraw$S)
    data<-dataraw[HB:(n-HB),]
    sax<- rollmean (dataraw$S, bin)
    data2<-cbind(data,sax)



I would aappreciate any help you could give me, I've been stuck with this for far too long. Thanks in advance.",3
7269377,09/01/2011 11:02:04,923322,09/01/2011 11:02:04,1,0,Function for certain values in rows,"I have a paneldata which looks like: 

(Only the substantially cutting for my question)

Persno 122 122 122 333 333 333 333 333 444 444 

Income 1500 1500 2000 2000 2100 2500 2500 1500 2000 2200

year 1 2 3 1 2 3 4 5 1 2

I need a command or function to  recognises the different Persno. For all rows with the same Persn I would like to give out the average income.

Thank you very much.

",r,,,,,,open,0,75,6,"Function for certain values in rows I have a paneldata which looks like: 

(Only the substantially cutting for my question)

Persno 122 122 122 333 333 333 333 333 444 444 

Income 1500 1500 2000 2000 2100 2500 2500 1500 2000 2200

year 1 2 3 1 2 3 4 5 1 2

I need a command or function to  recognises the different Persno. For all rows with the same Persn I would like to give out the average income.

Thank you very much.

",1
6968127,08/06/2011 16:23:54,318870,04/16/2010 20:29:22,1091,49,How can I dynamically regress and predict multiple items with R?,"I'm trying to write a function that regresses multiple items, then tries to predict data based on the model:

	""tnt"" <- function(train_dep, train_indep, test_dep, test_indep) 
	{
		y <- train_dep
		x <- train_indep
		mod <- lm (y ~ x)
		estimate <- predict(mod, data.frame(x=test_indep))
		rmse <- sqrt(sum((test_dep-estimate)^2)/length(test_dep)) 
		print(summary(mod))
		print(paste(""RMSE: "", rmse))		
	}

If I pass the above this, it fails:

	train_dep = vector1
	train_indep <- cbind(vector2, vector3)
	test_dep = vector4
	test_indep <- cbind(vector5, vector6)
	tnt(train_dep, train_indep, test_dep, test_indep)

Changing the above to something like the following works, but I want this done dynamically so I can pass it a matrix of any number of columns:

    x1 = x[,1]
    x2 = x[,2]
    mod <- lm(y ~ x1+x2)
    estimate <- predict(mod, data.frame(x1=test_indep[,1], x2=test_indep[,2]))

Looks like this could help, but I'm still confused on the rest of the process: http://finzi.psych.upenn.edu/R/Rhelp02a/archive/70843.html

",r,,,,,,open,0,131,11,"How can I dynamically regress and predict multiple items with R? I'm trying to write a function that regresses multiple items, then tries to predict data based on the model:

	""tnt"" <- function(train_dep, train_indep, test_dep, test_indep) 
	{
		y <- train_dep
		x <- train_indep
		mod <- lm (y ~ x)
		estimate <- predict(mod, data.frame(x=test_indep))
		rmse <- sqrt(sum((test_dep-estimate)^2)/length(test_dep)) 
		print(summary(mod))
		print(paste(""RMSE: "", rmse))		
	}

If I pass the above this, it fails:

	train_dep = vector1
	train_indep <- cbind(vector2, vector3)
	test_dep = vector4
	test_indep <- cbind(vector5, vector6)
	tnt(train_dep, train_indep, test_dep, test_indep)

Changing the above to something like the following works, but I want this done dynamically so I can pass it a matrix of any number of columns:

    x1 = x[,1]
    x2 = x[,2]
    mod <- lm(y ~ x1+x2)
    estimate <- predict(mod, data.frame(x1=test_indep[,1], x2=test_indep[,2]))

Looks like this could help, but I'm still confused on the rest of the process: http://finzi.psych.upenn.edu/R/Rhelp02a/archive/70843.html

",1
127137,09/24/2008 13:31:07,277,08/04/2008 10:55:22,707,23,Sample Code for R?,"Does anyone know a good online resource for example of R code?

The programs do not have to be written for illustrative purposes, I am really just looking for some places where a bunch of R code has been written to give me a sense of the syntax and capabilities of the language?",r,,,,,03/31/2012 23:40:40,not constructive,1,52,4,"Sample Code for R? Does anyone know a good online resource for example of R code?

The programs do not have to be written for illustrative purposes, I am really just looking for some places where a bunch of R code has been written to give me a sense of the syntax and capabilities of the language?",1
7016506,08/10/2011 19:21:54,564637,01/05/2011 21:54:08,53,3,Poblem installing packages into R,"Please I am trying to install two packages in R Vegan and RecordLinkage I keep on getting error because I guess I was on a n Ubuntu 9.10 machine. I reinstalled the machine and I am still finding it difficult to install.

The vegan says  Segmentation Fault out of memory
While RecordLinkage has a problem with lazy loading a package called data.table.

Anyone encountered this yet.",r,installation,,,,08/11/2011 14:01:06,too localized,1,64,5,"Poblem installing packages into R Please I am trying to install two packages in R Vegan and RecordLinkage I keep on getting error because I guess I was on a n Ubuntu 9.10 machine. I reinstalled the machine and I am still finding it difficult to install.

The vegan says  Segmentation Fault out of memory
While RecordLinkage has a problem with lazy loading a package called data.table.

Anyone encountered this yet.",2
3695310,09/12/2010 15:08:27,308883,04/04/2010 22:08:35,16,0,Nested while loop behavior in R,"I am puzzled by why the output is not what I expect it to be in the following nested while loops:

    i = 1
    j = 1
    while(i<5){
     print(""i"")
     print(i)
     i = i + 1
     while(j<5){
      print(""j"")
      print(j)
      j = j + 1
     }
    }

The output I get is:

    [1] ""i""
    [1] 1
    [1] ""j""
    [1] 1
    [1] ""j""
    [1] 2
    [1] ""j""
    [1] 3
    [1] ""j""
    [1] 4
    [1] ""i""
    [1] 2
    [1] ""i""
    [1] 3
    [1] ""i""
    [1] 4

But I was expecting something along the lines of 



    [1] ""i""
    [1] 1
    [1] ""j""
    [1] 1
    [1] ""j""
    [1] 2
    [1] ""j""
    [1] 3
    [1] ""j""
    [1] 4
    [1] ""i""
    [1] 2
    [1] ""j""
    [1] 1
    [1] ""j""
    [1] 2
    [1] ""j""
    [1] 3
    [1] ""j""
    [1] 4
    ...

Any suggestions? Thank you for your help.

",r,while-loops,cran,,,,open,0,295,6,"Nested while loop behavior in R I am puzzled by why the output is not what I expect it to be in the following nested while loops:

    i = 1
    j = 1
    while(i<5){
     print(""i"")
     print(i)
     i = i + 1
     while(j<5){
      print(""j"")
      print(j)
      j = j + 1
     }
    }

The output I get is:

    [1] ""i""
    [1] 1
    [1] ""j""
    [1] 1
    [1] ""j""
    [1] 2
    [1] ""j""
    [1] 3
    [1] ""j""
    [1] 4
    [1] ""i""
    [1] 2
    [1] ""i""
    [1] 3
    [1] ""i""
    [1] 4

But I was expecting something along the lines of 



    [1] ""i""
    [1] 1
    [1] ""j""
    [1] 1
    [1] ""j""
    [1] 2
    [1] ""j""
    [1] 3
    [1] ""j""
    [1] 4
    [1] ""i""
    [1] 2
    [1] ""j""
    [1] 1
    [1] ""j""
    [1] 2
    [1] ""j""
    [1] 3
    [1] ""j""
    [1] 4
    ...

Any suggestions? Thank you for your help.

",3
8774403,01/08/2012 00:40:13,914308,08/26/2011 14:38:22,892,10,R sampling to get around randomForest 32 factor limit,"I'm trying to work around the randomForest package limit of 32 levels for factors.  

I have a data set with 100 factors in one of the variables.  

I wrote the following code to see what things would look like using sampling WITH replacement and how many tries it would take to get certain % of factors selected.

    sampAll <- c()
    nums1 <- seq(1,102,1)
    for(i in 1:20){
    samp1 <- sample(nums1, 32)
    sampAll <- unique(cbind(sampAll, samp1))
    outSamp1 <- nums1[-(sampAll[,1:ncol(sampAll)])]
    print(paste(i, "" | Remaining: "",length(outSamp1)/102,sep=""""))
    flush.console()
    }

    [1] ""1 | Remaining: 0.686274509803922""
    [1] ""2 | Remaining: 0.490196078431373""
    [1] ""3 | Remaining: 0.333333333333333""
    [1] ""4 | Remaining: 0.254901960784314""
    [1] ""5 | Remaining: 0.215686274509804""
    [1] ""6 | Remaining: 0.147058823529412""
    [1] ""7 | Remaining: 0.117647058823529""
    [1] ""8 | Remaining: 0.0980392156862745""
    [1] ""9 | Remaining: 0.0784313725490196""
    [1] ""10 | Remaining: 0.0784313725490196""
    [1] ""11 | Remaining: 0.0490196078431373""
    [1] ""12 | Remaining: 0.0294117647058824""
    [1] ""13 | Remaining: 0.0196078431372549""
    [1] ""14 | Remaining: 0.00980392156862745""
    [1] ""15 | Remaining: 0.00980392156862745""
    [1] ""16 | Remaining: 0.00980392156862745""
    [1] ""17 | Remaining: 0.00980392156862745""
    [1] ""18 | Remaining: 0""
    [1] ""19 | Remaining: 0""
    [1] ""20 | Remaining: 0""

What I'm debating is whether to sample with or without replacement.  

I'm thinking about:

    1) getting a sample of 32 of the 100 factors, 
    2) using those lines to run the randomForest, 
    3) predicting the test set with the randomForest and 
    4) repeating this process either
         a) 3(WITHOUT replacement) or 
         b) 10-15 times (WITH replacement).  
    5) taking the 3 or 10-15 predicted values, finding the average and using that as a final predictor.

I'm curious if anyone has tried something like this or if I'm breaking any rules (introducing bias, etc.) or if anyone has any suggestions.",r,sampling,resampling,random-forest,,01/08/2012 17:13:33,off topic,1,406,9,"R sampling to get around randomForest 32 factor limit I'm trying to work around the randomForest package limit of 32 levels for factors.  

I have a data set with 100 factors in one of the variables.  

I wrote the following code to see what things would look like using sampling WITH replacement and how many tries it would take to get certain % of factors selected.

    sampAll <- c()
    nums1 <- seq(1,102,1)
    for(i in 1:20){
    samp1 <- sample(nums1, 32)
    sampAll <- unique(cbind(sampAll, samp1))
    outSamp1 <- nums1[-(sampAll[,1:ncol(sampAll)])]
    print(paste(i, "" | Remaining: "",length(outSamp1)/102,sep=""""))
    flush.console()
    }

    [1] ""1 | Remaining: 0.686274509803922""
    [1] ""2 | Remaining: 0.490196078431373""
    [1] ""3 | Remaining: 0.333333333333333""
    [1] ""4 | Remaining: 0.254901960784314""
    [1] ""5 | Remaining: 0.215686274509804""
    [1] ""6 | Remaining: 0.147058823529412""
    [1] ""7 | Remaining: 0.117647058823529""
    [1] ""8 | Remaining: 0.0980392156862745""
    [1] ""9 | Remaining: 0.0784313725490196""
    [1] ""10 | Remaining: 0.0784313725490196""
    [1] ""11 | Remaining: 0.0490196078431373""
    [1] ""12 | Remaining: 0.0294117647058824""
    [1] ""13 | Remaining: 0.0196078431372549""
    [1] ""14 | Remaining: 0.00980392156862745""
    [1] ""15 | Remaining: 0.00980392156862745""
    [1] ""16 | Remaining: 0.00980392156862745""
    [1] ""17 | Remaining: 0.00980392156862745""
    [1] ""18 | Remaining: 0""
    [1] ""19 | Remaining: 0""
    [1] ""20 | Remaining: 0""

What I'm debating is whether to sample with or without replacement.  

I'm thinking about:

    1) getting a sample of 32 of the 100 factors, 
    2) using those lines to run the randomForest, 
    3) predicting the test set with the randomForest and 
    4) repeating this process either
         a) 3(WITHOUT replacement) or 
         b) 10-15 times (WITH replacement).  
    5) taking the 3 or 10-15 predicted values, finding the average and using that as a final predictor.

I'm curious if anyone has tried something like this or if I'm breaking any rules (introducing bias, etc.) or if anyone has any suggestions.",4
5195070,03/04/2011 14:32:37,632777,02/24/2011 17:16:03,1,0,Kernlab: Relevance Vector Machine and Gaussian Process Regression Prediction Error..,"Does anyone know how to construct a confidence interval for predicting a new test value given a trained Relevance Vector Machine (rvm) and/or Gaussian Process Regression (gausspr) using Kernlab?

More specifically: how do i get:
1) The standard error/deviation (variance) of a new test point?
2) The parameters estimates of posterior distribution of the parameters?

If anyone know of a document that discuss how to obtain/calculate the above, from the output of function (rvm, or gausspr) call, i would appreciate if u can send me a link

Cheers
",r,bayesian,kernlab,,,03/04/2011 17:54:56,off topic,1,83,10,"Kernlab: Relevance Vector Machine and Gaussian Process Regression Prediction Error.. Does anyone know how to construct a confidence interval for predicting a new test value given a trained Relevance Vector Machine (rvm) and/or Gaussian Process Regression (gausspr) using Kernlab?

More specifically: how do i get:
1) The standard error/deviation (variance) of a new test point?
2) The parameters estimates of posterior distribution of the parameters?

If anyone know of a document that discuss how to obtain/calculate the above, from the output of function (rvm, or gausspr) call, i would appreciate if u can send me a link

Cheers
",3
7302695,09/04/2011 23:09:53,457898,01/29/2010 20:24:01,3715,117,Switch-like function for questionnaire grading,"I'd done a serious PHP/JS coding recently, and I kind-of lost my R muscle. While this problem can be easily tackled within PHP/JS, what is the most efficient way of solving this one: I have to grade a questionnaire, and I have following scenario:

    raw    t
    5      0
    6      2
    7-9    3
    10-12  4
    15-20  5

if `x` equals to, or is within range given in `raw`, value in according row in `t` should be returned. Of course, this can be done with `for` loop, or `switch`, but just imagine very lengthy set of value ranges in `raw`. How would you tackle this one?",r,for-loop,switch-statement,questionnaire,,,open,0,139,5,"Switch-like function for questionnaire grading I'd done a serious PHP/JS coding recently, and I kind-of lost my R muscle. While this problem can be easily tackled within PHP/JS, what is the most efficient way of solving this one: I have to grade a questionnaire, and I have following scenario:

    raw    t
    5      0
    6      2
    7-9    3
    10-12  4
    15-20  5

if `x` equals to, or is within range given in `raw`, value in according row in `t` should be returned. Of course, this can be done with `for` loop, or `switch`, but just imagine very lengthy set of value ranges in `raw`. How would you tackle this one?",4
10269576,04/22/2012 16:05:19,1005262,10/20/2011 13:11:23,1,0,How to HEATMAP the overall GC/AT content of several sequences with R or BioPerl?,"The situation is the following:

I have several DNA FASTA sequences that want to know the GC/AT content using for example a sliding window of 10bp. The resulting heatmap it has to represent the overall resulting sequence with the GC/AT content in every 10bp of all sequences (like an average).

For example the first row of heatmap could be GC content and second row the AT content

This can be possible to do it with R? I also thank suggestions in BioPerl.

Thanks for any help you could provide.",r,heatmap,fasta,bioperl,dna,04/24/2012 11:55:39,not a real question,1,85,14,"How to HEATMAP the overall GC/AT content of several sequences with R or BioPerl? The situation is the following:

I have several DNA FASTA sequences that want to know the GC/AT content using for example a sliding window of 10bp. The resulting heatmap it has to represent the overall resulting sequence with the GC/AT content in every 10bp of all sequences (like an average).

For example the first row of heatmap could be GC content and second row the AT content

This can be possible to do it with R? I also thank suggestions in BioPerl.

Thanks for any help you could provide.",5
10651170,05/18/2012 10:44:49,339681,05/12/2010 19:47:04,549,4,"Given a sample of random variables, and n, how do I find the ecdf of the sum of n Xs?","I can't fit `X` to a common distribution so currently I just have `X ~ ecdf(sample_data)`.

How do I calculate the empirical distribution of `sum(X1 + ... + Xn)`, given `n`?

",r,,,,,05/18/2012 15:14:03,off topic,1,30,20,"Given a sample of random variables, and n, how do I find the ecdf of the sum of n Xs? I can't fit `X` to a common distribution so currently I just have `X ~ ecdf(sample_data)`.

How do I calculate the empirical distribution of `sum(X1 + ... + Xn)`, given `n`?

",1
9641544,03/09/2012 21:57:18,906592,08/22/2011 20:13:19,488,3,"Weird result with plot option type=""b"". Why does this happen?","The following works, giving 3 points connected with a line.

    plot(c(1,7,12), c(0,0,2),type=""b"")

However this does not work (it plots the points but without connecting line and without any warning or error)

    t<-data.frame(x=1:20,y=c(0,NA, NA, NA, NA, NA,  0, NA, NA, NA, NA,  2, NA, NA, NA, NA, NA, NA, NA, NA))
    plot(t$x, t$y,type=""b"")

Why is that? Is it because of the `NA` in the data frame? I can't seem to find any reference on this.
",r,plot,,,,,open,0,83,10,"Weird result with plot option type=""b"". Why does this happen? The following works, giving 3 points connected with a line.

    plot(c(1,7,12), c(0,0,2),type=""b"")

However this does not work (it plots the points but without connecting line and without any warning or error)

    t<-data.frame(x=1:20,y=c(0,NA, NA, NA, NA, NA,  0, NA, NA, NA, NA,  2, NA, NA, NA, NA, NA, NA, NA, NA))
    plot(t$x, t$y,type=""b"")

Why is that? Is it because of the `NA` in the data frame? I can't seem to find any reference on this.
",2
11725479,07/30/2012 16:20:12,945039,09/14/2011 16:07:43,138,4,"In R, how to use update() dynamically?","In R I often need to cycle through a list of `lm` type objects (e.g. `glm`, `lme`, `clm`) and update them with a new formula or other arguments. I do this using `lapply()` because it returns them in the same list format with names intact, so I can continue the process.

Sometimes the argument I pass is dynamic and varies for each model. For example, 

    lapply(names(mylist) function(ii) {
      jj<-myotherlist[[ii]]; 
      update(mylist[[ii]],.~.+jj)
    }) 

where `jj` is a term that gets added to each model in the list, and is defined in some other list on a per-model basis.

Let's say the intended result formula for a particular model was supposed to be `y~a+b+c`. Instead of that, the call object inside the resulting `lm` object contains `y~a+b+jj`. So, in some circumstances the resulting `lm` object behaves normally, but whenever the formula needs to be evaluated, it errors out because `jj` is long gone. Is there a recommended way to either force `update()` to expand all the variables among its arguments when it writes the `call` object in its output or to force functions that use the `call` object to evaluate the call in the `lm` object's internal context instead of the global context, so that at least I can stuff the new variables into the `model` or `data` object that `lm` style objects typically contain?

AND, what if the part that gets updated with a dynamic variable *is* the data argument? E.g.:

    data=cbind(sharedByAllModels,y=kk)

...where `kk` is unique to the current model (perhaps some kind of aggregation of certain columns in the sharedByAllModels dataframe).",r,lm,,,,,open,0,277,7,"In R, how to use update() dynamically? In R I often need to cycle through a list of `lm` type objects (e.g. `glm`, `lme`, `clm`) and update them with a new formula or other arguments. I do this using `lapply()` because it returns them in the same list format with names intact, so I can continue the process.

Sometimes the argument I pass is dynamic and varies for each model. For example, 

    lapply(names(mylist) function(ii) {
      jj<-myotherlist[[ii]]; 
      update(mylist[[ii]],.~.+jj)
    }) 

where `jj` is a term that gets added to each model in the list, and is defined in some other list on a per-model basis.

Let's say the intended result formula for a particular model was supposed to be `y~a+b+c`. Instead of that, the call object inside the resulting `lm` object contains `y~a+b+jj`. So, in some circumstances the resulting `lm` object behaves normally, but whenever the formula needs to be evaluated, it errors out because `jj` is long gone. Is there a recommended way to either force `update()` to expand all the variables among its arguments when it writes the `call` object in its output or to force functions that use the `call` object to evaluate the call in the `lm` object's internal context instead of the global context, so that at least I can stuff the new variables into the `model` or `data` object that `lm` style objects typically contain?

AND, what if the part that gets updated with a dynamic variable *is* the data argument? E.g.:

    data=cbind(sharedByAllModels,y=kk)

...where `kk` is unique to the current model (perhaps some kind of aggregation of certain columns in the sharedByAllModels dataframe).",2
3716764,09/15/2010 10:31:17,185475,10/07/2009 08:46:28,882,38,Reset R instance,"Is it possible to reset an instance of R?

Eg. if I have used the commands

    x <- 1:10
    plot(x, -x)

And thus polluted the system with the x variable. In this state can I then revert back to a clean state without shutting R down and launching it again?",r,,,,,,open,0,54,3,"Reset R instance Is it possible to reset an instance of R?

Eg. if I have used the commands

    x <- 1:10
    plot(x, -x)

And thus polluted the system with the x variable. In this state can I then revert back to a clean state without shutting R down and launching it again?",1
1395147,09/08/2009 17:12:31,161808,08/24/2009 03:17:53,1,0,Best way to plot interaction effects from a linear model,"In an effort to help populate the R tag here, I am posting a few questions I have often received from students. I have developed my own answers to these over the years, but perhaps there are better ways floating around that I don't know about.

The question: I just ran a regression with continuous y and x but factor f (where levels(f) produces c(""level1"",""level2""))

     thelm<-lm(y~x*f,data=thedata)

Now I would like to plot the predicted values of y by x broken down by groups defined by f. All of the plots I get are ugly and show too many lines.

My answer: Try the predict() function.

    ##restrict prediction to the valid data 
    ##from the model by using thelm$model rather than thedata

     thedata$yhat<-predict(thelm,newdata=expand.grid(x=range(thelm$model$x),
                                                     f=levels(thelm$model$f)))

     plot(yhat~x,data=thethedata,subset=f==""level1"")
     lines(yhat~x,data=thedata,subset=f==""level2"")


Are there other ideas out there that are (1) easier to understand for a newcomer and/or (2) better from some other perspective?

Best,

Jake
",r,,,,,,open,0,217,10,"Best way to plot interaction effects from a linear model In an effort to help populate the R tag here, I am posting a few questions I have often received from students. I have developed my own answers to these over the years, but perhaps there are better ways floating around that I don't know about.

The question: I just ran a regression with continuous y and x but factor f (where levels(f) produces c(""level1"",""level2""))

     thelm<-lm(y~x*f,data=thedata)

Now I would like to plot the predicted values of y by x broken down by groups defined by f. All of the plots I get are ugly and show too many lines.

My answer: Try the predict() function.

    ##restrict prediction to the valid data 
    ##from the model by using thelm$model rather than thedata

     thedata$yhat<-predict(thelm,newdata=expand.grid(x=range(thelm$model$x),
                                                     f=levels(thelm$model$f)))

     plot(yhat~x,data=thethedata,subset=f==""level1"")
     lines(yhat~x,data=thedata,subset=f==""level2"")


Are there other ideas out there that are (1) easier to understand for a newcomer and/or (2) better from some other perspective?

Best,

Jake
",1
10904898,06/05/2012 21:01:51,688266,04/01/2011 20:42:09,645,7,How do i prepare my data for future statistical programs?,"I am currently designing a Survey system (where a Survey has many questions, a question has many answers, and a Response belongs_to a user, survey, question and answer).

I will have a lot of demographic data in the User model and expect 100's of thousands of responses to various questions, etc.

Eventually we will want to analyze the responses, for example. 80% of males like bananas, 20% of females own a Ford and whatnot.  

I am looking into statistical languages like R,SAS and SPSS, and am wondering if my data will need to be structured in any specific way in order to be used by these programs? Or do they all accept csv files?

Is there any advice that you have in terms of statistical data, and structuring data models for it?

",r,sass,sas,spss,analytic-functions,06/06/2012 01:30:13,not a real question,1,130,10,"How do i prepare my data for future statistical programs? I am currently designing a Survey system (where a Survey has many questions, a question has many answers, and a Response belongs_to a user, survey, question and answer).

I will have a lot of demographic data in the User model and expect 100's of thousands of responses to various questions, etc.

Eventually we will want to analyze the responses, for example. 80% of males like bananas, 20% of females own a Ford and whatnot.  

I am looking into statistical languages like R,SAS and SPSS, and am wondering if my data will need to be structured in any specific way in order to be used by these programs? Or do they all accept csv files?

Is there any advice that you have in terms of statistical data, and structuring data models for it?

",5
8895141,01/17/2012 13:09:34,390388,07/13/2010 11:18:58,317,5,extract a subset of a data frame where records are separated by a specific time period,"I have a dataset as follows:

`data <- structure(list(id = 1:8, personID = c(1L, 2L, 3L, 4L, 4L, 3L, 
2L, 1L), lastName = c(""james"", ""joan"", ""lucy"", ""mary"", ""mary"", 
""lucy"", ""joan"", ""james""), date = structure(c(15351, 15351, 15353, 
15355, 15442, 15340, 15350, 15149), class = ""Date""), status = c(1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L)), .Names = c(""id"", ""personID"", ""lastName"", 
""date"", ""status""), row.names = c(NA, -8L), class = ""data.frame"")`

I need to subset the data frame to only include records where each row occured more than once in a period of greater than 8 weeks.

That is all the records for mary and james would result because they appeared twice in periods separated by more than 8 weeks. Record id's 1,4,5, and 8 would make up the resulting dataset.

Thanks.
",r,data.frame,subset,datediff,,,open,0,125,16,"extract a subset of a data frame where records are separated by a specific time period I have a dataset as follows:

`data <- structure(list(id = 1:8, personID = c(1L, 2L, 3L, 4L, 4L, 3L, 
2L, 1L), lastName = c(""james"", ""joan"", ""lucy"", ""mary"", ""mary"", 
""lucy"", ""joan"", ""james""), date = structure(c(15351, 15351, 15353, 
15355, 15442, 15340, 15350, 15149), class = ""Date""), status = c(1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L)), .Names = c(""id"", ""personID"", ""lastName"", 
""date"", ""status""), row.names = c(NA, -8L), class = ""data.frame"")`

I need to subset the data frame to only include records where each row occured more than once in a period of greater than 8 weeks.

That is all the records for mary and james would result because they appeared twice in periods separated by more than 8 weeks. Record id's 1,4,5, and 8 would make up the resulting dataset.

Thanks.
",4
10554504,05/11/2012 15:39:02,1385369,05/09/2012 18:27:36,1,0,"R - ggplot2, several questions, multiple correlated plots","First question I've asked on stack and I'm pretty new to R, so please pardon any etiquette offenses. I'm plotting 2 stacked area charts using ggplot2. The data is wait events from an Oracle database. It's a performance tuning chart. I have several questions.

Since I'm a new user and can't post images, here's a link to the image of my plot:
http://dl.dropbox.com/u/4131944/Permanent/R-Questions/AAS-Plot/Rplot.png 

 1. The two plots below do not line-up correctly, most likely due to the width of text in the legend. Is there an easy solution to this?
 2. The two plots are really correlated, where the top plot shows wait classes like ""CPU"" and ""User I/O"" and the bottom plot shows the details of the specific wait events in those classes. I'd like the colors in the bottom to be based on the wait class, the same as the top, just different shades of that color for the specific events. I'm also open to other options if you don't like the concept. It's a lot of information to convey. I've limited the number of events to 12 to fit in the color scheme, but there are more if it can work.
 3. I'd like to either show more granular time ticks on the X, or perhaps even shade the off-business hours (6pm-8am) gray just to convey a better sense of time of day.
 4. Are there any color schemes with more than 12 colors people commonly use? Looked through brewer and this is the max. I know I could create my own, just curious.

Here's my code:
<!-- language: r -->

    library(ggplot2)
    library(RColorBrewer)
    library(gridExtra)
    
    DF_AAS <- read.csv('http://dl.dropbox.com/u/4131944/Permanent/R-Questions/AAS-Plot/DATA_FRAME_AAS.csv', head=TRUE,sep="","",stringsAsFactors=TRUE)
    DF_AAS <- within(DF_AAS, snap_time <- as.POSIXlt(snap_times2,
                                              format = ""%Y-%m-%d %H:%M:%S""))
    DF_AAS[c('snap_times2')] <- NULL
    
    DF_AAS_EVENT <- read.csv('http://dl.dropbox.com/u/4131944/Permanent/R-Questions/AAS-Plot/DF_AAS_EVENT.csv', head=TRUE,sep="","",stringsAsFactors=TRUE)
    DF_AAS_EVENT <- within(DF_AAS_EVENT, snap_time <- as.POSIXlt(snap_times2,
                                                     format = ""%Y-%m-%d %H:%M:%S""))
    DF_AAS_EVENT[c('snap_times2')] <- NULL
    
    plot_aas_wait_class <- ggplot()+
      geom_area(data=DF_AAS, aes(x = snap_time, y = aas,
                                        fill = wait_class),stat = ""identity"", position = ""stack"",alpha=.9)+
                                          scale_fill_brewer(palette=""Paired"",breaks = sort(levels(DF_AAS$wait_class)))+
                                          scale_y_continuous(breaks = seq(0, max(DF_AAS$aas)+(max(DF_AAS$aas)*.2), 5))+
                                          opts(panel.background = theme_rect(colour = ""#aaaaaa""))
    
    
    plot_aas_event <- ggplot()+
      geom_area(data=DF_AAS_EVENT, aes(x = snap_time, y = aas,
                                       fill = wait_class_event),stat = ""identity"", position = ""stack"")+
                                         scale_fill_brewer(palette=""Paired"",breaks = DF_AAS_EVENT$wait_class_event)+
                                         scale_y_continuous(breaks = seq(0, max(DF_AAS_EVENT$aas)+(max(DF_AAS_EVENT$aas)*.2), 5))+
                                         opts( panel.background = theme_rect(colour = ""#aaaaaa""))
    
    grid.arrange(arrangeGrob(plot_aas_wait_class, plot_aas_event),heights=c(1/2,1/2),ncol=1)

",r,ggplot2,,,,,open,0,850,8,"R - ggplot2, several questions, multiple correlated plots First question I've asked on stack and I'm pretty new to R, so please pardon any etiquette offenses. I'm plotting 2 stacked area charts using ggplot2. The data is wait events from an Oracle database. It's a performance tuning chart. I have several questions.

Since I'm a new user and can't post images, here's a link to the image of my plot:
http://dl.dropbox.com/u/4131944/Permanent/R-Questions/AAS-Plot/Rplot.png 

 1. The two plots below do not line-up correctly, most likely due to the width of text in the legend. Is there an easy solution to this?
 2. The two plots are really correlated, where the top plot shows wait classes like ""CPU"" and ""User I/O"" and the bottom plot shows the details of the specific wait events in those classes. I'd like the colors in the bottom to be based on the wait class, the same as the top, just different shades of that color for the specific events. I'm also open to other options if you don't like the concept. It's a lot of information to convey. I've limited the number of events to 12 to fit in the color scheme, but there are more if it can work.
 3. I'd like to either show more granular time ticks on the X, or perhaps even shade the off-business hours (6pm-8am) gray just to convey a better sense of time of day.
 4. Are there any color schemes with more than 12 colors people commonly use? Looked through brewer and this is the max. I know I could create my own, just curious.

Here's my code:
<!-- language: r -->

    library(ggplot2)
    library(RColorBrewer)
    library(gridExtra)
    
    DF_AAS <- read.csv('http://dl.dropbox.com/u/4131944/Permanent/R-Questions/AAS-Plot/DATA_FRAME_AAS.csv', head=TRUE,sep="","",stringsAsFactors=TRUE)
    DF_AAS <- within(DF_AAS, snap_time <- as.POSIXlt(snap_times2,
                                              format = ""%Y-%m-%d %H:%M:%S""))
    DF_AAS[c('snap_times2')] <- NULL
    
    DF_AAS_EVENT <- read.csv('http://dl.dropbox.com/u/4131944/Permanent/R-Questions/AAS-Plot/DF_AAS_EVENT.csv', head=TRUE,sep="","",stringsAsFactors=TRUE)
    DF_AAS_EVENT <- within(DF_AAS_EVENT, snap_time <- as.POSIXlt(snap_times2,
                                                     format = ""%Y-%m-%d %H:%M:%S""))
    DF_AAS_EVENT[c('snap_times2')] <- NULL
    
    plot_aas_wait_class <- ggplot()+
      geom_area(data=DF_AAS, aes(x = snap_time, y = aas,
                                        fill = wait_class),stat = ""identity"", position = ""stack"",alpha=.9)+
                                          scale_fill_brewer(palette=""Paired"",breaks = sort(levels(DF_AAS$wait_class)))+
                                          scale_y_continuous(breaks = seq(0, max(DF_AAS$aas)+(max(DF_AAS$aas)*.2), 5))+
                                          opts(panel.background = theme_rect(colour = ""#aaaaaa""))
    
    
    plot_aas_event <- ggplot()+
      geom_area(data=DF_AAS_EVENT, aes(x = snap_time, y = aas,
                                       fill = wait_class_event),stat = ""identity"", position = ""stack"")+
                                         scale_fill_brewer(palette=""Paired"",breaks = DF_AAS_EVENT$wait_class_event)+
                                         scale_y_continuous(breaks = seq(0, max(DF_AAS_EVENT$aas)+(max(DF_AAS_EVENT$aas)*.2), 5))+
                                         opts( panel.background = theme_rect(colour = ""#aaaaaa""))
    
    grid.arrange(arrangeGrob(plot_aas_wait_class, plot_aas_event),heights=c(1/2,1/2),ncol=1)

",2
8841277,01/12/2012 19:43:19,406278,07/29/2010 22:53:08,649,4,Error when running glm,"
I'm running a glm in R with the car package and got the following error message:

    > dlmod = glm(dlstatus ~ dlour_bid, data=dlmydat, family=binomial(link=""logit""))
    Error in eval(expr, envir, enclos) : y values must be 0 <= y <= 1

My response variable is coded 1/2 and not 0/1.
I'm just not sure how to change this.

    tat <- dldat$status
    dlstatus <- NA
    dlstatus[stat %in% levels(stat)[1:2]] <- ""Won""
    dlstatus[stat %in% levels(stat)[3]] <- ""Lost""
    dlstatus <- factor(status)
    levels(dlstatus)
    dlstatus = as.numeric(dlstatus)
    dlstatus


Can anyone help with changing the labels from 1/2 to 0/1 so that I can run a glm in R.R",r,,,,,01/12/2012 21:58:41,too localized,1,127,4,"Error when running glm 
I'm running a glm in R with the car package and got the following error message:

    > dlmod = glm(dlstatus ~ dlour_bid, data=dlmydat, family=binomial(link=""logit""))
    Error in eval(expr, envir, enclos) : y values must be 0 <= y <= 1

My response variable is coded 1/2 and not 0/1.
I'm just not sure how to change this.

    tat <- dldat$status
    dlstatus <- NA
    dlstatus[stat %in% levels(stat)[1:2]] <- ""Won""
    dlstatus[stat %in% levels(stat)[3]] <- ""Lost""
    dlstatus <- factor(status)
    levels(dlstatus)
    dlstatus = as.numeric(dlstatus)
    dlstatus


Can anyone help with changing the labels from 1/2 to 0/1 so that I can run a glm in R.R",1
11434918,07/11/2012 14:30:19,1426915,05/30/2012 19:45:22,67,4,R: Useful Resources for Built-In Functions,"I'm relatively new to programming and I've only just started programming with R.  Having checked out a lot of the discussions on programming with R, and attempting to answer a few, I've noticed that loops aren't a very popular solution - built-in functions are.  My experience in programming began earlier this year with Python (in a university course that I thoroughly enjoyed), and I was essentially taught that loops are the solution to everything, given their versatility.  However, I'm starting to see that they often lack efficiency.

Obviously most of the knowledge of the built-in functions comes from experience, but for the programming in R that I've done, I've been creating my own functions with loops.   The reason I've done this is because I haven't known that certain built-in functions exist that would greatly simplify my programs.

Thus, my question is as follows:  Do you know of any resources that would give me insight as to what built-in functions apply to a given programming obstacles that I may try to overcome in the future?  

Additionally, I've been told that many of these functions are ""vectorized"", and I was hoping that someone could enlighten me as to what that actually means.

Thanks!",r,function,resources,vectorization,built-in,07/12/2012 17:10:38,not constructive,1,205,6,"R: Useful Resources for Built-In Functions I'm relatively new to programming and I've only just started programming with R.  Having checked out a lot of the discussions on programming with R, and attempting to answer a few, I've noticed that loops aren't a very popular solution - built-in functions are.  My experience in programming began earlier this year with Python (in a university course that I thoroughly enjoyed), and I was essentially taught that loops are the solution to everything, given their versatility.  However, I'm starting to see that they often lack efficiency.

Obviously most of the knowledge of the built-in functions comes from experience, but for the programming in R that I've done, I've been creating my own functions with loops.   The reason I've done this is because I haven't known that certain built-in functions exist that would greatly simplify my programs.

Thus, my question is as follows:  Do you know of any resources that would give me insight as to what built-in functions apply to a given programming obstacles that I may try to overcome in the future?  

Additionally, I've been told that many of these functions are ""vectorized"", and I was hoping that someone could enlighten me as to what that actually means.

Thanks!",5
6396356,06/18/2011 13:30:36,804534,06/18/2011 13:19:48,1,0,R: inner correlation of occurrences (burstiness?),"I want to measure the inner correlation of the occurrences of events i.e. I want to distinguish between the two (drawn) [samples (follow the link for the sketches I have not yet enough reputation to post pictures)][1] and say ""in the second sample events occur more conglomerate compared to the first"".
Isn't this some kind of burstiness?

I have [drawn][1] a possible representation of the expected result (for S2) in the image below the samples (just as it is in my mind now), but don't hesitate to suggest different proposals.
 
I looked out and tried different build-in functions and googled for R and burstiness, but I didn't make progress. It further seemed to me (by reading papers about burstiness) that there is no common measure for burstiness and it would be cool to justify why choosing a specific one.

This is how the time series is represented, currently:


     [1]  3.256861  3.377142  3.941173  4.304236  4.485358  4.606512  4.707296
     [8]  5.473004  5.714746  5.815394  5.835405  5.936067  5.957008  6.964611
    [15]  7.045158  7.065171  7.165824  7.669618  8.173240  8.273692  9.503988
    [22]  9.604991  9.624853  9.725522 10.237766 10.954529 11.378399 12.687714
    [29] 13.291919 13.412580 13.675270 14.380529 14.743638 15.247138 15.851832
    [36] 15.952875 15.972497 16.456259 16.476052 17.201506 17.463708 18.068535
    [43] 18.309645 18.390292 18.410299 18.430323 18.531736 18.652921 18.793662
    [50] 19.297076 19.639692 19.760698 20.768096 20.868441 20.990499 21.494412
    [57] 21.856368 22.199341 22.219143 22.440472 22.481118 23.327013 23.447678
    [64] 23.811188 23.843000 24.113302

(Should I upload a corresponding csv?)


  [1]: http://dat-berger.de/data/burstiness.png",r,statistics,time-series,,,06/19/2011 19:40:38,off topic,1,285,6,"R: inner correlation of occurrences (burstiness?) I want to measure the inner correlation of the occurrences of events i.e. I want to distinguish between the two (drawn) [samples (follow the link for the sketches I have not yet enough reputation to post pictures)][1] and say ""in the second sample events occur more conglomerate compared to the first"".
Isn't this some kind of burstiness?

I have [drawn][1] a possible representation of the expected result (for S2) in the image below the samples (just as it is in my mind now), but don't hesitate to suggest different proposals.
 
I looked out and tried different build-in functions and googled for R and burstiness, but I didn't make progress. It further seemed to me (by reading papers about burstiness) that there is no common measure for burstiness and it would be cool to justify why choosing a specific one.

This is how the time series is represented, currently:


     [1]  3.256861  3.377142  3.941173  4.304236  4.485358  4.606512  4.707296
     [8]  5.473004  5.714746  5.815394  5.835405  5.936067  5.957008  6.964611
    [15]  7.045158  7.065171  7.165824  7.669618  8.173240  8.273692  9.503988
    [22]  9.604991  9.624853  9.725522 10.237766 10.954529 11.378399 12.687714
    [29] 13.291919 13.412580 13.675270 14.380529 14.743638 15.247138 15.851832
    [36] 15.952875 15.972497 16.456259 16.476052 17.201506 17.463708 18.068535
    [43] 18.309645 18.390292 18.410299 18.430323 18.531736 18.652921 18.793662
    [50] 19.297076 19.639692 19.760698 20.768096 20.868441 20.990499 21.494412
    [57] 21.856368 22.199341 22.219143 22.440472 22.481118 23.327013 23.447678
    [64] 23.811188 23.843000 24.113302

(Should I upload a corresponding csv?)


  [1]: http://dat-berger.de/data/burstiness.png",3
10806867,05/29/2012 21:19:38,1366658,04/30/2012 20:38:09,15,0,R has magic number 'ëPNG' error,"I have a R script that reads files and generates graphs automatically. It works when it runs the first time. If the .png files are there and the script needs to re-create them with the new data, I get this error:

    Error: bad restore file magic number (file may be corrupted) -- no data loaded
    In addition: Warning message:
    file 'test1.png' has magic number 'ëPNG'
       Use of save versions prior to 2 is deprecated
    Execution halted

what does this error mean? I tried to use source(""filename""), it did not work either.

thanks,
",r,,,,,05/30/2012 16:35:58,too localized,1,108,6,"R has magic number 'ëPNG' error I have a R script that reads files and generates graphs automatically. It works when it runs the first time. If the .png files are there and the script needs to re-create them with the new data, I get this error:

    Error: bad restore file magic number (file may be corrupted) -- no data loaded
    In addition: Warning message:
    file 'test1.png' has magic number 'ëPNG'
       Use of save versions prior to 2 is deprecated
    Execution halted

what does this error mean? I tried to use source(""filename""), it did not work either.

thanks,
",1
9882448,03/27/2012 02:08:19,1224740,02/22/2012 02:04:33,1,0,Does anyone know if it's possible to create a static build of R?, I have a requirement to run R WITHOUT R environment on CentOS.,r,static,compilation,,,03/27/2012 08:40:17,not a real question,1,13,13,Does anyone know if it's possible to create a static build of R?  I have a requirement to run R WITHOUT R environment on CentOS.,3
10094226,04/10/2012 18:18:58,535458,10/15/2010 17:37:57,425,2,How do I create a function for plyr,"I have a series of soccer results and wish to find out how many points a team has scored in a particular number of games

Here is the head of a subset with the cumulative points scored during a season since the latest result

I have been wrist=-slapped a couple of times for not using dput so bear with length

    allData <- structure(list(team = c(""Arsenal"", ""Tottenham H"", ""Tottenham H"", 
    ""Arsenal"", ""Arsenal"", ""Tottenham H""), venue = c(""H"", ""A"", ""H"", 
    ""A"", ""H"", ""A""), result = c(""W"", ""D"", ""W"", ""L"", ""W"", ""D""), GF = c(1L, 
    0L, 3L, 1L, 3L, 0L), GA = c(0L, 0L, 1L, 2L, 0L, 0L), gameDate = structure(c(1333868400, 
    1333782000, 1333263600, 1333177200, 1332572400, 1332572400), class = c(""POSIXct"", 
    ""POSIXt""), tzone = """"), season = structure(c(2L, 2L, 2L, 2L, 
    2L, 2L), .Label = c(""2010/2011"", ""2011/2012""), class = ""factor""), 
     points = c(3, 1, 3, 0, 3, 1), GD = c(1L, 0L, 2L, -1L, 3L, 
    0L), cumpts = c(3, 1, 4, 3, 6, 5)), .Names = c(""team"", ""venue"", 
    ""result"", ""GF"", ""GA"", ""gameDate"", ""season"", ""points"", ""GD"", ""cumpts""
    ), row.names = c(NA, 6L), class = ""data.frame"")

and here is the data for one team during one season


spurs <- structure(list(team = c(""Tottenham H"", ""Tottenham H"", ""Tottenham H"", 
""Tottenham H"", ""Tottenham H"", ""Tottenham H"", ""Tottenham H"", ""Tottenham H"", 
""Tottenham H"", ""Tottenham H"", ""Tottenham H"", ""Tottenham H"", ""Tottenham H"", 
""Tottenham H"", ""Tottenham H"", ""Tottenham H"", ""Tottenham H"", ""Tottenham H"", 
""Tottenham H"", ""Tottenham H"", ""Tottenham H"", ""Tottenham H"", ""Tottenham H"", 
""Tottenham H"", ""Tottenham H"", ""Tottenham H"", ""Tottenham H"", ""Tottenham H"", 
""Tottenham H"", ""Tottenham H"", ""Tottenham H"", ""Tottenham H""), 
    venue = c(""A"", ""H"", ""A"", ""H"", ""A"", ""H"", ""A"", ""H"", ""A"", ""H"", 
    ""A"", ""H"", ""H"", ""H"", ""A"", ""A"", ""H"", ""H"", ""A"", ""H"", ""A"", ""H"", 
    ""A"", ""H"", ""A"", ""A"", ""H"", ""A"", ""H"", ""A"", ""H"", ""A""), result = c(""D"", 
    ""W"", ""D"", ""D"", ""L"", ""L"", ""L"", ""W"", ""D"", ""W"", ""L"", ""D"", ""W"", 
    ""W"", ""D"", ""W"", ""D"", ""W"", ""L"", ""W"", ""W"", ""W"", ""W"", ""W"", ""W"", 
    ""D"", ""W"", ""W"", ""W"", ""W"", ""L"", ""L""), GF = c(0L, 3L, 0L, 1L, 
    0L, 1L, 2L, 5L, 0L, 3L, 2L, 1L, 2L, 1L, 1L, 2L, 1L, 1L, 1L, 
    3L, 3L, 2L, 3L, 3L, 2L, 2L, 2L, 2L, 4L, 2L, 1L, 0L), GA = c(0L, 
    1L, 0L, 1L, 1L, 3L, 5L, 0L, 0L, 1L, 3L, 1L, 0L, 0L, 1L, 0L, 
    1L, 0L, 2L, 0L, 1L, 0L, 1L, 1L, 1L, 2L, 1L, 1L, 0L, 0L, 5L, 
    3L), gameDate = structure(c(1333782000, 1333263600, 1332572400, 
    1332313200, 1331366400, 1330848000, 1330243200, 1328947200, 
    1328515200, 1327996800, 1327219200, 1326528000, 1326268800, 
    1325577600, 1325318400, 1324972800, 1324540800, 1324281600, 
    1323590400, 1322899200, 1322294400, 1321862400, 1320562800, 
    1319958000, 1319353200, 1318748400, 1317538800, 1316847600, 
    1316329200, 1315638000, 1314514800, 1313996400), class = c(""POSIXct"", 
    ""POSIXt""), tzone = """"), season = structure(c(2L, 2L, 2L, 
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), .Label = c(""2010/2011"", 
    ""2011/2012""), class = ""factor""), points = c(1, 3, 1, 1, 0, 
    0, 0, 3, 1, 3, 0, 1, 3, 3, 1, 3, 1, 3, 0, 3, 3, 3, 3, 3, 
    3, 1, 3, 3, 3, 3, 0, 0), GD = c(0L, 2L, 0L, 0L, -1L, -2L, 
    -3L, 5L, 0L, 2L, -1L, 0L, 2L, 1L, 0L, 2L, 0L, 1L, -1L, 3L, 
    2L, 2L, 2L, 2L, 1L, 0L, 1L, 1L, 4L, 2L, -4L, -3L), cumpts = c(1, 
    4, 5, 6, 6, 6, 6, 9, 10, 13, 13, 14, 17, 20, 21, 24, 25, 
    28, 28, 31, 34, 37, 40, 43, 46, 47, 50, 53, 56, 59, 59, 59
    )), .Names = c(""team"", ""venue"", ""result"", ""GF"", ""GA"", ""gameDate"", 
""season"", ""points"", ""GD"", ""cumpts""), row.names = c(NA, -32L), class = ""data.frame"")

I then have this code on the spurs dataframe to calculate points scored in specific game lengths(here 5)

    gameLength <- 5
    seasonLength <- nrow(spurs)
    cumPoints <- c()
    cumPoints[1] <- spurs[gameLength,]$cumpts
    for (i in gameLength+1:seasonLength) {
    cumPoints[i-(gameLength-1)] <- ((spurs[i,]$cumpts)- 
     (spurs[i-gameLength,]$cumpts))
    }
    cumPoints <- cumPoints[!is.na(cumPoints)] # not sure why throws up NAs

This produces the correct output

     [1]  6  5  2  4  4  7  7  8  8 10  8 11 11 11  8 10 10 12 12 15 15
    [22] 13 13 13 13 13 12  9

but I need to be able to transform the allData with a column containing this data for every season and team in the dataframe.

 I am assuming I should use ddply somehow, unless there is a better alternative
",r,plyr,,,,,open,0,916,8,"How do I create a function for plyr I have a series of soccer results and wish to find out how many points a team has scored in a particular number of games

Here is the head of a subset with the cumulative points scored during a season since the latest result

I have been wrist=-slapped a couple of times for not using dput so bear with length

    allData <- structure(list(team = c(""Arsenal"", ""Tottenham H"", ""Tottenham H"", 
    ""Arsenal"", ""Arsenal"", ""Tottenham H""), venue = c(""H"", ""A"", ""H"", 
    ""A"", ""H"", ""A""), result = c(""W"", ""D"", ""W"", ""L"", ""W"", ""D""), GF = c(1L, 
    0L, 3L, 1L, 3L, 0L), GA = c(0L, 0L, 1L, 2L, 0L, 0L), gameDate = structure(c(1333868400, 
    1333782000, 1333263600, 1333177200, 1332572400, 1332572400), class = c(""POSIXct"", 
    ""POSIXt""), tzone = """"), season = structure(c(2L, 2L, 2L, 2L, 
    2L, 2L), .Label = c(""2010/2011"", ""2011/2012""), class = ""factor""), 
     points = c(3, 1, 3, 0, 3, 1), GD = c(1L, 0L, 2L, -1L, 3L, 
    0L), cumpts = c(3, 1, 4, 3, 6, 5)), .Names = c(""team"", ""venue"", 
    ""result"", ""GF"", ""GA"", ""gameDate"", ""season"", ""points"", ""GD"", ""cumpts""
    ), row.names = c(NA, 6L), class = ""data.frame"")

and here is the data for one team during one season


spurs <- structure(list(team = c(""Tottenham H"", ""Tottenham H"", ""Tottenham H"", 
""Tottenham H"", ""Tottenham H"", ""Tottenham H"", ""Tottenham H"", ""Tottenham H"", 
""Tottenham H"", ""Tottenham H"", ""Tottenham H"", ""Tottenham H"", ""Tottenham H"", 
""Tottenham H"", ""Tottenham H"", ""Tottenham H"", ""Tottenham H"", ""Tottenham H"", 
""Tottenham H"", ""Tottenham H"", ""Tottenham H"", ""Tottenham H"", ""Tottenham H"", 
""Tottenham H"", ""Tottenham H"", ""Tottenham H"", ""Tottenham H"", ""Tottenham H"", 
""Tottenham H"", ""Tottenham H"", ""Tottenham H"", ""Tottenham H""), 
    venue = c(""A"", ""H"", ""A"", ""H"", ""A"", ""H"", ""A"", ""H"", ""A"", ""H"", 
    ""A"", ""H"", ""H"", ""H"", ""A"", ""A"", ""H"", ""H"", ""A"", ""H"", ""A"", ""H"", 
    ""A"", ""H"", ""A"", ""A"", ""H"", ""A"", ""H"", ""A"", ""H"", ""A""), result = c(""D"", 
    ""W"", ""D"", ""D"", ""L"", ""L"", ""L"", ""W"", ""D"", ""W"", ""L"", ""D"", ""W"", 
    ""W"", ""D"", ""W"", ""D"", ""W"", ""L"", ""W"", ""W"", ""W"", ""W"", ""W"", ""W"", 
    ""D"", ""W"", ""W"", ""W"", ""W"", ""L"", ""L""), GF = c(0L, 3L, 0L, 1L, 
    0L, 1L, 2L, 5L, 0L, 3L, 2L, 1L, 2L, 1L, 1L, 2L, 1L, 1L, 1L, 
    3L, 3L, 2L, 3L, 3L, 2L, 2L, 2L, 2L, 4L, 2L, 1L, 0L), GA = c(0L, 
    1L, 0L, 1L, 1L, 3L, 5L, 0L, 0L, 1L, 3L, 1L, 0L, 0L, 1L, 0L, 
    1L, 0L, 2L, 0L, 1L, 0L, 1L, 1L, 1L, 2L, 1L, 1L, 0L, 0L, 5L, 
    3L), gameDate = structure(c(1333782000, 1333263600, 1332572400, 
    1332313200, 1331366400, 1330848000, 1330243200, 1328947200, 
    1328515200, 1327996800, 1327219200, 1326528000, 1326268800, 
    1325577600, 1325318400, 1324972800, 1324540800, 1324281600, 
    1323590400, 1322899200, 1322294400, 1321862400, 1320562800, 
    1319958000, 1319353200, 1318748400, 1317538800, 1316847600, 
    1316329200, 1315638000, 1314514800, 1313996400), class = c(""POSIXct"", 
    ""POSIXt""), tzone = """"), season = structure(c(2L, 2L, 2L, 
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), .Label = c(""2010/2011"", 
    ""2011/2012""), class = ""factor""), points = c(1, 3, 1, 1, 0, 
    0, 0, 3, 1, 3, 0, 1, 3, 3, 1, 3, 1, 3, 0, 3, 3, 3, 3, 3, 
    3, 1, 3, 3, 3, 3, 0, 0), GD = c(0L, 2L, 0L, 0L, -1L, -2L, 
    -3L, 5L, 0L, 2L, -1L, 0L, 2L, 1L, 0L, 2L, 0L, 1L, -1L, 3L, 
    2L, 2L, 2L, 2L, 1L, 0L, 1L, 1L, 4L, 2L, -4L, -3L), cumpts = c(1, 
    4, 5, 6, 6, 6, 6, 9, 10, 13, 13, 14, 17, 20, 21, 24, 25, 
    28, 28, 31, 34, 37, 40, 43, 46, 47, 50, 53, 56, 59, 59, 59
    )), .Names = c(""team"", ""venue"", ""result"", ""GF"", ""GA"", ""gameDate"", 
""season"", ""points"", ""GD"", ""cumpts""), row.names = c(NA, -32L), class = ""data.frame"")

I then have this code on the spurs dataframe to calculate points scored in specific game lengths(here 5)

    gameLength <- 5
    seasonLength <- nrow(spurs)
    cumPoints <- c()
    cumPoints[1] <- spurs[gameLength,]$cumpts
    for (i in gameLength+1:seasonLength) {
    cumPoints[i-(gameLength-1)] <- ((spurs[i,]$cumpts)- 
     (spurs[i-gameLength,]$cumpts))
    }
    cumPoints <- cumPoints[!is.na(cumPoints)] # not sure why throws up NAs

This produces the correct output

     [1]  6  5  2  4  4  7  7  8  8 10  8 11 11 11  8 10 10 12 12 15 15
    [22] 13 13 13 13 13 12  9

but I need to be able to transform the allData with a column containing this data for every season and team in the dataframe.

 I am assuming I should use ddply somehow, unless there is a better alternative
",2
1753299,11/18/2009 02:42:43,3788,08/30/2008 19:55:42,1792,94,Help using predict() for kernlab's SVM in R?,"I am trying to use the `kernlab` R package to do Support Vector Machines (SVM). For my very simple example, I have two pieces of training data. A and B.

(A and B are of type `matrix` - they are adjacency matrices for graphs.)

So I wrote a function which takes A+B and generates a kernel matrix.

    > km
             [,1]     [,2]
    [1,] 14.33333 18.47368
    [2,] 18.47368 38.96053

Now I use `kernlab`'s `ksvm` function to generate my predictive model. Right now, I'm just trying to get the darn thing to work - I'm not worried about training error, etc.

So, **Question 1**: Am I generating my model correctly? Reasonably?

    # y are my classes. In this case, A is in class ""1"" and B is in class ""-1""
    > y
    [1]  1 -1

    > model2 =  ksvm(km, y, type=""C-svc"", kernel = ""matrix"");
    > model2
    Support Vector Machine object of class ""ksvm"" 
    
    SV type: C-svc  (classification) 
     parameter : cost C = 1 
    
    [1] "" Kernel matrix used as input.""
    
    Number of Support Vectors : 2 
    
    Objective Function Value : -0.1224 
    Training error : 0 

So far so good. We created our custom kernel matrix, and then we created a ksvm model using that matrix. We have our training data labeled as ""1"" and ""-1"".

Now to predict:

    > A
         [,1] [,2] [,3]
    [1,]    0    1    1
    [2,]    1    0    1
    [3,]    0    0    0
    
    > predict(model2, A)
    Error in as.matrix(Z) : object 'Z' not found

Uh-oh. This is okay. Kind of expected, really. ""Predict"" wants some sort of vector, not a matrix.

So lets try some things:

    > predict(model2, c(1))
    Error in as.matrix(Z) : object 'Z' not found
    > predict(model2, c(1,1))
    Error in as.matrix(Z) : object 'Z' not found
    > predict(model2, c(1,1,1))
    Error in as.matrix(Z) : object 'Z' not found
    > predict(model2, c(1,1,1,1))
    Error in as.matrix(Z) : object 'Z' not found
    > predict(model2, km)
    Error in as.matrix(Z) : object 'Z' not found

Some of the above tests are nonsensical, but that is my point: no matter what I do, I just can't get predict() to look at my data and do a prediction. Scalars don't work, vectors don't work. A 2x2 matrix doesn't work, nor does a 3x3 matrix.

**What am I doing wrong here?**

(Once I figure out what ksvm *wants*, then I can make sure that my test data can conform to that format in a sane/reasonable/mathematically sound way.)",r,kernlab,svm,,,,open,0,562,8,"Help using predict() for kernlab's SVM in R? I am trying to use the `kernlab` R package to do Support Vector Machines (SVM). For my very simple example, I have two pieces of training data. A and B.

(A and B are of type `matrix` - they are adjacency matrices for graphs.)

So I wrote a function which takes A+B and generates a kernel matrix.

    > km
             [,1]     [,2]
    [1,] 14.33333 18.47368
    [2,] 18.47368 38.96053

Now I use `kernlab`'s `ksvm` function to generate my predictive model. Right now, I'm just trying to get the darn thing to work - I'm not worried about training error, etc.

So, **Question 1**: Am I generating my model correctly? Reasonably?

    # y are my classes. In this case, A is in class ""1"" and B is in class ""-1""
    > y
    [1]  1 -1

    > model2 =  ksvm(km, y, type=""C-svc"", kernel = ""matrix"");
    > model2
    Support Vector Machine object of class ""ksvm"" 
    
    SV type: C-svc  (classification) 
     parameter : cost C = 1 
    
    [1] "" Kernel matrix used as input.""
    
    Number of Support Vectors : 2 
    
    Objective Function Value : -0.1224 
    Training error : 0 

So far so good. We created our custom kernel matrix, and then we created a ksvm model using that matrix. We have our training data labeled as ""1"" and ""-1"".

Now to predict:

    > A
         [,1] [,2] [,3]
    [1,]    0    1    1
    [2,]    1    0    1
    [3,]    0    0    0
    
    > predict(model2, A)
    Error in as.matrix(Z) : object 'Z' not found

Uh-oh. This is okay. Kind of expected, really. ""Predict"" wants some sort of vector, not a matrix.

So lets try some things:

    > predict(model2, c(1))
    Error in as.matrix(Z) : object 'Z' not found
    > predict(model2, c(1,1))
    Error in as.matrix(Z) : object 'Z' not found
    > predict(model2, c(1,1,1))
    Error in as.matrix(Z) : object 'Z' not found
    > predict(model2, c(1,1,1,1))
    Error in as.matrix(Z) : object 'Z' not found
    > predict(model2, km)
    Error in as.matrix(Z) : object 'Z' not found

Some of the above tests are nonsensical, but that is my point: no matter what I do, I just can't get predict() to look at my data and do a prediction. Scalars don't work, vectors don't work. A 2x2 matrix doesn't work, nor does a 3x3 matrix.

**What am I doing wrong here?**

(Once I figure out what ksvm *wants*, then I can make sure that my test data can conform to that format in a sane/reasonable/mathematically sound way.)",3
11658893,07/25/2012 21:26:34,1438637,06/06/2012 01:29:55,45,0,Neural network - data with both binary and continuous inputs?,"I'm using the nnet package in R to attempt to build an ANN to predict real estate prices for condos (personal project). I am new to this and don't have a math background so please bare with me.

I have input variables that are both binary and continuous. For example some binary variables which were originally yes/no were converted to 1/0 for the neural net. Other variables are continuous like sqft.

[Sample of input data][1]

I have normalized all values to be on a 0-1 scale. Maybe bedrooms and bathroom shouldn't be normalized since their range is only 0-4?

Do these mixed inputs present a problem for the ANN? I've gotten okay results, but upon closer examination the weights the ANN has chosen for certain variables don't seem to make sense. My code is below, any suggestions?

`ANN <- nnet(Price ~ Sqft + Bedrooms + Bathrooms + Parking2 + Elevator +` 
`Central.AC + Terrace + Washer.Dryer + Doorman + Exercise.Room +` `New.York.View,data[1:700,],size=3, maxit=5000, linout=TRUE, decay=.0001)`


  [1]: http://i.stack.imgur.com/KydHM.png",r,neural-network,,,,07/26/2012 13:00:20,off topic,1,163,10,"Neural network - data with both binary and continuous inputs? I'm using the nnet package in R to attempt to build an ANN to predict real estate prices for condos (personal project). I am new to this and don't have a math background so please bare with me.

I have input variables that are both binary and continuous. For example some binary variables which were originally yes/no were converted to 1/0 for the neural net. Other variables are continuous like sqft.

[Sample of input data][1]

I have normalized all values to be on a 0-1 scale. Maybe bedrooms and bathroom shouldn't be normalized since their range is only 0-4?

Do these mixed inputs present a problem for the ANN? I've gotten okay results, but upon closer examination the weights the ANN has chosen for certain variables don't seem to make sense. My code is below, any suggestions?

`ANN <- nnet(Price ~ Sqft + Bedrooms + Bathrooms + Parking2 + Elevator +` 
`Central.AC + Terrace + Washer.Dryer + Doorman + Exercise.Room +` `New.York.View,data[1:700,],size=3, maxit=5000, linout=TRUE, decay=.0001)`


  [1]: http://i.stack.imgur.com/KydHM.png",2
11442202,07/11/2012 22:04:28,1142618,01/11/2012 07:19:53,25,1,to get right form output,"here is my funciton to calculate  beta  in  china  stock market,   
>    mybeta <- function(company) {    
>    require(quantmod)    
>    setSymbolLookup(CSI300=list(name=""000300.ss"",src=""yahoo""))    
>    getSymbols(""CSI300"",from=""2010-01-01"",to=""2011-01-01"")    
>    setSymbolLookup(SDB=list(name=company,src=""yahoo""))   
>    getSymbols(""SDB"",from=""2010-01-01"",to=""2011-01-01"")   
>    csi=as.data.frame(weeklyReturn(CSI300))   
>    sdb=as.data.frame(weeklyReturn(SDB))  
>    cbeta=merge(csi, sdb, by=""row.names"")   
>    cov(cbeta[2],cbeta[3])/var(cbeta[2])      
>    }   

when i input  :
> mybeta(""600005.ss"")  
>                       weekly.returns.y            
>   weekly.returns.x               1.105631

i want  the output is only 1.105631,not include  weekly.returns.y  and weekly.returns.x

how can i do ?

",r,beta,,,,07/12/2012 12:04:11,not a real question,1,183,5,"to get right form output here is my funciton to calculate  beta  in  china  stock market,   
>    mybeta <- function(company) {    
>    require(quantmod)    
>    setSymbolLookup(CSI300=list(name=""000300.ss"",src=""yahoo""))    
>    getSymbols(""CSI300"",from=""2010-01-01"",to=""2011-01-01"")    
>    setSymbolLookup(SDB=list(name=company,src=""yahoo""))   
>    getSymbols(""SDB"",from=""2010-01-01"",to=""2011-01-01"")   
>    csi=as.data.frame(weeklyReturn(CSI300))   
>    sdb=as.data.frame(weeklyReturn(SDB))  
>    cbeta=merge(csi, sdb, by=""row.names"")   
>    cov(cbeta[2],cbeta[3])/var(cbeta[2])      
>    }   

when i input  :
> mybeta(""600005.ss"")  
>                       weekly.returns.y            
>   weekly.returns.x               1.105631

i want  the output is only 1.105631,not include  weekly.returns.y  and weekly.returns.x

how can i do ?

",2
11584648,07/20/2012 18:03:43,714319,04/18/2011 23:49:11,553,4,"setkey and the := operator, data.table, R","When using the `data.table` package, I am a bit unsure of when i need to `setkey()`. For example, when using the `:=` operator with the `by` option, things seem to still be very fast even though I have not set a key. Could someone please elucidate when `setkey()` is necessary and when it is not? And if it is not necessary prior to calling `:=` with `by` then how is the `data.table` package so fast since presumably it has to do the same thing as `apply` in standard `data.frame` R by doing a sequential search rather than a binary one since it doesn't know whether my `data.table` is actually sorted by the argument to `by`.

Thanks",r,data.frame,data.table,apply,,,open,0,115,7,"setkey and the := operator, data.table, R When using the `data.table` package, I am a bit unsure of when i need to `setkey()`. For example, when using the `:=` operator with the `by` option, things seem to still be very fast even though I have not set a key. Could someone please elucidate when `setkey()` is necessary and when it is not? And if it is not necessary prior to calling `:=` with `by` then how is the `data.table` package so fast since presumably it has to do the same thing as `apply` in standard `data.frame` R by doing a sequential search rather than a binary one since it doesn't know whether my `data.table` is actually sorted by the argument to `by`.

Thanks",4
9474472,02/28/2012 00:13:34,129735,06/27/2009 02:24:52,1126,18,"Effective clustering in R, outputting to other toolkits","I have a pretty big 5868 x 5868 matrix of users vs users that have values that indicate similar behavior. So for example the [i,j] cell of user X and user Y would be some value like 10 if they did the same things 10 times, and 1 if they did the same things 1 time.

I tried running the [cophenetic distances algorithm on SO][1] and this is what I get, and I'm at a loss to see how one can go about picking apart this data more effectively.

Any thoughts or ideas would be greatly appreciated!

![Cophonetic distances][2]


  [1]: http://stackoverflow.com/questions/5639794/in-r-how-can-i-plot-a-similarity-matrix-like-a-block-graph-after-clustering-d
  [2]: http://i.stack.imgur.com/ez5YN.png",r,matlab,matrix,statistics,data-visualization,02/28/2012 13:03:56,off topic,1,102,8,"Effective clustering in R, outputting to other toolkits I have a pretty big 5868 x 5868 matrix of users vs users that have values that indicate similar behavior. So for example the [i,j] cell of user X and user Y would be some value like 10 if they did the same things 10 times, and 1 if they did the same things 1 time.

I tried running the [cophenetic distances algorithm on SO][1] and this is what I get, and I'm at a loss to see how one can go about picking apart this data more effectively.

Any thoughts or ideas would be greatly appreciated!

![Cophonetic distances][2]


  [1]: http://stackoverflow.com/questions/5639794/in-r-how-can-i-plot-a-similarity-matrix-like-a-block-graph-after-clustering-d
  [2]: http://i.stack.imgur.com/ez5YN.png",5
10725564,05/23/2012 18:12:52,825050,07/01/2011 14:35:19,391,18,Find rows with a given difference between values in a column,"For a data.table (or data.frame) in R, I wish to find all rows which contain a value in column 'value' which are a given distance 'distance' from another that value in row with the same key. So, given the following:

    distance <- 22
       key value
       A     1
       B     1
       C     1
       D     1
       A     4
       B     4
       A    23
       B    23
       B    26
       B    26
       C    30

I would like to annotated the original table with a count of how many rows exist with the same key, and a value that is +22 from it:

      key value count
      A     1     1
      B     1     1
      C     1     0
      D     1     0
      A     4     0
      B     4     2
      A    23     0
      B    23     0
      B    26     0
      B    26     0
      C    30     0

I don't really know where to begin with this self-referential approach to manipulating data in R. My initial attempts involved creating a second table and trying to match against that, but that seemed a strange and poor approach.

Note: I'm using the `data.table` package but I'm happy to work from data.frame in this case if that makes things easier.

Reproducible:
    
    require(data.table)
    source <- data.table(data.frame(key=c(""A"",""B"",""C"",""D"",""A"",""B"",""A"",""B"",""B"",""C""),value=c(1,1,1,1,4,4,23,23,26,30)))
    result <- data.table(data.frame(key=c(""A"",""B"",""C"",""D"",""A"",""B"",""A"",""B"",""B"",""B"",""C""),value=c(1,1,1,1,4,4,23,23,26,26,30),count=c(1,1,0,0,0,2,0,0,0,0,0)))",r,datatable,data.frame,,,,open,0,464,11,"Find rows with a given difference between values in a column For a data.table (or data.frame) in R, I wish to find all rows which contain a value in column 'value' which are a given distance 'distance' from another that value in row with the same key. So, given the following:

    distance <- 22
       key value
       A     1
       B     1
       C     1
       D     1
       A     4
       B     4
       A    23
       B    23
       B    26
       B    26
       C    30

I would like to annotated the original table with a count of how many rows exist with the same key, and a value that is +22 from it:

      key value count
      A     1     1
      B     1     1
      C     1     0
      D     1     0
      A     4     0
      B     4     2
      A    23     0
      B    23     0
      B    26     0
      B    26     0
      C    30     0

I don't really know where to begin with this self-referential approach to manipulating data in R. My initial attempts involved creating a second table and trying to match against that, but that seemed a strange and poor approach.

Note: I'm using the `data.table` package but I'm happy to work from data.frame in this case if that makes things easier.

Reproducible:
    
    require(data.table)
    source <- data.table(data.frame(key=c(""A"",""B"",""C"",""D"",""A"",""B"",""A"",""B"",""B"",""C""),value=c(1,1,1,1,4,4,23,23,26,30)))
    result <- data.table(data.frame(key=c(""A"",""B"",""C"",""D"",""A"",""B"",""A"",""B"",""B"",""B"",""C""),value=c(1,1,1,1,4,4,23,23,26,26,30),count=c(1,1,0,0,0,2,0,0,0,0,0)))",3
10635429,05/17/2012 12:05:35,1105809,12/19/2011 11:50:22,67,8,How to install Rapache on CentOs5.8?,"I am trying to install Rapacke on CentOs,but I am facing some problems. How can I install Rapache on my system? Do i need to install other software? If yes then from where and how? ",r,apache2,rapache,,,05/18/2012 17:21:31,off topic,1,36,6,"How to install Rapache on CentOs5.8? I am trying to install Rapacke on CentOs,but I am facing some problems. How can I install Rapache on my system? Do i need to install other software? If yes then from where and how? ",3
5356629,03/18/2011 19:01:34,120731,06/10/2009 17:58:36,1,1,How do I strip the null byte from a string in R?,"How do I strip the null byte from a string in R?  
Something like this:

    > gsub('\0', '', 'doot\0')
    Error: embedded nul in string: '\0'",r,string,,,,03/21/2011 15:55:17,not a real question,1,32,12,"How do I strip the null byte from a string in R? How do I strip the null byte from a string in R?  
Something like this:

    > gsub('\0', '', 'doot\0')
    Error: embedded nul in string: '\0'",2
4110937,11/05/2010 23:23:23,199217,10/29/2009 20:28:01,175,5,How can I find the last row of a dynamically generated matrix in which the value of the column goes below zero?,"I am interested in finding the zero point of a multidimensional autocorrelation function.

To speed up a function to do this, I would like to calculate the acf for only the first n rows.

I have a simplified prototype function that doesn't yet work, partly because this is my first use of the while() function in R. Perhaps there is a better solution?

    set.seed(1)
    a <- cbind(rnorm(10), rnorm(10), rnorm(10), rnorm(10)) + 0.5
    # a is the pretend full acf
    findmin <- function(x, n.row) {
      row.n <- n.row
      anew <- a[1,] #analogous to calculating the acf for the first row
      while(!TRUE %in% anew <0) {
        anew <- a[1:row.n,]
        row.n <- row.n * 2
      }
      return(anew)
    }
 

",r,while-loops,,,,,open,0,168,22,"How can I find the last row of a dynamically generated matrix in which the value of the column goes below zero? I am interested in finding the zero point of a multidimensional autocorrelation function.

To speed up a function to do this, I would like to calculate the acf for only the first n rows.

I have a simplified prototype function that doesn't yet work, partly because this is my first use of the while() function in R. Perhaps there is a better solution?

    set.seed(1)
    a <- cbind(rnorm(10), rnorm(10), rnorm(10), rnorm(10)) + 0.5
    # a is the pretend full acf
    findmin <- function(x, n.row) {
      row.n <- n.row
      anew <- a[1,] #analogous to calculating the acf for the first row
      while(!TRUE %in% anew <0) {
        anew <- a[1:row.n,]
        row.n <- row.n * 2
      }
      return(anew)
    }
 

",2
7712745,10/10/2011 12:28:24,987652,10/10/2011 12:15:29,1,0,While loop in R code- how to make it faster,"In quantstrat package I have located one of the main culprits for slowness of the applyRule function and wonder if there is more efficient to write the while loop. Any feedback would be helpful. For anyone experience wrapping this part into Parallel R. 

As an option apply  would work instead while? Or should I re-write this part into new function such as ruleProc and nextIndex? I am also dveling on Rcpp but that may be a streach. Any help and constructive advice is much appreciated?  



       while (curIndex) {
        timestamp = Dates[curIndex]
        if (isTRUE(hold) & holdtill < timestamp) {
            hold = FALSE
            holdtill = NULL
        }
        types <- sort(factor(names(strategy$rules), levels = c(""pre"",
            ""risk"", ""order"", ""rebalance"", ""exit"", ""enter"", ""entry"",
            ""post"")))
        for (type in types) {
            switch(type, pre = {
                if (length(strategy$rules[[type]]) >= 1) {
                  ruleProc(strategy$rules$pre, timestamp = timestamp,
                    path.dep = path.dep, mktdata = mktdata, portfolio = portfolio,
                    symbol = symbol, ruletype = type, mktinstr = mktinstr)
                }
            }, risk = {
                if (length(strategy$rules$risk) >= 1) {
                  ruleProc(strategy$rules$risk, timestamp = timestamp,
                    path.dep = path.dep, mktdata = mktdata, portfolio = portfolio,
                    symbol = symbol, ruletype = type, mktinstr = mktinstr)
                }
            }, order = {
                if (length(strategy$rules[[type]]) >= 1) {
                  ruleProc(strategy$rules[[type]], timestamp = timestamp,
                    path.dep = path.dep, mktdata = mktdata, portfolio = portfolio,
                    symbol = symbol, ruletype = type, mktinstr = mktinstr,)
                } else {
                  if (isTRUE(path.dep)) {
                    timespan <- paste(""::"", timestamp, sep = """")
                  } else timespan = NULL
                  ruleOrderProc(portfolio = portfolio, symbol = symbol,
                    mktdata = mktdata, timespan = timespan)
                }
            }, rebalance = , exit = , enter = , entry = {
                if (isTRUE(hold)) next()
                if (type == ""exit"") {
                  if (getPosQty(Portfolio = portfolio, Symbol = symbol,
                    Date = timestamp) == 0) next()
                }
                if (length(strategy$rules[[type]]) >= 1) {
                  ruleProc(strategy$rules[[type]], timestamp = timestamp,
                    path.dep = path.dep, mktdata = mktdata, portfolio = portfolio,
                    symbol = symbol, ruletype = type, mktinstr = mktinstr)
                }
                if (isTRUE(path.dep) && length(getOrders(portfolio = portfolio,
                  symbol = symbol, status = ""open"", timespan = timestamp,
                  which.i = TRUE))) {
                }
            }, post = {
                if (length(strategy$rules$post) >= 1) {
                  ruleProc(strategy$rules$post, timestamp = timestamp,
                    path.dep = path.dep, mktdata = mktdata, portfolio = portfolio,
                    symbol = symbol, ruletype = type, mktinstr = mktinstr)
                }
            })
        }
        if (isTRUE(path.dep))
            curIndex <- nextIndex(curIndex)
        else curIndex = FALSE
    }",r,while-loops,quantstrat,,,,open,0,1239,10,"While loop in R code- how to make it faster In quantstrat package I have located one of the main culprits for slowness of the applyRule function and wonder if there is more efficient to write the while loop. Any feedback would be helpful. For anyone experience wrapping this part into Parallel R. 

As an option apply  would work instead while? Or should I re-write this part into new function such as ruleProc and nextIndex? I am also dveling on Rcpp but that may be a streach. Any help and constructive advice is much appreciated?  



       while (curIndex) {
        timestamp = Dates[curIndex]
        if (isTRUE(hold) & holdtill < timestamp) {
            hold = FALSE
            holdtill = NULL
        }
        types <- sort(factor(names(strategy$rules), levels = c(""pre"",
            ""risk"", ""order"", ""rebalance"", ""exit"", ""enter"", ""entry"",
            ""post"")))
        for (type in types) {
            switch(type, pre = {
                if (length(strategy$rules[[type]]) >= 1) {
                  ruleProc(strategy$rules$pre, timestamp = timestamp,
                    path.dep = path.dep, mktdata = mktdata, portfolio = portfolio,
                    symbol = symbol, ruletype = type, mktinstr = mktinstr)
                }
            }, risk = {
                if (length(strategy$rules$risk) >= 1) {
                  ruleProc(strategy$rules$risk, timestamp = timestamp,
                    path.dep = path.dep, mktdata = mktdata, portfolio = portfolio,
                    symbol = symbol, ruletype = type, mktinstr = mktinstr)
                }
            }, order = {
                if (length(strategy$rules[[type]]) >= 1) {
                  ruleProc(strategy$rules[[type]], timestamp = timestamp,
                    path.dep = path.dep, mktdata = mktdata, portfolio = portfolio,
                    symbol = symbol, ruletype = type, mktinstr = mktinstr,)
                } else {
                  if (isTRUE(path.dep)) {
                    timespan <- paste(""::"", timestamp, sep = """")
                  } else timespan = NULL
                  ruleOrderProc(portfolio = portfolio, symbol = symbol,
                    mktdata = mktdata, timespan = timespan)
                }
            }, rebalance = , exit = , enter = , entry = {
                if (isTRUE(hold)) next()
                if (type == ""exit"") {
                  if (getPosQty(Portfolio = portfolio, Symbol = symbol,
                    Date = timestamp) == 0) next()
                }
                if (length(strategy$rules[[type]]) >= 1) {
                  ruleProc(strategy$rules[[type]], timestamp = timestamp,
                    path.dep = path.dep, mktdata = mktdata, portfolio = portfolio,
                    symbol = symbol, ruletype = type, mktinstr = mktinstr)
                }
                if (isTRUE(path.dep) && length(getOrders(portfolio = portfolio,
                  symbol = symbol, status = ""open"", timespan = timestamp,
                  which.i = TRUE))) {
                }
            }, post = {
                if (length(strategy$rules$post) >= 1) {
                  ruleProc(strategy$rules$post, timestamp = timestamp,
                    path.dep = path.dep, mktdata = mktdata, portfolio = portfolio,
                    symbol = symbol, ruletype = type, mktinstr = mktinstr)
                }
            })
        }
        if (isTRUE(path.dep))
            curIndex <- nextIndex(curIndex)
        else curIndex = FALSE
    }",3
5537666,04/04/2011 11:13:46,670781,03/22/2011 07:48:56,23,0,[r] overrepresented Transcription Factor Binding Sites (TFBS),"I was wondering if there is a package available in R for identifying statistically overrepresented Transcription Factor Binding Sites (TFBS) in a set of sequences compared against a control set. Something like F-match.

In my investigation I have not find something useful.

Thanks for all answers,

Lisanne
",r,package,,,,04/04/2011 16:02:26,off topic,1,44,7,"[r] overrepresented Transcription Factor Binding Sites (TFBS) I was wondering if there is a package available in R for identifying statistically overrepresented Transcription Factor Binding Sites (TFBS) in a set of sequences compared against a control set. Something like F-match.

In my investigation I have not find something useful.

Thanks for all answers,

Lisanne
",2
10021866,04/05/2012 02:24:44,1314070,04/04/2012 23:38:32,8,0,Is it possible to evaluate only some of the variables?,"Is it possible to evaluate only some of the variables?

    t<- function(a,b,c){a+b+c}
    t(1,2,c)

which produces the error

> Error in a + b + c : non-numeric argument to binary operator",r,,,,,04/05/2012 09:14:49,not a real question,1,35,10,"Is it possible to evaluate only some of the variables? Is it possible to evaluate only some of the variables?

    t<- function(a,b,c){a+b+c}
    t(1,2,c)

which produces the error

> Error in a + b + c : non-numeric argument to binary operator",1
6246762,06/05/2011 23:57:39,471093,10/09/2010 18:28:29,396,12,slick one-lineRs,"What's your favorite one-liner in `R`? 


Include a short companion example, and limit to one tip per post, please.
Note, `;` is cheating. 

Example: calculate `x[i] / x[i-1]` for a vector `x`,

    x <- 1:10
    Reduce(""/"", as.data.frame(embed(x, 2)))

(collected from R-help, I forget who/when)",r,tips-and-tricks,,,,06/08/2011 01:05:49,not a real question,1,48,2,"slick one-lineRs What's your favorite one-liner in `R`? 


Include a short companion example, and limit to one tip per post, please.
Note, `;` is cheating. 

Example: calculate `x[i] / x[i-1]` for a vector `x`,

    x <- 1:10
    Reduce(""/"", as.data.frame(embed(x, 2)))

(collected from R-help, I forget who/when)",2
10094812,04/10/2012 19:02:43,1079898,12/04/2011 08:58:26,16,0,The for loop again,"df1 <- data.frame(chrom = c(""chr1"",""chr2"", ""chr5""), start=c(10,20,30), end = c(100,200,300), stringsAsFactors=FALSE)

df2 <- data.frame(chrom = c(""chr1"", ""chr4"", ""chr2"", ""chr1""),start=c(15,500,150,200), end = c(75,1000,300,300), stringsAsFactors=FALSE)

I want do the following:

for(i in 1:nrow(df2)){

	if((df1$start <= df2$start && df1$end >= df2$start)|| (df1$start >= df2$start && df1$start <= df2$end)) //only if this condition is true ie if there is overlap

then do:

	x <- df2[which(df2$chrom %in% df1$chrom),]
	
}
 People have been very patient with my queries and it has been a very helpful and learning experience. However I am trying to understand how looping works and the more I am thinking about it the more confused I am getting......for example:

for(i in 1:nrow(df2)){


	x <- df2[which(df2$chrom %in% df1$chrom),]
	
}

does exactly the same as:

x <- df2[which(df2$chrom %in% df1$chrom),], so you don't even need a loop...how is this possible....but I guess which() is doing the looping for you...I guess

Anyways thanks in advance.....
",r,,,,,,open,0,136,4,"The for loop again df1 <- data.frame(chrom = c(""chr1"",""chr2"", ""chr5""), start=c(10,20,30), end = c(100,200,300), stringsAsFactors=FALSE)

df2 <- data.frame(chrom = c(""chr1"", ""chr4"", ""chr2"", ""chr1""),start=c(15,500,150,200), end = c(75,1000,300,300), stringsAsFactors=FALSE)

I want do the following:

for(i in 1:nrow(df2)){

	if((df1$start <= df2$start && df1$end >= df2$start)|| (df1$start >= df2$start && df1$start <= df2$end)) //only if this condition is true ie if there is overlap

then do:

	x <- df2[which(df2$chrom %in% df1$chrom),]
	
}
 People have been very patient with my queries and it has been a very helpful and learning experience. However I am trying to understand how looping works and the more I am thinking about it the more confused I am getting......for example:

for(i in 1:nrow(df2)){


	x <- df2[which(df2$chrom %in% df1$chrom),]
	
}

does exactly the same as:

x <- df2[which(df2$chrom %in% df1$chrom),], so you don't even need a loop...how is this possible....but I guess which() is doing the looping for you...I guess

Anyways thanks in advance.....
",1
5234117,03/08/2011 14:56:26,649996,03/08/2011 14:56:26,1,0,"[R] How to access data frame columns ""dynamically"" in a loop","i have a large data set and i would like to read specific columns or drop all the others.

    data<-read.dta(""file.dta"")

i chose all the column names but the ones i'm interested in:

    var.out<-names(data)[!names(data) %in% c(""iden"", ""name"", ""x_serv"", ""m_serv"")]

and than i'd like to do something like:

    for(i in 1:length(var.out))
    {
       paste(""data$"",var.out[i],sep="""")<-NULL
    }

to drop all the unneeded columns.

is this a good way to get it done?

thank you very much for your help",r,,,,,,open,0,90,11,"[R] How to access data frame columns ""dynamically"" in a loop i have a large data set and i would like to read specific columns or drop all the others.

    data<-read.dta(""file.dta"")

i chose all the column names but the ones i'm interested in:

    var.out<-names(data)[!names(data) %in% c(""iden"", ""name"", ""x_serv"", ""m_serv"")]

and than i'd like to do something like:

    for(i in 1:length(var.out))
    {
       paste(""data$"",var.out[i],sep="""")<-NULL
    }

to drop all the unneeded columns.

is this a good way to get it done?

thank you very much for your help",1
11254381,06/28/2012 23:41:47,1489960,06/28/2012 23:30:32,1,0,convert a table to circle,"I have a rectangle table which have 7 rows and 46 columns the top row and the first columns are the headers. so this can be considered as a matrix and I would like to convert this table into a circles. what I mean when I say convert to circle:
a circle which have 7 tracks and also have radius drawn 45 times to make it like a table and then every box on the tracks get some color which was there in the table.

Drawing the circle is easy and I searched a lot on how to draw the radius of the circle but got nothing. 

Appreciate the help.

Thanks

",r,circle,radius,,,07/21/2012 05:58:24,not a real question,1,108,5,"convert a table to circle I have a rectangle table which have 7 rows and 46 columns the top row and the first columns are the headers. so this can be considered as a matrix and I would like to convert this table into a circles. what I mean when I say convert to circle:
a circle which have 7 tracks and also have radius drawn 45 times to make it like a table and then every box on the tracks get some color which was there in the table.

Drawing the circle is easy and I searched a lot on how to draw the radius of the circle but got nothing. 

Appreciate the help.

Thanks

",3
8719487,01/03/2012 22:22:28,559878,01/01/2011 17:38:21,147,1,How do you install a windows executable R package?,I have an executable R package and want to install it. How would I do this?,r,,,,,01/05/2012 02:53:24,not a real question,1,16,9,How do you install a windows executable R package? I have an executable R package and want to install it. How would I do this?,1
8987536,01/24/2012 13:23:22,179748,09/27/2009 12:50:48,855,11,How to extract data from a text file usign R or PowerShell?,"I have a text file containing data like this:

    This is just text
    -------------------------------
    Username:          SOMETHI           C:                 [Text]
    Account:           DFAG              Finish time:        1-JAN-2011 00:31:58.91
    Process ID:        2028aaB           Start time:        31-DEC-2010 20:27:15.30
    
    This is just text
    -------------------------------
    Username:          SOMEGG            C:                 [Text]
    Account:           DFAG              Finish time:        1-JAN-2011 00:31:58.91
    Process ID:        20dd33DB          Start time:        12-DEC-2010 20:27:15.30
    
    This is just text
    -------------------------------
    Username:          SOMEYY            C:                 [Text]
    Account:           DFAG              Finish time:        1-JAN-2011 00:31:58.91
    Process ID:        202223DB          Start time:        15-DEC-2010 20:27:15.30

Is there a way to extract Username, Finish time, Start time from this kind of data? I'm looking for some starting point usign R or Powershell.",r,powershell,powershell-v2.0,text-processing,,,open,0,420,12,"How to extract data from a text file usign R or PowerShell? I have a text file containing data like this:

    This is just text
    -------------------------------
    Username:          SOMETHI           C:                 [Text]
    Account:           DFAG              Finish time:        1-JAN-2011 00:31:58.91
    Process ID:        2028aaB           Start time:        31-DEC-2010 20:27:15.30
    
    This is just text
    -------------------------------
    Username:          SOMEGG            C:                 [Text]
    Account:           DFAG              Finish time:        1-JAN-2011 00:31:58.91
    Process ID:        20dd33DB          Start time:        12-DEC-2010 20:27:15.30
    
    This is just text
    -------------------------------
    Username:          SOMEYY            C:                 [Text]
    Account:           DFAG              Finish time:        1-JAN-2011 00:31:58.91
    Process ID:        202223DB          Start time:        15-DEC-2010 20:27:15.30

Is there a way to extract Username, Finish time, Start time from this kind of data? I'm looking for some starting point usign R or Powershell.",4
8435108,12/08/2011 17:31:11,1078621,12/03/2011 04:50:16,36,2,Structural equation modeling in R -- are there any good books or tutorials?,"Are there any good resources for learning how to construct structural equation models in R?  A friend asked for help transitioning from SPSS' Amos for structural equation modeling to R.  He has limited R skills and I have limited SEM knowledge.  Are there any books/book chapters/etc along the lines of the [Use R!][1] series that cover SEM packages for R?  


  [1]: http://www.springer.com/series/6991",r,,,,,12/09/2011 18:51:42,not constructive,1,68,13,"Structural equation modeling in R -- are there any good books or tutorials? Are there any good resources for learning how to construct structural equation models in R?  A friend asked for help transitioning from SPSS' Amos for structural equation modeling to R.  He has limited R skills and I have limited SEM knowledge.  Are there any books/book chapters/etc along the lines of the [Use R!][1] series that cover SEM packages for R?  


  [1]: http://www.springer.com/series/6991",1
9919548,03/29/2012 05:26:25,1299887,03/29/2012 05:22:57,1,0,How to use an excel data-set for a multi-line ggplot in R?,"I have a data set in excel that I am trying to create a multiple line plot with on R. The data set contains 7 food groups and the calories consumed daily associated to the groups. As well, there is that set of data over 38 years (from 1970-2008) and I am attempting to use this data set to create a multiple line plot on R. I have tried for hours on end but can not seem to get R to recognize the variables within the data set.",r,,,,,03/29/2012 11:35:47,not a real question,1,88,12,"How to use an excel data-set for a multi-line ggplot in R? I have a data set in excel that I am trying to create a multiple line plot with on R. The data set contains 7 food groups and the calories consumed daily associated to the groups. As well, there is that set of data over 38 years (from 1970-2008) and I am attempting to use this data set to create a multiple line plot on R. I have tried for hours on end but can not seem to get R to recognize the variables within the data set.",1
2098590,01/20/2010 02:03:12,170352,09/08/2009 18:23:21,105,6,Best Versioning System for R,"I have a plethora of both short and long pieces of R code that I would like to track as they evolve. 

Does anyone have any recommendation or experience using version-control software with R? ",r,version-control,,,,01/20/2010 19:56:28,not constructive,1,35,5,"Best Versioning System for R I have a plethora of both short and long pieces of R code that I would like to track as they evolve. 

Does anyone have any recommendation or experience using version-control software with R? ",2
9550638,03/03/2012 23:05:00,1217558,02/18/2012 03:44:59,7,0,R: Interpreting ur.df() results (Unit root test: Dickey-Fuller),"I am running the following unit root test (Dickey-Fuller) on a time series using the ur.df() function in the {urca} package.

The command is:

    summary(ur.df(d.Aus, type = ""drift"", 6))

The output is:

    ############################################### 
    # Augmented Dickey-Fuller Test Unit Root Test # 
    ############################################### 
    
    Test regression drift 
    
    
    Call:
    lm(formula = z.diff ~ z.lag.1 + 1 + z.diff.lag)
    
    Residuals:
          Min        1Q    Median        3Q       Max 
    -0.266372 -0.036882 -0.002716  0.036644  0.230738 
    
    Coefficients:
                 Estimate Std. Error t value Pr(>|t|)   
    (Intercept)  0.001114   0.003238   0.344  0.73089   
    z.lag.1     -0.010656   0.006080  -1.753  0.08031 . 
    z.diff.lag1  0.071471   0.044908   1.592  0.11214   
    z.diff.lag2  0.086806   0.044714   1.941  0.05279 . 
    z.diff.lag3  0.029537   0.044781   0.660  0.50983   
    z.diff.lag4  0.056348   0.044792   1.258  0.20899   
    z.diff.lag5  0.119487   0.044949   2.658  0.00811 **
    z.diff.lag6 -0.082519   0.045237  -1.824  0.06874 . 
    ---
    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
    
    Residual standard error: 0.06636 on 491 degrees of freedom
    Multiple R-squared: 0.04211,	Adjusted R-squared: 0.02845 
    F-statistic: 3.083 on 7 and 491 DF,  p-value: 0.003445 
    
    
    Value of test-statistic is: -1.7525 1.6091 
    
    Critical values for test statistics: 
          1pct  5pct 10pct
    tau2 -3.43 -2.86 -2.57
    phi1  6.43  4.59  3.78


(i) What do the significance codes (Signif. codes) mean? I noticed that some of them where written against: z.lag.1, z.diff.lag.2, z.diff.lag.3 (the ""."" significance code) and z.diff.lag.5 (the ""**"" significance  code).

(ii) The output gives me two (2) values of test statistic: -1.7525 and 1.6091. I know that the ADF test statistic is the first one (i.e. -1.7525). What is the second one then?

(iii) Finally, in order to test the hypothesis for unit root at the 95% significance level, I need to compare my ADF test statistic (i.e. -1.7525) to a critical value, which I normally get from a table. The output here seems to give me the critical values through. However, the question is: which critical value between ""tau2"" and ""phi1"" should I use.

Thank you for your response.",r,statistics,time-series,,,03/04/2012 03:28:33,off topic,1,562,8,"R: Interpreting ur.df() results (Unit root test: Dickey-Fuller) I am running the following unit root test (Dickey-Fuller) on a time series using the ur.df() function in the {urca} package.

The command is:

    summary(ur.df(d.Aus, type = ""drift"", 6))

The output is:

    ############################################### 
    # Augmented Dickey-Fuller Test Unit Root Test # 
    ############################################### 
    
    Test regression drift 
    
    
    Call:
    lm(formula = z.diff ~ z.lag.1 + 1 + z.diff.lag)
    
    Residuals:
          Min        1Q    Median        3Q       Max 
    -0.266372 -0.036882 -0.002716  0.036644  0.230738 
    
    Coefficients:
                 Estimate Std. Error t value Pr(>|t|)   
    (Intercept)  0.001114   0.003238   0.344  0.73089   
    z.lag.1     -0.010656   0.006080  -1.753  0.08031 . 
    z.diff.lag1  0.071471   0.044908   1.592  0.11214   
    z.diff.lag2  0.086806   0.044714   1.941  0.05279 . 
    z.diff.lag3  0.029537   0.044781   0.660  0.50983   
    z.diff.lag4  0.056348   0.044792   1.258  0.20899   
    z.diff.lag5  0.119487   0.044949   2.658  0.00811 **
    z.diff.lag6 -0.082519   0.045237  -1.824  0.06874 . 
    ---
    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
    
    Residual standard error: 0.06636 on 491 degrees of freedom
    Multiple R-squared: 0.04211,	Adjusted R-squared: 0.02845 
    F-statistic: 3.083 on 7 and 491 DF,  p-value: 0.003445 
    
    
    Value of test-statistic is: -1.7525 1.6091 
    
    Critical values for test statistics: 
          1pct  5pct 10pct
    tau2 -3.43 -2.86 -2.57
    phi1  6.43  4.59  3.78


(i) What do the significance codes (Signif. codes) mean? I noticed that some of them where written against: z.lag.1, z.diff.lag.2, z.diff.lag.3 (the ""."" significance code) and z.diff.lag.5 (the ""**"" significance  code).

(ii) The output gives me two (2) values of test statistic: -1.7525 and 1.6091. I know that the ADF test statistic is the first one (i.e. -1.7525). What is the second one then?

(iii) Finally, in order to test the hypothesis for unit root at the 95% significance level, I need to compare my ADF test statistic (i.e. -1.7525) to a critical value, which I normally get from a table. The output here seems to give me the critical values through. However, the question is: which critical value between ""tau2"" and ""phi1"" should I use.

Thank you for your response.",3
8273812,11/25/2011 20:15:10,979313,10/04/2011 21:03:14,84,0,How to get bootstrapped p-values and bootstrapped t-vaules and how does the function boot() work?,"I would like to get the bootstrapped t-value and the bootstrapped p-value of a lm.
I have the following code (basically copied from a paper) which works.

    # First of all you need the following packages
    install.packages(""car"") 
    install.packages(""MASS"")
    install.packages(""boot"")
    library(""car"")
    library(""MASS"")
    library(""boot"")
    
    boot.function <- function(data, indices){
    data <- data[indices,]
    mod <- lm(prestige ~ income + education, data=data) # the liear model

    # the first element of the following vector contains the t-value
    # and the second element is the p-value
    c(summary(mod)[[""coefficients""]][2,3], summary(mod)[[""coefficients""]][2,4])     
    }

Now, I compute the bootstrapping model, which gives me the following:

    duncan.boot <- boot(Duncan, boot.function, 1999)
    duncan.boot

    ORDINARY NONPARAMETRIC BOOTSTRAP
    
    
    Call:
    boot(data = Duncan, statistic = boot.function, R = 1999)
    
    
    Bootstrap Statistics :
            original      bias    std. error
    t1* 5.003310e+00 0.288746545  1.71684664
    t2* 1.053184e-05 0.002701685  0.01642399

I have two questions:

1)
My understanding is that the bootsrapped value is the original plus the bias, which means that
both bootstrapped values (the bootstrapped t-value as well as the bootstrapped p-value) are
greater than the original values. This in turn is not possible, because if the t-value rises
(which means more significance) the p-values MUSST be lower, right? Therefore I think that I
have not yet really understood the output of the boot function (here: duncan.boot). How do I
compute the bootstrapped values?

2)
I do not understand how the boot() works. If you look at
 
    duncan.boot <- boot(Duncan, boot.function, 1999)
you see that I have not passed any arguments for the function ""boot.function"". I suppose
that R sets data = Duncan. But since I have not passed anything for the argument ""indices"",
I do not understand how the following line in the function ""boot.function"" works

    data <- data[indices,]


I hope the questions make sense!??",r,boot,bootstrap,,,,open,0,389,15,"How to get bootstrapped p-values and bootstrapped t-vaules and how does the function boot() work? I would like to get the bootstrapped t-value and the bootstrapped p-value of a lm.
I have the following code (basically copied from a paper) which works.

    # First of all you need the following packages
    install.packages(""car"") 
    install.packages(""MASS"")
    install.packages(""boot"")
    library(""car"")
    library(""MASS"")
    library(""boot"")
    
    boot.function <- function(data, indices){
    data <- data[indices,]
    mod <- lm(prestige ~ income + education, data=data) # the liear model

    # the first element of the following vector contains the t-value
    # and the second element is the p-value
    c(summary(mod)[[""coefficients""]][2,3], summary(mod)[[""coefficients""]][2,4])     
    }

Now, I compute the bootstrapping model, which gives me the following:

    duncan.boot <- boot(Duncan, boot.function, 1999)
    duncan.boot

    ORDINARY NONPARAMETRIC BOOTSTRAP
    
    
    Call:
    boot(data = Duncan, statistic = boot.function, R = 1999)
    
    
    Bootstrap Statistics :
            original      bias    std. error
    t1* 5.003310e+00 0.288746545  1.71684664
    t2* 1.053184e-05 0.002701685  0.01642399

I have two questions:

1)
My understanding is that the bootsrapped value is the original plus the bias, which means that
both bootstrapped values (the bootstrapped t-value as well as the bootstrapped p-value) are
greater than the original values. This in turn is not possible, because if the t-value rises
(which means more significance) the p-values MUSST be lower, right? Therefore I think that I
have not yet really understood the output of the boot function (here: duncan.boot). How do I
compute the bootstrapped values?

2)
I do not understand how the boot() works. If you look at
 
    duncan.boot <- boot(Duncan, boot.function, 1999)
you see that I have not passed any arguments for the function ""boot.function"". I suppose
that R sets data = Duncan. But since I have not passed anything for the argument ""indices"",
I do not understand how the following line in the function ""boot.function"" works

    data <- data[indices,]


I hope the questions make sense!??",3
10159249,04/15/2012 03:19:59,1000343,10/18/2011 03:41:52,3226,122,Determine number of package downloads,"This may not be the appropriate place for this question as this site is more of a coding site but this also seems the most appropriate place for the question compared to the alternatives.  

**Is there a way to determine how many times an R package has been downloaded or some other means of assessing a package's popularity?**

*I apologize in advance if this is not the most appropriate place for this question.  If you have a better suggestion please direct me there.*",r,,,,,04/15/2012 11:28:55,off topic,1,84,5,"Determine number of package downloads This may not be the appropriate place for this question as this site is more of a coding site but this also seems the most appropriate place for the question compared to the alternatives.  

**Is there a way to determine how many times an R package has been downloaded or some other means of assessing a package's popularity?**

*I apologize in advance if this is not the most appropriate place for this question.  If you have a better suggestion please direct me there.*",1
11588765,07/21/2012 01:26:21,412655,08/06/2010 04:37:13,484,7,Using Rcpp with Windows-specific includes,"
I'm trying to write some C++ code that accesses some OS-level things in Windows, using Rcpp. As soon as I include `windows.h` or `shlobj.h`, I get a bunch of compilation errors. When I run this code, it works, so I know I'm getting some of the basics right. But when I uncomment either of the Windows-related `#include` lines, it doesn't work.


    library(inline)

    inc <- '
    #include <iostream>
    #include <stdio.h>
    // #include <windows.h>
    // #include <shlobj.h>

    using namespace std;
    '

    src <- '
        cout << ""foo\\n"";
        printf(""foo2\\n"");

        return Rcpp::wrap(20);
    '

    fun <- cxxfunction(signature(),
                       includes = inc,
                       src, plugin=""Rcpp"")
    fun()


Note: When I run this in RStudio, the output from `cout` and `printf` appear in the console, but when I run it from the Windows RGui, the output doesn't appear. I assume this has something to do with the way RGui handles text output.


When I uncomment those include lines, the errors I get look like this:

    In file included from c:\rtools\gcc-4.6.3\bin\../lib/gcc/i686-w64-mingw32/4.6.3/../../../../i686-w64-mingw32/include/objbase.h:154:0,
                     from c:\rtools\gcc-4.6.3\bin\../lib/gcc/i686-w64-mingw32/4.6.3/../../../../i686-w64-mingw32/include/ole2.h:16,
                     from c:\rtools\gcc-4.6.3\bin\../lib/gcc/i686-w64-mingw32/4.6.3/../../../../i686-w64-mingw32/include/windows.h:94,
                     from file43c2f9e3518.cpp:22:
    c:\rtools\gcc-4.6.3\bin\../lib/gcc/i686-w64-mingw32/4.6.3/../../../../i686-w64-mingw32/include/objidl.h:598:52: error: macro ""Realloc"" requires 3 arguments, but only 2 given
    c:\rtools\gcc-4.6.3\bin\../lib/gcc/i686-w64-mingw32/4.6.3/../../../../i686-w64-mingw32/include/objidl.h:598:56: error: ISO C++ forbids initialization of member 'Realloc' [-fpermissive]
    c:\rtools\gcc-4.6.3\bin\../lib/gcc/i686-w64-mingw32/4.6.3/../../../../i686-w64-mingw32/include/objidl.h:598:56: error: making 'Realloc' static [-fpermissive]

... and so on

Any hints on how to make this work?
",r,rcpp,,,,,open,0,374,5,"Using Rcpp with Windows-specific includes 
I'm trying to write some C++ code that accesses some OS-level things in Windows, using Rcpp. As soon as I include `windows.h` or `shlobj.h`, I get a bunch of compilation errors. When I run this code, it works, so I know I'm getting some of the basics right. But when I uncomment either of the Windows-related `#include` lines, it doesn't work.


    library(inline)

    inc <- '
    #include <iostream>
    #include <stdio.h>
    // #include <windows.h>
    // #include <shlobj.h>

    using namespace std;
    '

    src <- '
        cout << ""foo\\n"";
        printf(""foo2\\n"");

        return Rcpp::wrap(20);
    '

    fun <- cxxfunction(signature(),
                       includes = inc,
                       src, plugin=""Rcpp"")
    fun()


Note: When I run this in RStudio, the output from `cout` and `printf` appear in the console, but when I run it from the Windows RGui, the output doesn't appear. I assume this has something to do with the way RGui handles text output.


When I uncomment those include lines, the errors I get look like this:

    In file included from c:\rtools\gcc-4.6.3\bin\../lib/gcc/i686-w64-mingw32/4.6.3/../../../../i686-w64-mingw32/include/objbase.h:154:0,
                     from c:\rtools\gcc-4.6.3\bin\../lib/gcc/i686-w64-mingw32/4.6.3/../../../../i686-w64-mingw32/include/ole2.h:16,
                     from c:\rtools\gcc-4.6.3\bin\../lib/gcc/i686-w64-mingw32/4.6.3/../../../../i686-w64-mingw32/include/windows.h:94,
                     from file43c2f9e3518.cpp:22:
    c:\rtools\gcc-4.6.3\bin\../lib/gcc/i686-w64-mingw32/4.6.3/../../../../i686-w64-mingw32/include/objidl.h:598:52: error: macro ""Realloc"" requires 3 arguments, but only 2 given
    c:\rtools\gcc-4.6.3\bin\../lib/gcc/i686-w64-mingw32/4.6.3/../../../../i686-w64-mingw32/include/objidl.h:598:56: error: ISO C++ forbids initialization of member 'Realloc' [-fpermissive]
    c:\rtools\gcc-4.6.3\bin\../lib/gcc/i686-w64-mingw32/4.6.3/../../../../i686-w64-mingw32/include/objidl.h:598:56: error: making 'Realloc' static [-fpermissive]

... and so on

Any hints on how to make this work?
",2
11719847,07/30/2012 10:41:17,1562626,07/30/2012 10:14:01,1,0,merging two dendrograms at a specific node,"I would like to either:

1) remove a subtree and then merge a new subtree to the original dendrogram so that it is in the same position as the one removed.
2)or replace a subtree with another one.

I know that merge() can merge two dendrograms at the top. Does it also merge it at a specified node. If so how? If not, is there another method that would do that?

I know that cut() cuts the dendrogram at a certain height or into a specific number of node. But how do I make it remove only a specific subtree?

The specification of a subtree would be the attribute of the first node in it. eg attr(n,""attribute"")== something, which can be done through dendrapply.

Here's a sample code of how the dendrograms are made.
==

    library(""stats"")
    library(""fastcluster"")
    
    x=matrix(c(1:20),ncol=4)
    y=matrix(c(21:40),ncol=4)
    
    #creating hclusters
    xcl=hclust.vector(x)
    ycl=hclust.vector(y)
    
    #converting to dendrograms
    xdend=as.dendrogram(xcl)
    ydend=as.dendrogram(ycl)
    
    # merging two dendrograms at the top
    zdend=merge(xdend,ydend)

==

Thanks
",r,dendrogram,,,,,open,0,201,7,"merging two dendrograms at a specific node I would like to either:

1) remove a subtree and then merge a new subtree to the original dendrogram so that it is in the same position as the one removed.
2)or replace a subtree with another one.

I know that merge() can merge two dendrograms at the top. Does it also merge it at a specified node. If so how? If not, is there another method that would do that?

I know that cut() cuts the dendrogram at a certain height or into a specific number of node. But how do I make it remove only a specific subtree?

The specification of a subtree would be the attribute of the first node in it. eg attr(n,""attribute"")== something, which can be done through dendrapply.

Here's a sample code of how the dendrograms are made.
==

    library(""stats"")
    library(""fastcluster"")
    
    x=matrix(c(1:20),ncol=4)
    y=matrix(c(21:40),ncol=4)
    
    #creating hclusters
    xcl=hclust.vector(x)
    ycl=hclust.vector(y)
    
    #converting to dendrograms
    xdend=as.dendrogram(xcl)
    ydend=as.dendrogram(ycl)
    
    # merging two dendrograms at the top
    zdend=merge(xdend,ydend)

==

Thanks
",2
4774781,01/23/2011 16:07:02,37751,11/14/2008 18:18:35,4816,117,using R's internal tar function on a single file  ,"R has a handy cross platform tar() function that can tar and gzip files. It seems this function was designed to tar up entire directories. I was hoping to use this function to tar and compress a subset of a directory, or a single file. I can't seem to do this, however. I was expecting the following to tar up all CSV files in the current working directory:

    tar( ""tst.tgz"", ""./*.csv"", compression=""gzip"" )

Naturally I also tried this function without the ""./"" to no avail. So am I doing something wrong, or is there just no good way to use wildcards in the path of this function? 

I've temporarily gotten around this by creating a temp directory, copying my files, and then tarring the whole temp dir. But I was hoping for a bit simpler solution. That would not require copying the files which is somewhat time consuming for large files. 
",r,tar,,,,,open,0,154,11,"using R's internal tar function on a single file   R has a handy cross platform tar() function that can tar and gzip files. It seems this function was designed to tar up entire directories. I was hoping to use this function to tar and compress a subset of a directory, or a single file. I can't seem to do this, however. I was expecting the following to tar up all CSV files in the current working directory:

    tar( ""tst.tgz"", ""./*.csv"", compression=""gzip"" )

Naturally I also tried this function without the ""./"" to no avail. So am I doing something wrong, or is there just no good way to use wildcards in the path of this function? 

I've temporarily gotten around this by creating a temp directory, copying my files, and then tarring the whole temp dir. But I was hoping for a bit simpler solution. That would not require copying the files which is somewhat time consuming for large files. 
",2
6969344,08/06/2011 22:21:35,636656,02/27/2011 17:23:21,2867,96,Is it possible/advisable to skip roxygen in favor of roxygen2?,"I've recently been pointed towards Roxygen to solve my documentation woes/laziness.  But then there's this shiny Roxygen2 which, in my understanding, is somewhat its own thing.  Hadley's package tools require the use of Roxygen2, but there doesn't seem to be much by way of a walk-through anywhere.  

Given that I'm starting from scratch in the learning process, should I jump straight over Roxygen and go to Roxygen2, and if so what resources are out there for learning it (preferably those which don't rely on prior knowledge of Roxygen)?",r,roxygen,roxygen2,,,08/07/2011 20:12:23,not a real question,1,91,10,"Is it possible/advisable to skip roxygen in favor of roxygen2? I've recently been pointed towards Roxygen to solve my documentation woes/laziness.  But then there's this shiny Roxygen2 which, in my understanding, is somewhat its own thing.  Hadley's package tools require the use of Roxygen2, but there doesn't seem to be much by way of a walk-through anywhere.  

Given that I'm starting from scratch in the learning process, should I jump straight over Roxygen and go to Roxygen2, and if so what resources are out there for learning it (preferably those which don't rely on prior knowledge of Roxygen)?",3
10653125,05/18/2012 12:59:34,168775,09/04/2009 20:42:51,5766,306,Labeling coplot subplots,"Is there a way to get the labels of each ""given"" variable to show up in it's respective subplot? For example, in the following plot (`x` and `y` numeric, `c` factor, called using `coplot(y ~ x | c)`), it would be helpful to have a little textbox in or label above each plot indicating which factor of `c` the plot represents (click for embiggerment):

[![large-ish coplot][1]][2]

Thanks!

  [1]: http://i.stack.imgur.com/Y6tZY.png
  [2]: http://i.stack.imgur.com/PE8fj.png",r,plot,,,,,open,0,71,3,"Labeling coplot subplots Is there a way to get the labels of each ""given"" variable to show up in it's respective subplot? For example, in the following plot (`x` and `y` numeric, `c` factor, called using `coplot(y ~ x | c)`), it would be helpful to have a little textbox in or label above each plot indicating which factor of `c` the plot represents (click for embiggerment):

[![large-ish coplot][1]][2]

Thanks!

  [1]: http://i.stack.imgur.com/Y6tZY.png
  [2]: http://i.stack.imgur.com/PE8fj.png",2
9281527,02/14/2012 17:34:45,989691,10/11/2011 14:08:18,416,6,How to Start a Local R User Group - Best Practices,"I'm thinking about trying to start a local user group in the greater Munich area in Germany. 

In that respect, I wanted to ask for some **advice/best practices/""Dos and Don'ts""** from people that have already started a local user group.",r,open-source,usergroups,,,02/14/2012 18:33:02,off topic,1,40,11,"How to Start a Local R User Group - Best Practices I'm thinking about trying to start a local user group in the greater Munich area in Germany. 

In that respect, I wanted to ask for some **advice/best practices/""Dos and Don'ts""** from people that have already started a local user group.",3
8598602,12/22/2011 01:49:08,1106532,12/19/2011 18:49:16,339,2,creating a barplot with breaks and densities,"I am looking to create a specific kind of barplot and am not expert in ggplot2 or R graphics. The height of each bar corresponds to the density vector below. (The sum of the density vector is one.)

The x-axis breaks for each bar are identified in the breaks vector. The breaks are contiguous. The length of the density and the breaks vector are identical.

To illustrate, the density of the first bar is 0%. The range of the bar corresponds to -Inf to -4.6. The next bar has a height of 2.43e-05 and the x-axis ranges from -4.6 to -4.4. The second bar has height 0 and the x-axis ranges from -4.4 to -4.2 an so on. The last bar has a density of 0% and ranges from 2.6 to Infinity. (Naturally, we would set the xlim of the plot object to min and max of the breaks so that the first and last bars of 0% density are not plotted). 

How does one create this barplot in R?

    # density for each bar
    density = c(0, 2.43053372991266e-05, 0, 2.56155325481663e-05, 7.85928661230842e-05, 
    6.65974683477407e-05, 0.000191167180262139, 0.000391190191728852, 
    0.000773173013145194, 0.000994581155560843, 0.00186993240600829, 
    0.00301523228215973, 0.0024027636820586, 0.00178958309972533, 
    0.00197757576002083, 0.0037305807759235, 0.00751360121824956, 
    0.0161785199339545, 0.0285084660871918, 0.0470377898959775, 0.0749650429960432, 
    0.0995404136577645, 0.122970891515022, 0.137345727945268, 0.129721517357472, 
    0.111609726931989, 0.0833285279873897, 0.0569221001742823, 0.034426013022441, 
    0.0191745738000343, 0.00810133792335031, 0.00342022026994395, 
    0.00120148435128416, 0.000598579482992183, 5.22199596137814e-05, 
    5.23550471126253e-05, 0)
    
    # breaks for barplot
    breaks = c(-4.6, -4.4, -4.2, -4, -3.8, -3.6, -3.4, -3.2, -3, -2.8, -2.6, 
    -2.4, -2.2, -2, -1.8, -1.6, -1.4, -1.2, -1, -0.8, -0.6, -0.399999999999999, 
    -0.199999999999999, 0, 0.2, 0.4, 0.600000000000001, 0.800000000000001, 
    1, 1.2, 1.4, 1.6, 1.8, 2, 2.2, 2.4, 2.6)

",r,graphics,plot,ggplot2,histogram,,open,0,323,7,"creating a barplot with breaks and densities I am looking to create a specific kind of barplot and am not expert in ggplot2 or R graphics. The height of each bar corresponds to the density vector below. (The sum of the density vector is one.)

The x-axis breaks for each bar are identified in the breaks vector. The breaks are contiguous. The length of the density and the breaks vector are identical.

To illustrate, the density of the first bar is 0%. The range of the bar corresponds to -Inf to -4.6. The next bar has a height of 2.43e-05 and the x-axis ranges from -4.6 to -4.4. The second bar has height 0 and the x-axis ranges from -4.4 to -4.2 an so on. The last bar has a density of 0% and ranges from 2.6 to Infinity. (Naturally, we would set the xlim of the plot object to min and max of the breaks so that the first and last bars of 0% density are not plotted). 

How does one create this barplot in R?

    # density for each bar
    density = c(0, 2.43053372991266e-05, 0, 2.56155325481663e-05, 7.85928661230842e-05, 
    6.65974683477407e-05, 0.000191167180262139, 0.000391190191728852, 
    0.000773173013145194, 0.000994581155560843, 0.00186993240600829, 
    0.00301523228215973, 0.0024027636820586, 0.00178958309972533, 
    0.00197757576002083, 0.0037305807759235, 0.00751360121824956, 
    0.0161785199339545, 0.0285084660871918, 0.0470377898959775, 0.0749650429960432, 
    0.0995404136577645, 0.122970891515022, 0.137345727945268, 0.129721517357472, 
    0.111609726931989, 0.0833285279873897, 0.0569221001742823, 0.034426013022441, 
    0.0191745738000343, 0.00810133792335031, 0.00342022026994395, 
    0.00120148435128416, 0.000598579482992183, 5.22199596137814e-05, 
    5.23550471126253e-05, 0)
    
    # breaks for barplot
    breaks = c(-4.6, -4.4, -4.2, -4, -3.8, -3.6, -3.4, -3.2, -3, -2.8, -2.6, 
    -2.4, -2.2, -2, -1.8, -1.6, -1.4, -1.2, -1, -0.8, -0.6, -0.399999999999999, 
    -0.199999999999999, 0, 0.2, 0.4, 0.600000000000001, 0.800000000000001, 
    1, 1.2, 1.4, 1.6, 1.8, 2, 2.2, 2.4, 2.6)

",5
8198035,11/19/2011 23:03:33,410137,08/03/2010 21:03:48,117,1,Should I use R or Octave for modification of tab delimitated text files?,"I have a bunch of text files with tab delimited data, like so

    X    Y    Z
    1    2    Q
    K    4    2

I want to examine these for certain properties, and then modify them in different ways depending on the result, eventually creating new text files with information added and/or deleted.

Now, according to my own investigations, both R and Octave could probably do this. Should I prefer one of them over the other for this task?
",r,octave,,,,,open,0,102,13,"Should I use R or Octave for modification of tab delimitated text files? I have a bunch of text files with tab delimited data, like so

    X    Y    Z
    1    2    Q
    K    4    2

I want to examine these for certain properties, and then modify them in different ways depending on the result, eventually creating new text files with information added and/or deleted.

Now, according to my own investigations, both R and Octave could probably do this. Should I prefer one of them over the other for this task?
",2
3588961,08/28/2010 00:41:16,282892,02/28/2010 00:19:02,8,0,Specifying formula in R with glm without explicit declaration of each covariate,"I would like to force specific variables into glm regressions without fully specifying each one.  My real data set has ~200 variables.  I haven't been able to find samples of this in my online searching thus far.

For example (with just 3 variables):

    n=200
    set.seed(39) 
    samp = data.frame(W1 = runif(n, min = 0, max = 1), W2=runif(n, min = 0, max = 5)) 
    samp = transform(samp, # add A
    A = rbinom(n, 1, 1/(1+exp(-(W1^2-4*W1+1))))) 
    samp = transform(samp, # add Y
    Y = rbinom(n, 1,1/(1+exp(-(A-sin(W1^2)+sin(W2^2)*A+10*log(W1)*A+15*log(W2)-1+rnorm(1,mean=0,sd=.25))))))

If I want to include all main terms, this has an easy shortcut:

    glm(Y~., family=binomial, data=samp)

But say I want to include all main terms (W1, W2, and A) plus W2^2:

    glm(Y~A+W1+W2+I(W2^2), family=binomial, data=samp)

Is there a shortcut for this?

[editing self before publishing:] This works! `glm(formula = Y ~ . + I(W2^2), family = binomial, data = samp)` 

Okay, so what about this one!

I want to omit one main terms variable and include only two main terms (A, W2) and W2^2 and W2^2:A:

    glm(Y~A+W2+A*I(W2^2), family=binomial, data=samp)

Obviously with just a few variables no shortcut is really needed, but I work with high dimensional data.  The current data set has ""only"" 200 variables, but some others have thousands and thousands.


",r,main,regression,terms,glm,,open,0,234,12,"Specifying formula in R with glm without explicit declaration of each covariate I would like to force specific variables into glm regressions without fully specifying each one.  My real data set has ~200 variables.  I haven't been able to find samples of this in my online searching thus far.

For example (with just 3 variables):

    n=200
    set.seed(39) 
    samp = data.frame(W1 = runif(n, min = 0, max = 1), W2=runif(n, min = 0, max = 5)) 
    samp = transform(samp, # add A
    A = rbinom(n, 1, 1/(1+exp(-(W1^2-4*W1+1))))) 
    samp = transform(samp, # add Y
    Y = rbinom(n, 1,1/(1+exp(-(A-sin(W1^2)+sin(W2^2)*A+10*log(W1)*A+15*log(W2)-1+rnorm(1,mean=0,sd=.25))))))

If I want to include all main terms, this has an easy shortcut:

    glm(Y~., family=binomial, data=samp)

But say I want to include all main terms (W1, W2, and A) plus W2^2:

    glm(Y~A+W1+W2+I(W2^2), family=binomial, data=samp)

Is there a shortcut for this?

[editing self before publishing:] This works! `glm(formula = Y ~ . + I(W2^2), family = binomial, data = samp)` 

Okay, so what about this one!

I want to omit one main terms variable and include only two main terms (A, W2) and W2^2 and W2^2:A:

    glm(Y~A+W2+A*I(W2^2), family=binomial, data=samp)

Obviously with just a few variables no shortcut is really needed, but I work with high dimensional data.  The current data set has ""only"" 200 variables, but some others have thousands and thousands.


",5
7333801,09/07/2011 12:20:24,177390,09/22/2009 20:22:06,365,7,Using snow (and snowfall) with AWS for parallel processing in R,"In relation to my earlier [similar SO question ][1], I tried using snow/snowfall on AWS for parallel computing.

What I did was:

- In the `sfInit()` function, I provided the public DNS to `socketHosts` parameter like so 
`sfInit(parallel=TRUE,socketHosts =list(""ec2-00-00-00-000.compute-1.amazonaws.com""))`
- The error returned was `Permission denied (publickey)`
- I then followed the instructions (I presume correctly!) on http://www.imbi.uni-freiburg.de/parallel/ in the 'Passwordless Secure Shell (SSH) login' section
- I just cat the contents of the .pem file that I created on AWS into the ~/.ssh/authorized_keys of the AWS instance I want to connect to from my master AWS instance and for the master AWS instance as well

Is there anything I am missing out ?
I would be very grateful if users can share their experiences in the use of snow on AWS.

Thank you very much for your suggestions.


  [1]: http://stackoverflow.com/questions/7241244/using-aws-for-parallel-processing-with-r",r,amazon-web-services,parallel-processing,snowfall,,,open,0,134,11,"Using snow (and snowfall) with AWS for parallel processing in R In relation to my earlier [similar SO question ][1], I tried using snow/snowfall on AWS for parallel computing.

What I did was:

- In the `sfInit()` function, I provided the public DNS to `socketHosts` parameter like so 
`sfInit(parallel=TRUE,socketHosts =list(""ec2-00-00-00-000.compute-1.amazonaws.com""))`
- The error returned was `Permission denied (publickey)`
- I then followed the instructions (I presume correctly!) on http://www.imbi.uni-freiburg.de/parallel/ in the 'Passwordless Secure Shell (SSH) login' section
- I just cat the contents of the .pem file that I created on AWS into the ~/.ssh/authorized_keys of the AWS instance I want to connect to from my master AWS instance and for the master AWS instance as well

Is there anything I am missing out ?
I would be very grateful if users can share their experiences in the use of snow on AWS.

Thank you very much for your suggestions.


  [1]: http://stackoverflow.com/questions/7241244/using-aws-for-parallel-processing-with-r",4
