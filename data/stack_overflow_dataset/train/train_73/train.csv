PostId,PostCreationDate,OwnerUserId,OwnerCreationDate,ReputationAtPostCreation,OwnerUndeletedAnswerCountAtPostTime,Title,BodyMarkdown,Tag1,Tag2,Tag3,Tag4,Tag5,PostClosedDate,OpenStatus,OpenStatusInt,BodyLength,TitleLength,TitleConcatWithBody,NumberOfTags
6110310,05/24/2011 12:21:26,766066,05/23/2011 13:15:18,8,0,count number of part of string by columns,"i have a text file like this:

    V1 V2     V3
    X  N    aaaaaabbbabab
    C  T    ababaaabaaabb
    V  H    babbbabaabbba


what i want to do is count how much a and how much b there is in column of each V3 .

so the output would be like this:

      col1 col2 col3 ....... col13
    a  2     2    2             1
    b  1     1    1             2


how this can be done ??

i tried the count function along with sub-string , but it did not worked

thanks
",r,,,,,,open,0,158,8,"count number of part of string by columns i have a text file like this:

    V1 V2     V3
    X  N    aaaaaabbbabab
    C  T    ababaaabaaabb
    V  H    babbbabaabbba


what i want to do is count how much a and how much b there is in column of each V3 .

so the output would be like this:

      col1 col2 col3 ....... col13
    a  2     2    2             1
    b  1     1    1             2


how this can be done ??

i tried the count function along with sub-string , but it did not worked

thanks
",1
7719750,10/10/2011 23:20:54,471324,10/10/2010 04:44:55,104,6,Is R and Hadoop related?,"Is R and Hadoop related? As I understand both are used for large scale data analysis, and calculations. Also I noticed Google and Facebook use R, Mapreduce framework is from Google and they use it for search. 

Is R and Hadoop even comparable? If not for what purpose each one is used?",r,comparison,hadoop,analysis,large-data,10/11/2011 01:49:57,not a real question,1,52,5,"Is R and Hadoop related? Is R and Hadoop related? As I understand both are used for large scale data analysis, and calculations. Also I noticed Google and Facebook use R, Mapreduce framework is from Google and they use it for search. 

Is R and Hadoop even comparable? If not for what purpose each one is used?",5
8936099,01/20/2012 02:30:17,412082,08/05/2010 14:54:00,1571,9,Returning multiple objects in an R function,"How can I return multiple objects in an R function? In Java, I would make a Class, maybe ""Person"" which has some private variables and encapsulates, maybe, height, age, etc. 

But in R, I need to pass around groups of data. For example, how can I make an R function return both an list of characters and an integer?

",r,function,,,,,open,0,59,7,"Returning multiple objects in an R function How can I return multiple objects in an R function? In Java, I would make a Class, maybe ""Person"" which has some private variables and encapsulates, maybe, height, age, etc. 

But in R, I need to pass around groups of data. For example, how can I make an R function return both an list of characters and an integer?

",2
10824440,05/30/2012 21:19:47,721975,04/23/2011 18:04:39,66,0,How to install and manage multiple versions of R,"I am working on a machine that has older version of R. I don't have root access on the machine, and the sys admin is on a vacation. 

So the question I have is, how can I install and manage (the packages, especially) the latest version of R? Obviously I need to download the R package and install it, but a few detailed instructions from the community will help.

R noob here. Help will be greatly appreciated. Thanks",r,ubuntu,installation,,,,open,0,77,9,"How to install and manage multiple versions of R I am working on a machine that has older version of R. I don't have root access on the machine, and the sys admin is on a vacation. 

So the question I have is, how can I install and manage (the packages, especially) the latest version of R? Obviously I need to download the R package and install it, but a few detailed instructions from the community will help.

R noob here. Help will be greatly appreciated. Thanks",3
8420288,12/07/2011 18:09:39,1078084,12/02/2011 19:19:37,19,0,R: How can I delete rows if an element in a row satisfies certain characteristic?,"I am trying to figure out a way to delete rows of matrix if a cell in that row satisfies a certain characteristic. For example:

    > mm <- matrix(c(1,2,3,2,3,4,1,2,3,4),5,2)
    > mm
         [,1] [,2]
    [1,]    1    4
    [2,]    2    1
    [3,]    3    2
    [4,]    2    3
    [5,]    3    4

I want to delete rows if the 1st column element in that row is 2. At the end I want this:

  

       [,1] [,2]
    [1,]    1    4
    [2,]    3    2
    [3,]    3    4

How could I do this?

And what about a more general method if instead of deleting all rows who's first column element is 2, I needed to delete rows who's first column element corresponds to a set of numbers that are contained in a list? For example

    delete_list <- c(2,3)

What is the best way to do this?

Thank You in advance.",r,delete-row,,,,,open,0,234,15,"R: How can I delete rows if an element in a row satisfies certain characteristic? I am trying to figure out a way to delete rows of matrix if a cell in that row satisfies a certain characteristic. For example:

    > mm <- matrix(c(1,2,3,2,3,4,1,2,3,4),5,2)
    > mm
         [,1] [,2]
    [1,]    1    4
    [2,]    2    1
    [3,]    3    2
    [4,]    2    3
    [5,]    3    4

I want to delete rows if the 1st column element in that row is 2. At the end I want this:

  

       [,1] [,2]
    [1,]    1    4
    [2,]    3    2
    [3,]    3    4

How could I do this?

And what about a more general method if instead of deleting all rows who's first column element is 2, I needed to delete rows who's first column element corresponds to a set of numbers that are contained in a list? For example

    delete_list <- c(2,3)

What is the best way to do this?

Thank You in advance.",2
8867001,01/15/2012 01:59:56,912429,08/25/2011 15:17:36,78,0,Loading stock information of Japan using quantmod package in R,"I encounter one problem of using R/quantmod package. I can get the stock information for Korea, but I failed in getting the information for Japan:

    getSymbols(""DEXKOUS"",src=""FRED"") #load Korea
[1] ""DEXKOUS""
      
    getSymbols(""DEXJPUS"",src=""FRED"") #load Japan
    
Error as.POSIXlt.character(x, tz, ...) : 
  character string is not in a standard unambiguous format

Your comments are welcome.",r,quantmod,japan,,,04/30/2012 03:23:27,too localized,1,66,10,"Loading stock information of Japan using quantmod package in R I encounter one problem of using R/quantmod package. I can get the stock information for Korea, but I failed in getting the information for Japan:

    getSymbols(""DEXKOUS"",src=""FRED"") #load Korea
[1] ""DEXKOUS""
      
    getSymbols(""DEXJPUS"",src=""FRED"") #load Japan
    
Error as.POSIXlt.character(x, tz, ...) : 
  character string is not in a standard unambiguous format

Your comments are welcome.",3
7571409,09/27/2011 15:08:05,552650,12/23/2010 17:21:34,96,1,Evaluation of as.integer(max(factorize(15))),"This appears quite bizarre to me and I would like an explanation.  I

    library(gmp)
    factorize(15) # => ""3"" ""5""
    max(factorize(15)) # => ""5""
    as.integer(""5"") # => 5
    as.integer(max(factorize(15))) # => 1 0 0 0 1 0 0 0 1 0 0 0 5 0 0 0

I can do what I want with:

    max(as.numeric(factorize(15))) # => 15

But it shook me that I couldn't rely on nesting functions inside of functions in a supposedly scheme-like language.  Am I missing anything?",r,,,,,,open,0,98,3,"Evaluation of as.integer(max(factorize(15))) This appears quite bizarre to me and I would like an explanation.  I

    library(gmp)
    factorize(15) # => ""3"" ""5""
    max(factorize(15)) # => ""5""
    as.integer(""5"") # => 5
    as.integer(max(factorize(15))) # => 1 0 0 0 1 0 0 0 1 0 0 0 5 0 0 0

I can do what I want with:

    max(as.numeric(factorize(15))) # => 15

But it shook me that I couldn't rely on nesting functions inside of functions in a supposedly scheme-like language.  Am I missing anything?",1
10721517,05/23/2012 13:58:27,1165199,01/23/2012 15:02:55,41,0,Count number of times data is in another dataframe in R,"I have a three dataframes, and I want to add some columns to the first dataframe which counts the number of times the first two columns in the first dataframe appear in the other dataframes e.g.

dataframe - x   
a b  
1 1  
1 2  
2 1  
2 2  

dataframe - y  
a b  
1 1  
1 1  
1 2  
2 2  
2 2

dataframe - z  
a b  
1 2  
2 1  
2 1   
2 2  

So the first dataframe would become  
a b y z    
1 1 2 0  
1 2 1 1  
2 1 0 2  
2 2 1 2

I have ways to do this, e.g. I am currently doing 

    x$y<- sapply(1:nrow(x), function(i){
        sum(y$a == x$a[i] & y$b == x$b[i])
      }

    x$z<- sapply(1:nrow(x), function(i){
        sum(z$a == x$a[i] & z$b == x$b[i])
      }

But my dataframe is very large and my way takes a while to complete so I was wondering of the quickest way to do this.

Please ask if anything is unclear.

Thanks in advance

",r,count,row,data.frame,,,open,0,222,11,"Count number of times data is in another dataframe in R I have a three dataframes, and I want to add some columns to the first dataframe which counts the number of times the first two columns in the first dataframe appear in the other dataframes e.g.

dataframe - x   
a b  
1 1  
1 2  
2 1  
2 2  

dataframe - y  
a b  
1 1  
1 1  
1 2  
2 2  
2 2

dataframe - z  
a b  
1 2  
2 1  
2 1   
2 2  

So the first dataframe would become  
a b y z    
1 1 2 0  
1 2 1 1  
2 1 0 2  
2 2 1 2

I have ways to do this, e.g. I am currently doing 

    x$y<- sapply(1:nrow(x), function(i){
        sum(y$a == x$a[i] & y$b == x$b[i])
      }

    x$z<- sapply(1:nrow(x), function(i){
        sum(z$a == x$a[i] & z$b == x$b[i])
      }

But my dataframe is very large and my way takes a while to complete so I was wondering of the quickest way to do this.

Please ask if anything is unclear.

Thanks in advance

",4
6524478,06/29/2011 17:00:52,199217,10/29/2009 20:28:01,1378,24,Is it possible to align the title of a ggplot to the right?,"I am generating a figure that will be used as a column of labels to the right of a three-panel figure, and I would like the title of the figure to right-align as do the labels in the figure itself.

here is a minimal example in which I would like to right-align the title 'words'.

    ggplot() + 
      geom_text(aes(y = 1, x = seq(4), label = c('fee', 'fi', 'fo', 'fum'), hjust = 1)) +      
      opts(title = 'words') + 
      coord_flip() + 
      scale_y_continuous(breaks = c(0,0), limits = c(0,1)) 


Which produces this:

![enter image description here][1]

  [1]: http://i.stack.imgur.com/42hQN.png
",r,ggplot2,,,,,open,0,125,13,"Is it possible to align the title of a ggplot to the right? I am generating a figure that will be used as a column of labels to the right of a three-panel figure, and I would like the title of the figure to right-align as do the labels in the figure itself.

here is a minimal example in which I would like to right-align the title 'words'.

    ggplot() + 
      geom_text(aes(y = 1, x = seq(4), label = c('fee', 'fi', 'fo', 'fum'), hjust = 1)) +      
      opts(title = 'words') + 
      coord_flip() + 
      scale_y_continuous(breaks = c(0,0), limits = c(0,1)) 


Which produces this:

![enter image description here][1]

  [1]: http://i.stack.imgur.com/42hQN.png
",2
5008092,02/15/2011 18:49:24,211116,11/14/2009 18:45:27,4350,227,"Setting up ""configure"" for openMP in R","I have an R package which is easily sped up by using OpenMP. If your compiler supports it then you get the win, if it doesn't then the pragmas are ignored and you get one core.

My problem is how to get the package build system to use the right compiler options and libraries. Currently I have:

    PKG_CPPFLAGS=-fopenmp
    PKG_LIBS=-fopenmp

hardcoded into src/Makevars on my machine, and this builds it with OpenMP support. But it produces a warning about non-standard compiler flags on check, and will probably fail hard on a machine with no openMP capabilities.

The solution seems to be to use configure and autoconf. There's some information around here:

http://cran.r-project.org/doc/manuals/R-exts.html#Using-Makevars

including a complex example to compile in odbc functionality. But I can't see how to begin tweaking that to check for openmp and libgomp. 

None of the R packages I've looked at that talk about using openMP seem to have this set up either.

So does anyone have a walkthrough for setting up an R package with OpenMP?
",r,packages,openmp,configure,,,open,0,169,7,"Setting up ""configure"" for openMP in R I have an R package which is easily sped up by using OpenMP. If your compiler supports it then you get the win, if it doesn't then the pragmas are ignored and you get one core.

My problem is how to get the package build system to use the right compiler options and libraries. Currently I have:

    PKG_CPPFLAGS=-fopenmp
    PKG_LIBS=-fopenmp

hardcoded into src/Makevars on my machine, and this builds it with OpenMP support. But it produces a warning about non-standard compiler flags on check, and will probably fail hard on a machine with no openMP capabilities.

The solution seems to be to use configure and autoconf. There's some information around here:

http://cran.r-project.org/doc/manuals/R-exts.html#Using-Makevars

including a complex example to compile in odbc functionality. But I can't see how to begin tweaking that to check for openmp and libgomp. 

None of the R packages I've looked at that talk about using openMP seem to have this set up either.

So does anyone have a walkthrough for setting up an R package with OpenMP?
",4
9579177,03/06/2012 06:49:20,1172558,01/27/2012 00:52:40,41,1,Covariance models with lme and gls,"I'm trying to fit several covariance models using gls and lme. My aim is to identify which covariance model fits my data better. I'm afraid, however, that I'm not specifying the code properly. Could someone take a look on my code and help me figuring out whether I'm pursuing everything correctly?

    # Unstructured covariance matrix
    UN <- gls(y ~ ses + time, data, corr=corSymm(form=~1|id), weights=varIdent(form=~1|time), method=""REML"", control=lmeControl(msMaxIter = 500, msVerbose = TRUE), na.action=na.omit)

    # Independence covariance matrix
    IN <- gls(y ~ ses + time, data, corr=NULL, weights=NULL, method=""REML"", control=lmeControl(msMaxIter = 500, msVerbose = TRUE))

    # Fit Random Intercept Model (RI)
    RI <- lme(y ~ ses + time, data, na.action=na.omit, method=""REML"", random=~1|id, control=lmeControl(msMaxIter = 200, msVerbose = TRUE))

    # Fit Random Intercept and Slopes Model (RIAS)
    RIAS <- lme(y ~ ses + time, data, na.action=na.omit, method=""REML"", random=~time | id, control=lmeControl(msMaxIter = 200, msVerbose = TRUE))

    # Fit Compound Symmetry Error Covariance Matrix
    CS <- gls(y ~ ses + time, data, na.action=na.omit, method=""REML"", correlation=corCompSymm(,form=~1|id), control=lmeControl(msMaxIter = 500, msVerbose = TRUE))

    # Fit Heterogeneous Compound Symmetry Error Covariance Matrix
    CSH <- gls(y ~ ses + time, data, na.action=na.omit, method=""REML"", correlation=corCompSymm(,form=~1|id), weights=varIdent(form=~1|time), control=lmeControl(msMaxIter = 500, msVerbose = TRUE))

    # AR(1)
    AR1 <- gls(y ~ ses + time, data, na.action=na.omit, method=""REML"", correlation=corAR1(,form=~1|id), control=lmeControl(msMaxIter = 200, msVerbose = TRUE))

    # AR(1) under heterocedasticity
    ARH1 <- gls(y ~ ses + time, data, na.action=na.omit, method=""REML"", correlation=corAR1(,form=~1|id), weights=varIdent(form=~1|time), control=lmeControl(msMaxIter = 200, msVerbose = TRUE))

    # RI plus AR(1) 
    RIAR1 <- lme(y ~ ses + time, data, na.action=na.omit, method=""REML"", random=~1|id, correlation=corAR1(form=~1|id), control=lmeControl(msMaxIter = 200, msVerbose = TRUE))

    # RI plus AR(1) under heterocedasticity
    RIARH1 <- lme(y ~ ses + time, data, na.action=na.omit, method=""REML"", random=~1|id, correlation=corAR1(form=~1|id), weights=varIdent(form=~1|time), control=lmeControl(msMaxIter = 200, msVerbose = TRUE))

    # RIAS plus AR(1)  
    RIASAR1<- lme(y ~ ses + time, data, na.action=na.omit, method=""REML"", random=~time|id, correlation=corAR1(form=~1|id), control=lmeControl(msMaxIter = 200, msVerbose = TRUE)) 

    # ARMA(1,1)
    ARMA11 <- gls(y ~ ses + time, data, na.action=na.omit, method=""REML"", correlation=corARMA(,form=~time|id, p=1, q=1), control=lmeControl(msMaxIter = 200, msVerbose = TRUE))

    # ARMA(1,1) under heterocedasticity
    ARMA11HE <- gls(y ~ ses + time, data, na.action=na.omit, method=""REML"", correlation=corARMA(,form=~time|id, p=1, q=1), weights=varIdent(form=~1|time),  control = lmeControl(msMaxIter = 200, msVerbose = TRUE))

    # Fit Toeplitz Error Covariance Matrix
    TOEP <- gls(y ~ ses + time, data, na.action=na.omit, method=""REML"", correlation=corARMA(,form=~1|id, p=3, q=0))

    # RIAQS plus AR(1) allow for heterocedasticity
    RIAQSAR1 <- lme(y ~ ses + time, data, na.action=na.omit, method=""REML"", weights=varPower(form=~time), random=~time + I(time^2)|id, correlation=corAR1(form=~time), control=lmeControl(msMaxIter = 200, msVerbose = TRUE))

thanks",r,model,statistics,covariance,,03/06/2012 15:21:09,not a real question,1,492,6,"Covariance models with lme and gls I'm trying to fit several covariance models using gls and lme. My aim is to identify which covariance model fits my data better. I'm afraid, however, that I'm not specifying the code properly. Could someone take a look on my code and help me figuring out whether I'm pursuing everything correctly?

    # Unstructured covariance matrix
    UN <- gls(y ~ ses + time, data, corr=corSymm(form=~1|id), weights=varIdent(form=~1|time), method=""REML"", control=lmeControl(msMaxIter = 500, msVerbose = TRUE), na.action=na.omit)

    # Independence covariance matrix
    IN <- gls(y ~ ses + time, data, corr=NULL, weights=NULL, method=""REML"", control=lmeControl(msMaxIter = 500, msVerbose = TRUE))

    # Fit Random Intercept Model (RI)
    RI <- lme(y ~ ses + time, data, na.action=na.omit, method=""REML"", random=~1|id, control=lmeControl(msMaxIter = 200, msVerbose = TRUE))

    # Fit Random Intercept and Slopes Model (RIAS)
    RIAS <- lme(y ~ ses + time, data, na.action=na.omit, method=""REML"", random=~time | id, control=lmeControl(msMaxIter = 200, msVerbose = TRUE))

    # Fit Compound Symmetry Error Covariance Matrix
    CS <- gls(y ~ ses + time, data, na.action=na.omit, method=""REML"", correlation=corCompSymm(,form=~1|id), control=lmeControl(msMaxIter = 500, msVerbose = TRUE))

    # Fit Heterogeneous Compound Symmetry Error Covariance Matrix
    CSH <- gls(y ~ ses + time, data, na.action=na.omit, method=""REML"", correlation=corCompSymm(,form=~1|id), weights=varIdent(form=~1|time), control=lmeControl(msMaxIter = 500, msVerbose = TRUE))

    # AR(1)
    AR1 <- gls(y ~ ses + time, data, na.action=na.omit, method=""REML"", correlation=corAR1(,form=~1|id), control=lmeControl(msMaxIter = 200, msVerbose = TRUE))

    # AR(1) under heterocedasticity
    ARH1 <- gls(y ~ ses + time, data, na.action=na.omit, method=""REML"", correlation=corAR1(,form=~1|id), weights=varIdent(form=~1|time), control=lmeControl(msMaxIter = 200, msVerbose = TRUE))

    # RI plus AR(1) 
    RIAR1 <- lme(y ~ ses + time, data, na.action=na.omit, method=""REML"", random=~1|id, correlation=corAR1(form=~1|id), control=lmeControl(msMaxIter = 200, msVerbose = TRUE))

    # RI plus AR(1) under heterocedasticity
    RIARH1 <- lme(y ~ ses + time, data, na.action=na.omit, method=""REML"", random=~1|id, correlation=corAR1(form=~1|id), weights=varIdent(form=~1|time), control=lmeControl(msMaxIter = 200, msVerbose = TRUE))

    # RIAS plus AR(1)  
    RIASAR1<- lme(y ~ ses + time, data, na.action=na.omit, method=""REML"", random=~time|id, correlation=corAR1(form=~1|id), control=lmeControl(msMaxIter = 200, msVerbose = TRUE)) 

    # ARMA(1,1)
    ARMA11 <- gls(y ~ ses + time, data, na.action=na.omit, method=""REML"", correlation=corARMA(,form=~time|id, p=1, q=1), control=lmeControl(msMaxIter = 200, msVerbose = TRUE))

    # ARMA(1,1) under heterocedasticity
    ARMA11HE <- gls(y ~ ses + time, data, na.action=na.omit, method=""REML"", correlation=corARMA(,form=~time|id, p=1, q=1), weights=varIdent(form=~1|time),  control = lmeControl(msMaxIter = 200, msVerbose = TRUE))

    # Fit Toeplitz Error Covariance Matrix
    TOEP <- gls(y ~ ses + time, data, na.action=na.omit, method=""REML"", correlation=corARMA(,form=~1|id, p=3, q=0))

    # RIAQS plus AR(1) allow for heterocedasticity
    RIAQSAR1 <- lme(y ~ ses + time, data, na.action=na.omit, method=""REML"", weights=varPower(form=~time), random=~time + I(time^2)|id, correlation=corAR1(form=~time), control=lmeControl(msMaxIter = 200, msVerbose = TRUE))

thanks",4
10710373,05/22/2012 21:22:03,394963,07/18/2010 03:39:56,53,0,"what does it mean that ""cacheSweave doesn't cache side-effects""?","I'm using cachesweave, but I don't think I get how everything works. I've tried to separate the code  into  simulation chunks and the plotting chunks, but some of the code is very long and written before I started the sweave document, so I instead use something like 

     <<foo,cache=TRUE>>
     source(""mainScript.R"")
     @
     <<plot,fig=TRUE>>
     a<- print(str(F1))
     plot(F1)
     @
The thing is mainScript.R is somewhat convoluted simulation code including plot functions and so on. I've read in cacheSweave vignette ""cacheSweave doesn't cache side-effects"" and plots are not cached, so I was wondering if the plotting functions in mainScript.R effect how the expressions are evaluated?

This might be an obvious question. Let's say I have another chunk after the two above. all of the results of the expressions in both ""foo"" and ""plot"" can be used in this new chunk, right? For example,

     <<post-chunk>>
     print(a)
     print(str(F1))
     @

",r,caching,sweave,chunks,,,open,0,187,9,"what does it mean that ""cacheSweave doesn't cache side-effects""? I'm using cachesweave, but I don't think I get how everything works. I've tried to separate the code  into  simulation chunks and the plotting chunks, but some of the code is very long and written before I started the sweave document, so I instead use something like 

     <<foo,cache=TRUE>>
     source(""mainScript.R"")
     @
     <<plot,fig=TRUE>>
     a<- print(str(F1))
     plot(F1)
     @
The thing is mainScript.R is somewhat convoluted simulation code including plot functions and so on. I've read in cacheSweave vignette ""cacheSweave doesn't cache side-effects"" and plots are not cached, so I was wondering if the plotting functions in mainScript.R effect how the expressions are evaluated?

This might be an obvious question. Let's say I have another chunk after the two above. all of the results of the expressions in both ""foo"" and ""plot"" can be used in this new chunk, right? For example,

     <<post-chunk>>
     print(a)
     print(str(F1))
     @

",4
6105729,05/24/2011 04:35:36,707145,04/14/2011 02:08:22,108,0,Tinn-R Error: Cannot focus a disabled or invisible windows,"I'm trying to use Tinn-R on windows 7 and with R-2.13.0. I got frustrated with this error message from Tinn-R ""Cannot focus a disabled or invisible windows"". I'd highly appreciate if anyone help me to figure out this problem. Thanks",r,,,,,05/25/2011 05:39:46,off topic,1,40,9,"Tinn-R Error: Cannot focus a disabled or invisible windows I'm trying to use Tinn-R on windows 7 and with R-2.13.0. I got frustrated with this error message from Tinn-R ""Cannot focus a disabled or invisible windows"". I'd highly appreciate if anyone help me to figure out this problem. Thanks",1
9139917,02/04/2012 09:44:45,1160354,01/20/2012 09:56:52,8,0,the elements of a character vector are lists - how to combine them,"given that I have a vector:    x <- c(""m1"", ""m2"", ""m3"")   and every element m1, m2 and m3 is a list e.g.  

    m1 = list(a=1:3, b=2:4, c=1:10)  
    m2 = list(a=0:3, b=0:4, c=6:10)  
    m2 = list(a=1:30, b=1:2, c=6:10)  

I want to be able to create ""super list"" using loop: 
 
    mylist <- list()  
    for(i in x)mylist[[i]] <- ...??....  
when i=""m1"" then mylist[[""m1""]] <- m1  

any suggestion would be appreciate.    

Robert
    ",r,,,,,,open,0,108,13,"the elements of a character vector are lists - how to combine them given that I have a vector:    x <- c(""m1"", ""m2"", ""m3"")   and every element m1, m2 and m3 is a list e.g.  

    m1 = list(a=1:3, b=2:4, c=1:10)  
    m2 = list(a=0:3, b=0:4, c=6:10)  
    m2 = list(a=1:30, b=1:2, c=6:10)  

I want to be able to create ""super list"" using loop: 
 
    mylist <- list()  
    for(i in x)mylist[[i]] <- ...??....  
when i=""m1"" then mylist[[""m1""]] <- m1  

any suggestion would be appreciate.    

Robert
    ",1
8477933,12/12/2011 17:01:39,632998,02/24/2011 19:38:38,1216,61,Passing two parameter functions into apply,"Say I have a function called 

> myfun <- function(x,y) {median(x,y)} #obviously the actual function is something more complicated

Now lets say in a certain use, the y parameter is constant, (say `c(1,2,3,4,5)`). Is there any way I can pass this into apply without wrapping it in another function?
i.e.
<br />instead of 

>apply(mydf, 2, function(x) myfun(x, c(1,2,3,4,5)))

to pass something like 

> apply(mydf, 2, myfun(,(c(1,2,3,4,5))))

This is purely cosmetic and I know it won't make much difference to the running time. I just want to know if an option like this is possible because wrapping my function in a function each time seems inefficient

",r,,,,,,open,0,100,6,"Passing two parameter functions into apply Say I have a function called 

> myfun <- function(x,y) {median(x,y)} #obviously the actual function is something more complicated

Now lets say in a certain use, the y parameter is constant, (say `c(1,2,3,4,5)`). Is there any way I can pass this into apply without wrapping it in another function?
i.e.
<br />instead of 

>apply(mydf, 2, function(x) myfun(x, c(1,2,3,4,5)))

to pass something like 

> apply(mydf, 2, myfun(,(c(1,2,3,4,5))))

This is purely cosmetic and I know it won't make much difference to the running time. I just want to know if an option like this is possible because wrapping my function in a function each time seems inefficient

",1
11546718,07/18/2012 17:08:39,1222694,02/21/2012 06:18:27,6,0,Data import error with RODBC from Access,"I am trying to import the results of a query performed on an access database into R. I can establish a connection using the RODBC package and I am able to retrieve records, so a connection with the database is not an issue. The problem is that two columns are inaccurate when I read them into R. 

The query in access reads data from a table, and then produces two additonal columns based on calculations I perform. I import 18 columns, 16 of which are direcly from the table, with the other two being the calculated columns. When I perform this query in access, everything is fine. The results of the two calculated columns in access (the result of many IIF statements), which I will call V1 and V2, look like this in access after running the query:

    V1        V2
    A        -.1
    A          0
    A         .2
    B         .2
    A        -.3
    C          0
    A         .03

V1 takes three values (A, B, C), and V2 contains floating point numbers, ranging from about -.35 to 1.5. 

These values are exactly what I want, so I know the query works when I run it in access. 

I want to obtain these same values in R, so I run the same query that generated the above results in access like this:

    query = ""Select ...""
    channel = odbcConnectAccess2007(""DatabaseLocation.accdb"")
    df = sqlQuery(channel, query)

Again, a connection with the database is not an issue. After I attempt to run this query from R, the results of the two calculated columns (V1 and V2) do not match the results from when I run the query in access. The other columns, which are read straight from a table without any calculations, are as they should be. After running the query in R V1 and V2 look like this:

    V1          V2
    A           0
    A           0
    A          .2
    A          .2
    A           0
    A           0
    A          .03

For V1, every value is now the same (A). For V2, *some* of the values are read in correctly, but not all. Many values have been changed to 0, and now the lowest value in V2 is -.07, while when I run the same query in access the lowest value in V2 is -.35. The values that have not been changed to 0 are correct.  

My next idea is to make a table out of the query in access, and then try and read in that entire new table. However, I would strongly prefer to do get the data the way I originally planned, and I would like to figure out this issue so I can avoid it in the future. ",r,ms-access,rodbc,,,,open,0,631,7,"Data import error with RODBC from Access I am trying to import the results of a query performed on an access database into R. I can establish a connection using the RODBC package and I am able to retrieve records, so a connection with the database is not an issue. The problem is that two columns are inaccurate when I read them into R. 

The query in access reads data from a table, and then produces two additonal columns based on calculations I perform. I import 18 columns, 16 of which are direcly from the table, with the other two being the calculated columns. When I perform this query in access, everything is fine. The results of the two calculated columns in access (the result of many IIF statements), which I will call V1 and V2, look like this in access after running the query:

    V1        V2
    A        -.1
    A          0
    A         .2
    B         .2
    A        -.3
    C          0
    A         .03

V1 takes three values (A, B, C), and V2 contains floating point numbers, ranging from about -.35 to 1.5. 

These values are exactly what I want, so I know the query works when I run it in access. 

I want to obtain these same values in R, so I run the same query that generated the above results in access like this:

    query = ""Select ...""
    channel = odbcConnectAccess2007(""DatabaseLocation.accdb"")
    df = sqlQuery(channel, query)

Again, a connection with the database is not an issue. After I attempt to run this query from R, the results of the two calculated columns (V1 and V2) do not match the results from when I run the query in access. The other columns, which are read straight from a table without any calculations, are as they should be. After running the query in R V1 and V2 look like this:

    V1          V2
    A           0
    A           0
    A          .2
    A          .2
    A           0
    A           0
    A          .03

For V1, every value is now the same (A). For V2, *some* of the values are read in correctly, but not all. Many values have been changed to 0, and now the lowest value in V2 is -.07, while when I run the same query in access the lowest value in V2 is -.35. The values that have not been changed to 0 are correct.  

My next idea is to make a table out of the query in access, and then try and read in that entire new table. However, I would strongly prefer to do get the data the way I originally planned, and I would like to figure out this issue so I can avoid it in the future. ",3
9624948,03/08/2012 21:06:32,1096474,12/13/2011 19:23:45,8,0,Extended Survival Plot Lines in R,"I've obtained a survival plot from the following code:

        s = Surv(outcome.[,1], outcome.[,2])
        survplot= (survfit(s ~ person.list[,1]))
        plot(survplot, mark.time = FALSE)

person.list is just a list of 15 people.  

When I plot this, the lines on my plot all end at different time points.  Is there a way to extend all the lines to make them end at a certain time point? (i.e outcome.[,1] is a time to event variable and I would like the survival lines on the plot to extend out to say 5(years) ) 

Thanks,
Matt

",r,,,,,,open,0,110,6,"Extended Survival Plot Lines in R I've obtained a survival plot from the following code:

        s = Surv(outcome.[,1], outcome.[,2])
        survplot= (survfit(s ~ person.list[,1]))
        plot(survplot, mark.time = FALSE)

person.list is just a list of 15 people.  

When I plot this, the lines on my plot all end at different time points.  Is there a way to extend all the lines to make them end at a certain time point? (i.e outcome.[,1] is a time to event variable and I would like the survival lines on the plot to extend out to say 5(years) ) 

Thanks,
Matt

",1
4983331,02/13/2011 09:03:18,397521,07/21/2010 05:02:05,65,6,analysing wind data with R,"Hi i am analaysing wind data for estimating energy from a wind turbine.    
I have taken 10 years of wind data and graphed a histogram;   
my second stage was to fit a Weibull distribution to the data.   
I used R with the package `lmom` to compute the Weibul shape and scale 
this is the code i used:

    >library(lmom)    
    wind.moments<-samlmu(as.numeric(pp$WS))      
    moments<-pelwei(wind.moments)     
    x.wei<-rweibull(n=length(pp$WS), shape=moments[""delta""], scale=moments[""beta""])    
    hist(as.numeric(pp$WS), freq=FALSE)    
    lines(density(x.wei), col=""red"", lwd=4)    

It seems like there is some lag between the data and the density function; can you help me with this? 
Another question is can you help me in calculating the anual energy from the density function?

 ![enter image description here][1]   
thank you 
  


  [1]: http://i.stack.imgur.com/AqVij.png",r,statistics,distribution,histogram,,02/13/2011 21:50:48,off topic,1,169,5,"analysing wind data with R Hi i am analaysing wind data for estimating energy from a wind turbine.    
I have taken 10 years of wind data and graphed a histogram;   
my second stage was to fit a Weibull distribution to the data.   
I used R with the package `lmom` to compute the Weibul shape and scale 
this is the code i used:

    >library(lmom)    
    wind.moments<-samlmu(as.numeric(pp$WS))      
    moments<-pelwei(wind.moments)     
    x.wei<-rweibull(n=length(pp$WS), shape=moments[""delta""], scale=moments[""beta""])    
    hist(as.numeric(pp$WS), freq=FALSE)    
    lines(density(x.wei), col=""red"", lwd=4)    

It seems like there is some lag between the data and the density function; can you help me with this? 
Another question is can you help me in calculating the anual energy from the density function?

 ![enter image description here][1]   
thank you 
  


  [1]: http://i.stack.imgur.com/AqVij.png",4
11646149,07/25/2012 09:00:37,1531646,07/17/2012 11:40:51,6,0,Alternative to SQLDF package in R,"I need to write sql queries on R objects. Could anyone let me know if there is a package apart from SQLDF which does that. I am having an issue installing sqldf on linux platform.

Thanks..",r,,,,,07/26/2012 12:31:19,not a real question,1,35,6,"Alternative to SQLDF package in R I need to write sql queries on R objects. Could anyone let me know if there is a package apart from SQLDF which does that. I am having an issue installing sqldf on linux platform.

Thanks..",1
5729185,04/20/2011 10:59:05,334755,05/06/2010 18:53:39,1084,33,Record linking databases with function RLBigDataLinkage in package RecordLinkage in R,"I would like to link two databases that use different numeric keys, but _similar_ strings. Specifically, one database uses a numeric key `GVKEY` and a company name `CONML`

    > head(temp.compustat[order(temp.compustat$CONML, decreasing = T), ])
            GVKEY             CONML
    225994  13023  ZZZZ Best Co Inc
    211017  11696       Zytrex Corp
    213816  11951 Zytec Systems Inc
    309886  29163        Zytec Corp
    373950 129441         Zynex Inc
    383184 145228  ZymoGenetics Inc
    > dim(temp.compustat)
    [1] 31354     2

The other database uses a different numeric key `companyid` and a company name `company` that may be slightly different than `CONML` in the first database.

    > head(temp.dealscan[ order(temp.dealscan$company, decreasing = T), ])
          companyid
    70473     18192
    32025     16969
    19714     92271
    80473     13185
    1901      24303
    33993     21219
                                                                              company
    70473 Zytec Corp                                                                 
    32025 Zynaxis Inc                                                                
    19714 ZYGO Teraoptix Inc                                                         
    80473 Zygo Corp                                                                  
    1901  Zycon Corp SDN Bhd                                                         
    33993 Zycon Corp                                                                 
    > dim(temp.dealscan)
    [1] 85818     2

(I am sorting in reverse, because the DealScan databases has blanks and * at the beginning of some entries). It seems that the function `RLBigDataLinkage` in package `RecordLinkage` is the solution, but I can't get even unsupervised linking to work. Here's my error.

    > library(RecordLinkage)
    > rpairs <- RLBigDataLinkage(dataset1 = temp.compustat, dataset2 = temp.dealscan, exclude = 1, strcmp = 2, strcmpfun = ""levenshtein"")
    > result <- epiClassify(rpairs, threshold.upper = 0.5)
    Error in if (max <= min) stop(""must have max > min"") : 
      missing value where TRUE/FALSE needed
    In addition: Warning message:
    In nData1 * nData2 : NAs produced by integer overflow

Coincident with me getting started yesterday, I saw this [question][1] about using the Levenshtein distance function manually. Is this approach a better option for me? These databases are fairly large, so it seems that `RLBigData` should be the correct approach (the package authors recommend it for > 1000 entries). Thanks!


  [1]: http://stackoverflow.com/questions/5721883/agrep-only-return-best-matches",r,,,,,,open,0,931,11,"Record linking databases with function RLBigDataLinkage in package RecordLinkage in R I would like to link two databases that use different numeric keys, but _similar_ strings. Specifically, one database uses a numeric key `GVKEY` and a company name `CONML`

    > head(temp.compustat[order(temp.compustat$CONML, decreasing = T), ])
            GVKEY             CONML
    225994  13023  ZZZZ Best Co Inc
    211017  11696       Zytrex Corp
    213816  11951 Zytec Systems Inc
    309886  29163        Zytec Corp
    373950 129441         Zynex Inc
    383184 145228  ZymoGenetics Inc
    > dim(temp.compustat)
    [1] 31354     2

The other database uses a different numeric key `companyid` and a company name `company` that may be slightly different than `CONML` in the first database.

    > head(temp.dealscan[ order(temp.dealscan$company, decreasing = T), ])
          companyid
    70473     18192
    32025     16969
    19714     92271
    80473     13185
    1901      24303
    33993     21219
                                                                              company
    70473 Zytec Corp                                                                 
    32025 Zynaxis Inc                                                                
    19714 ZYGO Teraoptix Inc                                                         
    80473 Zygo Corp                                                                  
    1901  Zycon Corp SDN Bhd                                                         
    33993 Zycon Corp                                                                 
    > dim(temp.dealscan)
    [1] 85818     2

(I am sorting in reverse, because the DealScan databases has blanks and * at the beginning of some entries). It seems that the function `RLBigDataLinkage` in package `RecordLinkage` is the solution, but I can't get even unsupervised linking to work. Here's my error.

    > library(RecordLinkage)
    > rpairs <- RLBigDataLinkage(dataset1 = temp.compustat, dataset2 = temp.dealscan, exclude = 1, strcmp = 2, strcmpfun = ""levenshtein"")
    > result <- epiClassify(rpairs, threshold.upper = 0.5)
    Error in if (max <= min) stop(""must have max > min"") : 
      missing value where TRUE/FALSE needed
    In addition: Warning message:
    In nData1 * nData2 : NAs produced by integer overflow

Coincident with me getting started yesterday, I saw this [question][1] about using the Levenshtein distance function manually. Is this approach a better option for me? These databases are fairly large, so it seems that `RLBigData` should be the correct approach (the package authors recommend it for > 1000 entries). Thanks!


  [1]: http://stackoverflow.com/questions/5721883/agrep-only-return-best-matches",1
8022761,11/05/2011 19:40:36,856014,07/21/2011 13:14:14,32,0,Rscript on ubuntu,Where can I install Rscript from? I need to run an R script from a php file using exec. However I need to install Rscript first.,r,ubuntu,,,,,open,0,26,3,Rscript on ubuntu Where can I install Rscript from? I need to run an R script from a php file using exec. However I need to install Rscript first.,2
7129213,08/20/2011 02:22:09,43118,12/04/2008 06:57:07,1468,27,R .libPaths() difference between RStudio and command-line R,"When I run R from the command line:

    > library(ggplot2)
    ...
    > path.package('ggplot2')
    [1] ""/home/yang/R/x86_64-pc-linux-gnu-library/2.13/ggplot2""
    > .libPaths()
    [1] ""/home/yang/R/x86_64-pc-linux-gnu-library/2.13""
    [2] ""/usr/local/lib/R/site-library""                
    [3] ""/usr/lib/R/site-library""                      
    [4] ""/usr/lib/R/library""                           
    > Sys.getenv('R_LIBS_USER')
    [1] ""~/R/x86_64-pc-linux-gnu-library/2.13""

(Note: that environment variable actually doesn't exist when I check from my shell.)

But from RStudio Server running on the same box, and after logging in as the same user:

    > path.package('ggplot2')
    [1] ""/home/yang/R/library/ggplot2""
    > .libPaths()
    [1] ""/home/yang/R/library""              ""/usr/local/lib/R/site-library""    
    [3] ""/usr/lib/R/site-library""           ""/usr/lib/R/library""               
    [5] ""/usr/lib/rstudio-server/R/library""
    > Sys.getenv('R_LIBS_USER')
    [1] ""/home/yang/R/library""

Can you explain why these are different by default? Is this an RStudio customization? (Why?) Thanks in advance.",r,rstudio,,,,,open,0,256,8,"R .libPaths() difference between RStudio and command-line R When I run R from the command line:

    > library(ggplot2)
    ...
    > path.package('ggplot2')
    [1] ""/home/yang/R/x86_64-pc-linux-gnu-library/2.13/ggplot2""
    > .libPaths()
    [1] ""/home/yang/R/x86_64-pc-linux-gnu-library/2.13""
    [2] ""/usr/local/lib/R/site-library""                
    [3] ""/usr/lib/R/site-library""                      
    [4] ""/usr/lib/R/library""                           
    > Sys.getenv('R_LIBS_USER')
    [1] ""~/R/x86_64-pc-linux-gnu-library/2.13""

(Note: that environment variable actually doesn't exist when I check from my shell.)

But from RStudio Server running on the same box, and after logging in as the same user:

    > path.package('ggplot2')
    [1] ""/home/yang/R/library/ggplot2""
    > .libPaths()
    [1] ""/home/yang/R/library""              ""/usr/local/lib/R/site-library""    
    [3] ""/usr/lib/R/site-library""           ""/usr/lib/R/library""               
    [5] ""/usr/lib/rstudio-server/R/library""
    > Sys.getenv('R_LIBS_USER')
    [1] ""/home/yang/R/library""

Can you explain why these are different by default? Is this an RStudio customization? (Why?) Thanks in advance.",2
5588694,04/07/2011 23:11:10,564164,01/05/2011 15:37:40,2589,101,How to aggregate on IQR in SPSS?,"I have to aggregate (of course with a categorical break variable) a quite big data table containing some continuous variables by resulting the mean, median, standard deviation and interquartile range (IQR) of the required variables.

The first three is an easy one with the SPSS *Aggregate* command, but I have no idea how to compute IQR by aggregating the data table.

I know I could compute IQR by using *Descriptives* (by quartiles), but as I need the calculations in aggregation - this is not an option. Unfortunately using R fails also thanks to some odd circumstances (not able to load a huge comma separated file in R neither with base:: read.table, neither with [sqldf][1], neither with [bigmemory][2] and neither with [ff][3] packages).

Any idea is welcomed! And of course: thank you in advance.

---

P.S.: I thought about estimating IQR by multiplying the standard deviation by 1.5, but that method would not work as the distributions are skewed, so assuming normality does not stands.

P.S.: do you think using R within SPSS would not result in memory problems like while opening the dataset in pure R?


  [1]: http://code.google.com/p/sqldf/
  [2]: http://www.bigmemory.org/
  [3]: http://cran.r-project.org/web/packages/ff/index.html",r,aggregate,spss,iqr,,,open,0,189,7,"How to aggregate on IQR in SPSS? I have to aggregate (of course with a categorical break variable) a quite big data table containing some continuous variables by resulting the mean, median, standard deviation and interquartile range (IQR) of the required variables.

The first three is an easy one with the SPSS *Aggregate* command, but I have no idea how to compute IQR by aggregating the data table.

I know I could compute IQR by using *Descriptives* (by quartiles), but as I need the calculations in aggregation - this is not an option. Unfortunately using R fails also thanks to some odd circumstances (not able to load a huge comma separated file in R neither with base:: read.table, neither with [sqldf][1], neither with [bigmemory][2] and neither with [ff][3] packages).

Any idea is welcomed! And of course: thank you in advance.

---

P.S.: I thought about estimating IQR by multiplying the standard deviation by 1.5, but that method would not work as the distributions are skewed, so assuming normality does not stands.

P.S.: do you think using R within SPSS would not result in memory problems like while opening the dataset in pure R?


  [1]: http://code.google.com/p/sqldf/
  [2]: http://www.bigmemory.org/
  [3]: http://cran.r-project.org/web/packages/ff/index.html",4
10070443,04/09/2012 07:53:44,984735,10/07/2011 21:18:59,46,0,Row wise operations in octave,"

Is there a way to perform row wise operations on matrices in octave ?

I have a `(mXn)` matrix of integers, can i obtain a `m dimensional` vector where each element is the maximum element of the corresponding row ? How can i do this in octave ?

",r,matlab,octave,,,,open,0,47,5,"Row wise operations in octave 

Is there a way to perform row wise operations on matrices in octave ?

I have a `(mXn)` matrix of integers, can i obtain a `m dimensional` vector where each element is the maximum element of the corresponding row ? How can i do this in octave ?

",3
10053548,04/07/2012 10:10:54,426176,08/20/2010 09:28:03,446,23,how to plot estimates through model in R,"I'm trying to use R to do some modelling, I've started to use BodyWeight library, since I've seen some examples online. Just to understand and get used to the commands.

I've come to my final model, with estimates and I was wondering how to plot these estimates, but I haven't seen anything online..

![enter image description here][1]


  [1]: http://i.stack.imgur.com/kls86.png

Is there a way to plot the values of the estimates with a line, and dots for the values of each observation?

Where can I find information about how to do this, do I have to extract the values myself or it is possible to say plot the estimates of these model?

I'm only starting with R. Any help is welcome.

Thank you",r,plot,modeling,,,,open,0,117,8,"how to plot estimates through model in R I'm trying to use R to do some modelling, I've started to use BodyWeight library, since I've seen some examples online. Just to understand and get used to the commands.

I've come to my final model, with estimates and I was wondering how to plot these estimates, but I haven't seen anything online..

![enter image description here][1]


  [1]: http://i.stack.imgur.com/kls86.png

Is there a way to plot the values of the estimates with a line, and dots for the values of each observation?

Where can I find information about how to do this, do I have to extract the values myself or it is possible to say plot the estimates of these model?

I'm only starting with R. Any help is welcome.

Thank you",3
11298087,07/02/2012 17:05:19,1480106,06/25/2012 13:23:03,34,0,simplify gam with first order AR process,"In the following example I am trying to model the temperature variation at two locations in the states, to do this I use a gam: 

   

    set.seed(10)
    RandData <- rnorm(8760*2)
    America <- rep(c('NewYork','Miami'),each=8760)
    
    Date = seq(from=as.POSIXct(""1991-01-01 00:00""), 
               to=as.POSIXct(""1991-12-31 23:00""), length=8760)
    
    DatNew <- data.frame(Loc = America,
                         Doy = as.numeric(format(Date,format = ""%j"")),
                         Tod = as.numeric(format(Date,format = ""%H"")),
                         Temp = RandData,
                         DecTime = rep(seq(1, length(RandData)/2) / (length(RandData)/2),
                                       2))
    require(mgcv)
    mod1 <- gam(Temp ~ Loc + s(Doy) + s(Doy,by = Loc) +
      s(Tod) + s(Tod,by = Loc),data = DatNew, method = ""ML"")

From this, there still seems to be a pattern in the residuals, I assume this is caused by the correlation in the residuals (due to it being a times series). I have tried to account for this by fitting the gam and assuming AR(1) correlations in the model residuals:

    mod1 <- gam(Temp ~ Loc + s(Doy) + s(Doy,by = Loc) +
      s(Tod) + s(Tod,by = Loc),data = DatNew, 
                method = ""ML"", correlation = corAR1(form = ~ DecTime | Loc))

But this crashes my computer, so, I read that it is possible to simplify the model further by restricting the order of the autocorrelations to a small number of lags i.e. the first order AR process. In other words, build the covariance matrix by only using the first lag in the acf. 

How would I create this covariance matrix by considering that I have two locations so I would need to create 2 (1 for each location)?

thanks in advance 
",r,gam,,,,07/03/2012 13:59:53,off topic,1,455,7,"simplify gam with first order AR process In the following example I am trying to model the temperature variation at two locations in the states, to do this I use a gam: 

   

    set.seed(10)
    RandData <- rnorm(8760*2)
    America <- rep(c('NewYork','Miami'),each=8760)
    
    Date = seq(from=as.POSIXct(""1991-01-01 00:00""), 
               to=as.POSIXct(""1991-12-31 23:00""), length=8760)
    
    DatNew <- data.frame(Loc = America,
                         Doy = as.numeric(format(Date,format = ""%j"")),
                         Tod = as.numeric(format(Date,format = ""%H"")),
                         Temp = RandData,
                         DecTime = rep(seq(1, length(RandData)/2) / (length(RandData)/2),
                                       2))
    require(mgcv)
    mod1 <- gam(Temp ~ Loc + s(Doy) + s(Doy,by = Loc) +
      s(Tod) + s(Tod,by = Loc),data = DatNew, method = ""ML"")

From this, there still seems to be a pattern in the residuals, I assume this is caused by the correlation in the residuals (due to it being a times series). I have tried to account for this by fitting the gam and assuming AR(1) correlations in the model residuals:

    mod1 <- gam(Temp ~ Loc + s(Doy) + s(Doy,by = Loc) +
      s(Tod) + s(Tod,by = Loc),data = DatNew, 
                method = ""ML"", correlation = corAR1(form = ~ DecTime | Loc))

But this crashes my computer, so, I read that it is possible to simplify the model further by restricting the order of the autocorrelations to a small number of lags i.e. the first order AR process. In other words, build the covariance matrix by only using the first lag in the acf. 

How would I create this covariance matrix by considering that I have two locations so I would need to create 2 (1 for each location)?

thanks in advance 
",2
10193130,04/17/2012 14:25:12,889604,08/11/2011 09:34:29,854,30,Selecting specific values following a modulus operation,"I've got a vector `test <- c(1:90)` (approximately representing 3 months in daily time-steps), I want to give a value to a variable during a run of a model only when the time is between days 10 and 20:

    //model code

    if(month-day>10 && month-day<20)
    {
        parameter <- 10
    } else {
        parameter <- 5
    }

By finding the modulus (`test%%30`), I can get an vector containing the days for each month, but need to get the positions of the days 10-20 for each month out of this subsequent vector

    > test%%30
    [1] 1 2 3 4 [...] 27 28 29 0 1 2 3 ...
    
I've just hit a brickwall with how to get the values I want (i.e. in this simple example I want `test[10:20]`, `test[41:49]` and `test[71:79]`, but there must be a way to get these values using some clever mathematical operators that I can't think of at the moment...

 ",r,mathematical-expressions,,,,,open,0,190,7,"Selecting specific values following a modulus operation I've got a vector `test <- c(1:90)` (approximately representing 3 months in daily time-steps), I want to give a value to a variable during a run of a model only when the time is between days 10 and 20:

    //model code

    if(month-day>10 && month-day<20)
    {
        parameter <- 10
    } else {
        parameter <- 5
    }

By finding the modulus (`test%%30`), I can get an vector containing the days for each month, but need to get the positions of the days 10-20 for each month out of this subsequent vector

    > test%%30
    [1] 1 2 3 4 [...] 27 28 29 0 1 2 3 ...
    
I've just hit a brickwall with how to get the values I want (i.e. in this simple example I want `test[10:20]`, `test[41:49]` and `test[71:79]`, but there must be a way to get these values using some clever mathematical operators that I can't think of at the moment...

 ",2
11534403,07/18/2012 04:40:40,1199712,02/09/2012 13:06:07,100,1,r sequence recognition for univariate data sequence mining arules,"I would like to be able to have an method of finding sequences within data, without specifying the length of the sequence or the exact nature of the sequence.

for example if I had the vector

    x <- c(round(rnorm(100)*10),
           c(1:5),
           c(6,4,6),
           round(rnorm(300)*10), 
           c(1:5), 
           round(rnorm(70)*10),
           c(1:5), 
           round(rnorm(100)*10),
           c(6,4,6),
           round(rnorm(200)*10),
           c(1:5), 
           round(rnorm(70)*10),
           c(1:5),
           c(6,4,6),
           round(rnorm(70)*10),
           c(1:5), 
           round(rnorm(100)*10),
           c(6,4,6),
           round(rnorm(200)*10),
           c(1:5), 
           round(rnorm(70)*10),
           c(1:5),
           c(6,4,6))

I would hope that the method would be able to identify the fact that x has the sequence 1,2,3,4,5 is in there at least eight times and the sequence 6,4,6 appears at least five times. (the at least is due to the random normal potentially generating the same sequence)

I have found the `arules` and `arulesSequences` package but can't work out how to get it to work for univariate data. Maybe there might be other packages that might be better? I'm guessing statistics would be needed to identify them. 

I'm aware that only eight or five occurrences for each sequence is not going to be enough to generate statistically significant information, but my question was to ask if there was a good method of doing this, assuming the data repeated several times. 

Also note the important part is that the method is done without knowing beforehand that the structure in the data had the sequences `1,2,3,4,5` and `6,4,6` built into it. The aim was to find those sequences from `x` and identify where it occurs in the data.

Any help would be greatly appreciated! ",r,data-mining,sequences,recognition,,07/18/2012 11:14:55,off topic,1,471,9,"r sequence recognition for univariate data sequence mining arules I would like to be able to have an method of finding sequences within data, without specifying the length of the sequence or the exact nature of the sequence.

for example if I had the vector

    x <- c(round(rnorm(100)*10),
           c(1:5),
           c(6,4,6),
           round(rnorm(300)*10), 
           c(1:5), 
           round(rnorm(70)*10),
           c(1:5), 
           round(rnorm(100)*10),
           c(6,4,6),
           round(rnorm(200)*10),
           c(1:5), 
           round(rnorm(70)*10),
           c(1:5),
           c(6,4,6),
           round(rnorm(70)*10),
           c(1:5), 
           round(rnorm(100)*10),
           c(6,4,6),
           round(rnorm(200)*10),
           c(1:5), 
           round(rnorm(70)*10),
           c(1:5),
           c(6,4,6))

I would hope that the method would be able to identify the fact that x has the sequence 1,2,3,4,5 is in there at least eight times and the sequence 6,4,6 appears at least five times. (the at least is due to the random normal potentially generating the same sequence)

I have found the `arules` and `arulesSequences` package but can't work out how to get it to work for univariate data. Maybe there might be other packages that might be better? I'm guessing statistics would be needed to identify them. 

I'm aware that only eight or five occurrences for each sequence is not going to be enough to generate statistically significant information, but my question was to ask if there was a good method of doing this, assuming the data repeated several times. 

Also note the important part is that the method is done without knowing beforehand that the structure in the data had the sequences `1,2,3,4,5` and `6,4,6` built into it. The aim was to find those sequences from `x` and identify where it occurs in the data.

Any help would be greatly appreciated! ",4
7132110,08/20/2011 13:41:18,902432,08/19/2011 12:33:48,6,0,Which is better R with SimpleDB or Mahout with SimpleDB?,"**Background:**
I am currently working on system that generated product recommendations like those on Amazon : ""People who bought this also bought this..""
 
**Question:**
Which is better combination of data processor and data storage to generate recommendations and respond to API request.

 - R for machine learning (generating recommendations) with SimpleDB for storage.
                                
----------
 OR

 - Mahout for machine learning (generating recommendations) with SimpleDB for storage.

Please help!
",r,machine-learning,amazon-simpledb,mahout,recommendations,08/20/2011 14:46:07,not constructive,1,96,10,"Which is better R with SimpleDB or Mahout with SimpleDB? **Background:**
I am currently working on system that generated product recommendations like those on Amazon : ""People who bought this also bought this..""
 
**Question:**
Which is better combination of data processor and data storage to generate recommendations and respond to API request.

 - R for machine learning (generating recommendations) with SimpleDB for storage.
                                
----------
 OR

 - Mahout for machine learning (generating recommendations) with SimpleDB for storage.

Please help!
",5
11312219,07/03/2012 13:39:32,1318686,04/07/2012 06:34:31,72,0,"fixed effect, instrumental variable regression like xtivreg in stata (FE IV regression)","Does anyone know about a R package that supports fixed effect, instrumental variable regression like `xtivreg` in stata (FE IV regression). Yes, I can just include dummy variables but that just gets impossible when the number of groups increases. 

Thanks!


",r,,,,,,open,0,40,12,"fixed effect, instrumental variable regression like xtivreg in stata (FE IV regression) Does anyone know about a R package that supports fixed effect, instrumental variable regression like `xtivreg` in stata (FE IV regression). Yes, I can just include dummy variables but that just gets impossible when the number of groups increases. 

Thanks!


",1
11367485,07/06/2012 18:07:45,934898,09/08/2011 13:29:40,47,1,Rename columns of a data frame by searching column name,"I am writing a wrapper to ggplot to produce multiple graphs based on various datasets. As I am passing the column names to the function, I need to rename the column names so that ggplot can understand the reference. 

However, I am struggling with renaming of the columns of a data frame 

here's a data frame:

    df <- data.frame(col1=1:3,col2=3:5,col3=6:8)

here are my column names for search:

    col1_search <- ""col1""
    col2_search <- ""col2""
    col3_search <- ""col3""

and here are column names to replace:

    col1_replace <- ""new_col1""
    col2_replace <- ""new_col2""
    col3_replace <- ""new_col3""

when I search for column names, R sorts the column indexes and disregards the search location.

for example, when I run the following code, I expected the new headers to be new_col1, new_col2, and new_col3, instead the new column names are: new_col3, new_col2, and new_col1

    colnames(df)[names(df) %in% c(col3_search,col2_search,col1_search)] <- c(col3_replace,col2_replace,col1_replace)

Does anyone have a solution where I can search for column names and replace them in that order?",r,,,,,,open,0,178,10,"Rename columns of a data frame by searching column name I am writing a wrapper to ggplot to produce multiple graphs based on various datasets. As I am passing the column names to the function, I need to rename the column names so that ggplot can understand the reference. 

However, I am struggling with renaming of the columns of a data frame 

here's a data frame:

    df <- data.frame(col1=1:3,col2=3:5,col3=6:8)

here are my column names for search:

    col1_search <- ""col1""
    col2_search <- ""col2""
    col3_search <- ""col3""

and here are column names to replace:

    col1_replace <- ""new_col1""
    col2_replace <- ""new_col2""
    col3_replace <- ""new_col3""

when I search for column names, R sorts the column indexes and disregards the search location.

for example, when I run the following code, I expected the new headers to be new_col1, new_col2, and new_col3, instead the new column names are: new_col3, new_col2, and new_col1

    colnames(df)[names(df) %in% c(col3_search,col2_search,col1_search)] <- c(col3_replace,col2_replace,col1_replace)

Does anyone have a solution where I can search for column names and replace them in that order?",1
2390119,03/05/2010 21:40:52,287474,03/05/2010 21:40:52,1,0,is the garbage collection in R effecient?,I use 32bit R on Windows machines and ran into out of memory problem from time to time. Can anyone comment on the garbage collection in R? I am not sure if the memory management issue would be a legit reason for me to switch to F# or python/numpy or something.   ,r,,,,,10/09/2011 04:50:32,not constructive,1,54,7,is the garbage collection in R effecient? I use 32bit R on Windows machines and ran into out of memory problem from time to time. Can anyone comment on the garbage collection in R? I am not sure if the memory management issue would be a legit reason for me to switch to F# or python/numpy or something.   ,1
5198964,03/04/2011 20:34:28,311525,04/08/2010 01:45:59,305,33,Which modern IDE do you use with R,"I realize that there was a similar question asked [here][1], but this is over a year and a half old and the landscape has changed quite a bit.  I'm not sure the accepted answer would be the same if asked today, so I'd like to get people's feedback.

There seems to be a lot of buzz going on for http://www.rstudio.org.  Have people changed over from previous IDE's to this one recently?  What were the impacts of this change (good and bad).  Has anyone done an analysis of the landscape recently.  What are your thoughts on the prior list?

This question is probably best as a wiki that people can edit as new IDEs come out for R.


  [1]: http://stackoverflow.com/questions/1097367/which-ide-for-r-in-linux",r,ide,,,,03/04/2011 21:40:19,not constructive,1,123,8,"Which modern IDE do you use with R I realize that there was a similar question asked [here][1], but this is over a year and a half old and the landscape has changed quite a bit.  I'm not sure the accepted answer would be the same if asked today, so I'd like to get people's feedback.

There seems to be a lot of buzz going on for http://www.rstudio.org.  Have people changed over from previous IDE's to this one recently?  What were the impacts of this change (good and bad).  Has anyone done an analysis of the landscape recently.  What are your thoughts on the prior list?

This question is probably best as a wiki that people can edit as new IDEs come out for R.


  [1]: http://stackoverflow.com/questions/1097367/which-ide-for-r-in-linux",2
10486670,05/07/2012 17:49:52,680111,03/28/2011 11:21:51,62,0,Is there a package for non-local means filtering in R?,"As the title says, is there a premade function for non-local filtering?",r,signal-processing,,,,05/07/2012 17:58:52,not constructive,1,12,10,"Is there a package for non-local means filtering in R? As the title says, is there a premade function for non-local filtering?",2
8872525,01/15/2012 19:13:22,688080,04/01/2011 18:20:07,157,3,R: Plot histogram and density function curve on one chart,"I have a density function f, and I do MCMC sampling for it. To evaluate the goodness of the sampling, I need to plot the `hist` and `curve` within the same chart. The problem of

    hist(samples);
    curve(dfun,add=TRUE);

is that they are on the different scale: the frequency of a certain bin is usually hundreds, while the maximum of a density function is about 1 or so. What I want to do is to configure two plots at the same height, with one y-axis on the left and the other on the right. Can anyone help? Thank you.",r,plot,histogram,density,,,open,0,102,10,"R: Plot histogram and density function curve on one chart I have a density function f, and I do MCMC sampling for it. To evaluate the goodness of the sampling, I need to plot the `hist` and `curve` within the same chart. The problem of

    hist(samples);
    curve(dfun,add=TRUE);

is that they are on the different scale: the frequency of a certain bin is usually hundreds, while the maximum of a density function is about 1 or so. What I want to do is to configure two plots at the same height, with one y-axis on the left and the other on the right. Can anyone help? Thank you.",4
4290672,11/27/2010 08:03:34,302274,08/21/2009 14:50:04,86,0,How to add a dynamic value into RMySQL getQuery,"I am wondering if it is possible to pass a value into the query in dbGetQuery from the RMySQL package. 

For example, if I have a set of values in a data.frame:

    df <- c('a','b','c')

And I want to loop through the values to pull out a specific value from a database for each.

    library(RMySQL)    
    res <- dbGetQuery(con, ""SELECT max(ID) FROM table WHERE columna='df[2]')

When I try to add the reference to the value I get an error. Wondering if it is possible to add a value from an R object in the query.

I appreciate any advice. 

Thanks,

Jason",r,,,,,,open,0,109,9,"How to add a dynamic value into RMySQL getQuery I am wondering if it is possible to pass a value into the query in dbGetQuery from the RMySQL package. 

For example, if I have a set of values in a data.frame:

    df <- c('a','b','c')

And I want to loop through the values to pull out a specific value from a database for each.

    library(RMySQL)    
    res <- dbGetQuery(con, ""SELECT max(ID) FROM table WHERE columna='df[2]')

When I try to add the reference to the value I get an error. Wondering if it is possible to add a value from an R object in the query.

I appreciate any advice. 

Thanks,

Jason",1
8790143,01/09/2012 14:44:25,168775,09/04/2009 20:42:51,5290,292,Does the ternary operator exist in R?,"As the question asks, is there a control sequence in R similar to C's [ternary operator](http://stackoverflow.com/a/392946/168775)? If so, how do you use it? Thanks!
",r,operators,,,,,open,0,24,7,"Does the ternary operator exist in R? As the question asks, is there a control sequence in R similar to C's [ternary operator](http://stackoverflow.com/a/392946/168775)? If so, how do you use it? Thanks!
",2
6712638,07/15/2011 20:12:16,707145,04/14/2011 02:08:22,192,1,R package equivalent to MPlus,I wonder if there is any R package equivalent to MPlus. Thanks in advance.,r,,,,,07/17/2011 12:00:16,off topic,1,14,5,R package equivalent to MPlus I wonder if there is any R package equivalent to MPlus. Thanks in advance.,1
3252057,07/15/2010 02:21:40,144278,07/24/2009 06:53:53,567,10,"data.table and ""by must evaluate to list"" Error","I would like to use the data.table package in R to dynamically generate aggregations, but I am running into an error. Below, let `my.dt` be of type `data.table`.

    grouping.vars <- c(""sex"", ""age"")
    for (i in 1:5) {
         my.dt[,sum(dependent.variable), by=grouping.vars[i]]
    }

If I run this, I get errors:

    Error in `[.data.table`(my.dt, , sum(dependent.variable), by = grouping.vars[i] :
      by must evaluate to list

Yet the following works without error:

    my.dt[,sum(dependent.variable), by=sex]

I see why the error is occurring, but I do not see how to use a vector with the `by` parameter.",r,rstats,,,,,open,0,115,8,"data.table and ""by must evaluate to list"" Error I would like to use the data.table package in R to dynamically generate aggregations, but I am running into an error. Below, let `my.dt` be of type `data.table`.

    grouping.vars <- c(""sex"", ""age"")
    for (i in 1:5) {
         my.dt[,sum(dependent.variable), by=grouping.vars[i]]
    }

If I run this, I get errors:

    Error in `[.data.table`(my.dt, , sum(dependent.variable), by = grouping.vars[i] :
      by must evaluate to list

Yet the following works without error:

    my.dt[,sum(dependent.variable), by=sex]

I see why the error is occurring, but I do not see how to use a vector with the `by` parameter.",2
11651671,07/25/2012 14:09:06,1046337,11/14/2011 20:36:12,40,0,"Multilevel regression, multiple response (multiple outcomes) using lmer function in R?","I am trying to run a multilevel regression with multiple responses (multiple outcome measures) using lmer in R. Or at least that is what I think I want to be doing. My key resource is this article by Gelman ([1]), section 5 ""Multiple Outcomes and Other Challenges"".

I have data on workers from different countries (*Country*) and two different outcome measures: the effort they exerted (*Hours_Work*) and the amount they communicated with others (*Nr_Messages*). Right now, I have two linear models:

fit1 <- lm( Hours_Worked ~ Country -1, m)
fit2 <- lm( Nr_Messages ~ Country -1, m)

From what I read in the literature (and Gelman's work in [1] in particular) I think I am running into some kind of ""multiple comparisons"" issue, which could/should be eliminated by moving to a multilevel/hierarchical modeling approach. My ultimate goal is to plot some kind of two-dimensional coefficient plot of the different countries using the coefficient for *Hours_Worked* on the x-axis and the coefficient for *Nr_Messages* on the y-axis but somehow take the joint distribution of the two models into account. Does this make sense?

I realize that this help request is rather unspecific but I'm a little bit lost here. I'll quickly respond to any questions asking for clarification.

Chris

  [1]: http://stat.columbia.edu/~gelman/research/published/multiple2f.pdf",r,multiple,lme4,,,07/25/2012 15:47:03,off topic,1,205,11,"Multilevel regression, multiple response (multiple outcomes) using lmer function in R? I am trying to run a multilevel regression with multiple responses (multiple outcome measures) using lmer in R. Or at least that is what I think I want to be doing. My key resource is this article by Gelman ([1]), section 5 ""Multiple Outcomes and Other Challenges"".

I have data on workers from different countries (*Country*) and two different outcome measures: the effort they exerted (*Hours_Work*) and the amount they communicated with others (*Nr_Messages*). Right now, I have two linear models:

fit1 <- lm( Hours_Worked ~ Country -1, m)
fit2 <- lm( Nr_Messages ~ Country -1, m)

From what I read in the literature (and Gelman's work in [1] in particular) I think I am running into some kind of ""multiple comparisons"" issue, which could/should be eliminated by moving to a multilevel/hierarchical modeling approach. My ultimate goal is to plot some kind of two-dimensional coefficient plot of the different countries using the coefficient for *Hours_Worked* on the x-axis and the coefficient for *Nr_Messages* on the y-axis but somehow take the joint distribution of the two models into account. Does this make sense?

I realize that this help request is rather unspecific but I'm a little bit lost here. I'll quickly respond to any questions asking for clarification.

Chris

  [1]: http://stat.columbia.edu/~gelman/research/published/multiple2f.pdf",3
9698807,03/14/2012 09:11:00,249691,01/13/2010 10:01:22,1625,39,Strange behavior of facet_grid(),"I just met a strange beahvior of `facet_grid()` in ggplot2 0.9, and I wonder if someone could explain it to me...

Take the following `df` data frame :

    var <- sample(c(""red"", ""blue""), 100, replace=TRUE)
    group <- sample(c(""group1"", ""group2""), 100, replace=TRUE)
    df <- data.frame(var=factor(var), group=factor(group))

Which looks like :

      var  group
    1 red group2
    2 red group1
    3 red group1
    4 red group2
    5 red group2
    6 red group1

If I draw a bar chart of `var` faceted by `group`, I get a rather strange set of y-values :

    ggplot(data=df, aes(x=var)) + geom_bar() + facet_grid(~group)

![enter image description here][1]

It seems strange because the y-values seem correct if I use `facet_warp` instead of `facet_grid` :

    ggplot(data=df, aes(x=var)) + geom_bar() + facet_wrap(~group)

![enter image description here][2]

Moreover, I can get back correct values with `facet_grid` if I introduce another dummy variable in the data frame :

    df$tmp <- 1:nrow(df)
    ggplot(data=df, aes(x=var)) + geom_bar() + facet_grid(~group)

![enter image description here][3]

So, is it some sort of bug, or a normal behavior that I didn't understand ?

Thanks for any hint !

  [1]: http://i.stack.imgur.com/kI7pJ.png
  [2]: http://i.stack.imgur.com/QLl76.png
  [3]: http://i.stack.imgur.com/DcJYI.png",r,ggplot2,,,,,open,0,222,4,"Strange behavior of facet_grid() I just met a strange beahvior of `facet_grid()` in ggplot2 0.9, and I wonder if someone could explain it to me...

Take the following `df` data frame :

    var <- sample(c(""red"", ""blue""), 100, replace=TRUE)
    group <- sample(c(""group1"", ""group2""), 100, replace=TRUE)
    df <- data.frame(var=factor(var), group=factor(group))

Which looks like :

      var  group
    1 red group2
    2 red group1
    3 red group1
    4 red group2
    5 red group2
    6 red group1

If I draw a bar chart of `var` faceted by `group`, I get a rather strange set of y-values :

    ggplot(data=df, aes(x=var)) + geom_bar() + facet_grid(~group)

![enter image description here][1]

It seems strange because the y-values seem correct if I use `facet_warp` instead of `facet_grid` :

    ggplot(data=df, aes(x=var)) + geom_bar() + facet_wrap(~group)

![enter image description here][2]

Moreover, I can get back correct values with `facet_grid` if I introduce another dummy variable in the data frame :

    df$tmp <- 1:nrow(df)
    ggplot(data=df, aes(x=var)) + geom_bar() + facet_grid(~group)

![enter image description here][3]

So, is it some sort of bug, or a normal behavior that I didn't understand ?

Thanks for any hint !

  [1]: http://i.stack.imgur.com/kI7pJ.png
  [2]: http://i.stack.imgur.com/QLl76.png
  [3]: http://i.stack.imgur.com/DcJYI.png",2
11267286,06/29/2012 18:42:01,1489975,06/28/2012 23:44:41,11,0,Replacing missing values in R,"I have tried some code from here.

But i got some error message.


> ind <- which( is.na( pm25$PM25_9to9_ma5 ) )

> pm25[ind, ""PM25_9to9_ma5""]<- pm25[ind, "" PM25_pred_9to9_ma5""]

**Error in `[<-.data.frame`(`*tmp*`, ind, ""PM25_9to9_ma5"", value = NULL) : 
  replacement has length zero**

my data called pm25 and there are two columns called PM25_9to9_ma5 and PM25_pred_9to9_ma5.

Thanks",r,missing,replacing,,,07/01/2012 15:33:42,too localized,1,50,5,"Replacing missing values in R I have tried some code from here.

But i got some error message.


> ind <- which( is.na( pm25$PM25_9to9_ma5 ) )

> pm25[ind, ""PM25_9to9_ma5""]<- pm25[ind, "" PM25_pred_9to9_ma5""]

**Error in `[<-.data.frame`(`*tmp*`, ind, ""PM25_9to9_ma5"", value = NULL) : 
  replacement has length zero**

my data called pm25 and there are two columns called PM25_9to9_ma5 and PM25_pred_9to9_ma5.

Thanks",3
11550277,07/18/2012 21:00:05,1536106,07/18/2012 20:47:39,1,0,Which program produces this strange plot: multiple y values for single x,"I got the plot from a dissertation.  It is a time series of wind speed.  Time is in x-axis, wind speed is in y-axis. The strange thing is for each value of x (time), there are multiple values of y (wind speed).  I have no idea how the plot was produced.  If someone know, please enlighten me.
Thanks a lot.

[Strange Plot][1]


  [1]: http://i.imgur.com/vBoXE.jpg",r,matlab,,,,07/19/2012 11:39:50,too localized,1,67,12,"Which program produces this strange plot: multiple y values for single x I got the plot from a dissertation.  It is a time series of wind speed.  Time is in x-axis, wind speed is in y-axis. The strange thing is for each value of x (time), there are multiple values of y (wind speed).  I have no idea how the plot was produced.  If someone know, please enlighten me.
Thanks a lot.

[Strange Plot][1]


  [1]: http://i.imgur.com/vBoXE.jpg",2
7960682,10/31/2011 22:43:50,613299,02/11/2011 15:52:54,104,2,get rows which not contains 0 in R,"I would like to make new matrix from matrix but only with rows which do not contains 0, how can I do that?
I am working in R
Thanks
",r,matrix,columns,,,,open,0,27,8,"get rows which not contains 0 in R I would like to make new matrix from matrix but only with rows which do not contains 0, how can I do that?
I am working in R
Thanks
",3
8039380,11/07/2011 01:56:41,927589,09/04/2011 13:38:07,421,1,Converting mixed model formula from SAS to R,"I want to fit a mixed model using nlme package in R which is equivalent to following SAS codes:

    proc mixed data = one;
    class var1 var2  year loc rep;
    model yld = var1 * var2;
    random loc year(loc) rep*year(loc);

EDITS: Explanation of what is experiment about 

the same combination of var1 and var2 were tested in replicates (rep- replicates are numbered 1:3). The replicates (rep) is considered random. This set of experiment is repeated over locations (loc) and years (year). Although replicates are numbered 1:3 within each location and year for covinience because they do not have any name, replication 1 within a location and a year doesnot have correlation replication 1 within other location and other year



I tried the following codes:
    

     require(nlme) 
        fm1 <- lme(yld ~ var1*var2, data = one, random = loc + year / loc + rep * year / loc)  

Is my codes correct? 

EDITS: data and model based on suggestions 
you can download the example data file from the following link: 
https://sites.google.com/site/johndatastuff/mydata1.csv

    data$var1 <- as.factor(data$var1)
    data$var2 <- as.factor(data$var2)
    data$year <- as.factor(data$year)
    data$loc <- as.factor(data$loc)
    data$rep <- as.factor(data$rep)
    
    following suggestions from the comments below:
    fm1 <- lme(yld ~ var1*var2, data = data, random = ~ loc + year / loc + rep * year / loc)
    
    Error in getGroups.data.frame(dataMix, groups) : 
      Invalid formula for groups

EXPECTED BASED ON SAS OUTPUT

    Type 3 tests of fixed effects 
    var1*var2         14         238       F value 16.12 Pr >F = < 0.0001
    
    Covariance parameters:
    loc = 0, year(loc) = 922161, year*rep(loc) = 2077492, residual = 1109238 

I tried the following model, I still getting some errors: 

    Edits: Just for information I tried the following model
    require(lme4)  
     fm1 <- lmer(yld ~ var1*var2 + (1|loc) +  (1|year / loc) + (1|rep : (year / loc)),  
                data = data)  
    Error in rep:`:` : NA/NaN argument 
    In addition: Warning message: 
    In rep:`:` : numerical expression has 270 elements: only the first used






",r,sas,,,,,open,0,467,8,"Converting mixed model formula from SAS to R I want to fit a mixed model using nlme package in R which is equivalent to following SAS codes:

    proc mixed data = one;
    class var1 var2  year loc rep;
    model yld = var1 * var2;
    random loc year(loc) rep*year(loc);

EDITS: Explanation of what is experiment about 

the same combination of var1 and var2 were tested in replicates (rep- replicates are numbered 1:3). The replicates (rep) is considered random. This set of experiment is repeated over locations (loc) and years (year). Although replicates are numbered 1:3 within each location and year for covinience because they do not have any name, replication 1 within a location and a year doesnot have correlation replication 1 within other location and other year



I tried the following codes:
    

     require(nlme) 
        fm1 <- lme(yld ~ var1*var2, data = one, random = loc + year / loc + rep * year / loc)  

Is my codes correct? 

EDITS: data and model based on suggestions 
you can download the example data file from the following link: 
https://sites.google.com/site/johndatastuff/mydata1.csv

    data$var1 <- as.factor(data$var1)
    data$var2 <- as.factor(data$var2)
    data$year <- as.factor(data$year)
    data$loc <- as.factor(data$loc)
    data$rep <- as.factor(data$rep)
    
    following suggestions from the comments below:
    fm1 <- lme(yld ~ var1*var2, data = data, random = ~ loc + year / loc + rep * year / loc)
    
    Error in getGroups.data.frame(dataMix, groups) : 
      Invalid formula for groups

EXPECTED BASED ON SAS OUTPUT

    Type 3 tests of fixed effects 
    var1*var2         14         238       F value 16.12 Pr >F = < 0.0001
    
    Covariance parameters:
    loc = 0, year(loc) = 922161, year*rep(loc) = 2077492, residual = 1109238 

I tried the following model, I still getting some errors: 

    Edits: Just for information I tried the following model
    require(lme4)  
     fm1 <- lmer(yld ~ var1*var2 + (1|loc) +  (1|year / loc) + (1|rep : (year / loc)),  
                data = data)  
    Error in rep:`:` : NA/NaN argument 
    In addition: Warning message: 
    In rep:`:` : numerical expression has 270 elements: only the first used






",2
9798441,03/21/2012 04:02:43,714319,04/18/2011 23:49:11,217,2,panel data in R,I have been working with `zoo` to utilize lagging and differencing for my time series data.  I am not working with a panel data set that consists of firm and date.  It becomes very cumbersome to lag each firm individually and then merge the results.  Are there any good packages that work with panel data in R? I am aware of `plm` currently.  Others? `plm` has the  weird issue that the `lag` order (ie -1 vs +1) is exactly opposite of `zoo` and `ts` and thus I foresee headaches ahead.  Any packages that anyone like?,r,panel,time-series,regression,,,open,0,101,4,panel data in R I have been working with `zoo` to utilize lagging and differencing for my time series data.  I am not working with a panel data set that consists of firm and date.  It becomes very cumbersome to lag each firm individually and then merge the results.  Are there any good packages that work with panel data in R? I am aware of `plm` currently.  Others? `plm` has the  weird issue that the `lag` order (ie -1 vs +1) is exactly opposite of `zoo` and `ts` and thus I foresee headaches ahead.  Any packages that anyone like?,4
11680983,07/27/2012 03:16:37,905197,08/22/2011 03:19:15,19,0,Inserting an EMF into LaTeX,"After years of playing around with different figure formats in R I have found the one to hold it's quality best is an EMF. I have just started using LaTeX and cannot get LaTeX to read my EMFs. I am using the graphicx package (which I have read supports EMF files) but its still not reading it. 

    \usepackage{graphicx}
    \begin{figure}[h] 
    \centering
    \includegraphics*[width=1\textwidth]{combined_DMC}
    \caption{Frequency of fires for each month of the year}
    \label{fig_fire_month}
    \end{figure}

I am not sure if I am telling latex to use the graphicx package (as opposed to the standard graphics package) by using includegraphics* 

Thanks",r,latex,.emf,,,07/27/2012 07:21:05,off topic,1,119,5,"Inserting an EMF into LaTeX After years of playing around with different figure formats in R I have found the one to hold it's quality best is an EMF. I have just started using LaTeX and cannot get LaTeX to read my EMFs. I am using the graphicx package (which I have read supports EMF files) but its still not reading it. 

    \usepackage{graphicx}
    \begin{figure}[h] 
    \centering
    \includegraphics*[width=1\textwidth]{combined_DMC}
    \caption{Frequency of fires for each month of the year}
    \label{fig_fire_month}
    \end{figure}

I am not sure if I am telling latex to use the graphicx package (as opposed to the standard graphics package) by using includegraphics* 

Thanks",3
10811669,05/30/2012 07:18:22,1425304,05/30/2012 06:23:04,1,0,R rgl distance between axis ticks and tick labels,"Thanks firstly, to anyone who is taking time out of their day to help me solve my  problem - I am a newbie and this is my first post, so please be gentle!

I am plotting some point data using plot3d().
I would like to bring my y axis tick labels a little closer to my y axis tick marks.

The best way I can think of doing this is to

1) plot the data first, without drawing the axes

2) call on axis3d() to draw the y axis and tick marks but suppress labels from being drawn.

3) query the current position of each tick mark in 3D space. Store positions in a vector.

4) use mtext3d() to add labels at positions based on an adjustment to the vector


I am having a problem at step 3. I don't know how to query the position of each tick mark.
par3d() allows you to query a number of graphical parameters, is there something similar I can use to get the position each y axis tick?

Am I approaching this wrong? Probably.

Here is an example piece of code, without text added for y axis labels....

    require(rgl)
    x <- rnorm(5)
    y <- rnorm(5)
    z <- rnorm(5)
    open3d()
    plot3d(x,y,z,axes=F,xlab="""",ylab="""",zlab="""")
    par3d(ignoreExtent=TRUE)
    par3d(FOV=0)
    par3d(userMatrix=rotationMatrix(0,1,0,0))
    axis3d('y',nticks=5,labels = FALSE)
    par3d(zoom=1)
    par3d(windowRect=c(580,60,1380,900))

thanks a bunch

",r,rgl,,,,,open,0,243,9,"R rgl distance between axis ticks and tick labels Thanks firstly, to anyone who is taking time out of their day to help me solve my  problem - I am a newbie and this is my first post, so please be gentle!

I am plotting some point data using plot3d().
I would like to bring my y axis tick labels a little closer to my y axis tick marks.

The best way I can think of doing this is to

1) plot the data first, without drawing the axes

2) call on axis3d() to draw the y axis and tick marks but suppress labels from being drawn.

3) query the current position of each tick mark in 3D space. Store positions in a vector.

4) use mtext3d() to add labels at positions based on an adjustment to the vector


I am having a problem at step 3. I don't know how to query the position of each tick mark.
par3d() allows you to query a number of graphical parameters, is there something similar I can use to get the position each y axis tick?

Am I approaching this wrong? Probably.

Here is an example piece of code, without text added for y axis labels....

    require(rgl)
    x <- rnorm(5)
    y <- rnorm(5)
    z <- rnorm(5)
    open3d()
    plot3d(x,y,z,axes=F,xlab="""",ylab="""",zlab="""")
    par3d(ignoreExtent=TRUE)
    par3d(FOV=0)
    par3d(userMatrix=rotationMatrix(0,1,0,0))
    axis3d('y',nticks=5,labels = FALSE)
    par3d(zoom=1)
    par3d(windowRect=c(580,60,1380,900))

thanks a bunch

",2
6341743,06/14/2011 09:45:12,471448,10/10/2010 09:38:12,126,0,R how to calculate 12 months rolling return,"do you know how to calculate the 12 months rolling return and volatility?

performance analysis?",r,return,rolling,,,06/14/2011 12:18:40,off topic,1,14,8,"R how to calculate 12 months rolling return do you know how to calculate the 12 months rolling return and volatility?

performance analysis?",3
6267659,06/07/2011 15:29:51,759442,05/18/2011 14:58:24,8,1,How can subset assignment be useful?,"I have been reading the R Language Definition file.
Recently I came across subset assignment as a shortcut for writing expressions.  For example


    > x <- c(1:16)
    > x[3:5] <- 13:15
    > x
    [1]  1  2 13 14 15  6  7  8  9 10 11 12 13 14 15 16

instead of

    > x <- c(1:16)
    > x[3:5] <- x[13:15]
    > x

This can be made much more elaborate as in

    > x[3:5] <- 13:15 + 15
    > x
    [1]  1  2 28 29 30  6  7  8  9 10 11 12 13 14 15 16
    > x[3:5] <- 13:15*15:15
    > x
    [1]   1   2 195 210 225   6   7   8   9  10  11  12  13  14  15  16

To me this seems like a neat trick.  On the other hand it seems like using it will inevitably lead to unreadable code.

Does anyone know of a good reason to use this kind of feature?",r,indexing,subset,,,06/07/2011 16:32:32,not a real question,1,222,6,"How can subset assignment be useful? I have been reading the R Language Definition file.
Recently I came across subset assignment as a shortcut for writing expressions.  For example


    > x <- c(1:16)
    > x[3:5] <- 13:15
    > x
    [1]  1  2 13 14 15  6  7  8  9 10 11 12 13 14 15 16

instead of

    > x <- c(1:16)
    > x[3:5] <- x[13:15]
    > x

This can be made much more elaborate as in

    > x[3:5] <- 13:15 + 15
    > x
    [1]  1  2 28 29 30  6  7  8  9 10 11 12 13 14 15 16
    > x[3:5] <- 13:15*15:15
    > x
    [1]   1   2 195 210 225   6   7   8   9  10  11  12  13  14  15  16

To me this seems like a neat trick.  On the other hand it seems like using it will inevitably lead to unreadable code.

Does anyone know of a good reason to use this kind of feature?",3
8142362,11/15/2011 19:58:09,774837,05/29/2011 03:12:45,23,0,Apply function in R,"In R, how do you replace the following code using functions like `apply`, `lapply`, `rapply`, etc.? 
    
    u <- 10:15
    slist <- list()
    
    for (i in 1:length(u)) {
      p <- combn(u, i) 
      for (j in 1:ncol(p)) {
        s <- paste(p[,j], collapse="","")
        slist[[s]] <- 0
      }
    }

<br>

For this part: 

      for (j in 1:ncol(p)) {
        s <- paste(p[,j], collapse="","")

I tried something like:

      s <- apply(p, 2, function(x) paste(x, collapse="",""))

Which works. But then for that `slist[[s]] <- 0` part inside that same for-loop, I don't know what to do. 

Yeah, I'm just trying to learn how to use apply and stuff properly. ",r,function,vector,apply,lapply,,open,0,168,4,"Apply function in R In R, how do you replace the following code using functions like `apply`, `lapply`, `rapply`, etc.? 
    
    u <- 10:15
    slist <- list()
    
    for (i in 1:length(u)) {
      p <- combn(u, i) 
      for (j in 1:ncol(p)) {
        s <- paste(p[,j], collapse="","")
        slist[[s]] <- 0
      }
    }

<br>

For this part: 

      for (j in 1:ncol(p)) {
        s <- paste(p[,j], collapse="","")

I tried something like:

      s <- apply(p, 2, function(x) paste(x, collapse="",""))

Which works. But then for that `slist[[s]] <- 0` part inside that same for-loop, I don't know what to do. 

Yeah, I'm just trying to learn how to use apply and stuff properly. ",5
8970093,01/23/2012 10:18:13,1164661,01/23/2012 09:50:59,1,0,How to reverse only one y-axis in xyplot of zoo object,"I am having some trouble in creating a xyplot of a zoo object.

I have created a simplified example:

    z <- zoo(cbind(a=1:4,b=11:14,c=10:13,d=5:8,e=10:7,f=1:4), 1991:1994)

I am using the following code to create a multipanel xyplot:

    numLbl <- 4
    xx <- seq(from = min(time(z)), to = max(time(z)), length.out = numLbl)
    xyplot(z[,c(1,2,4,6)],scales = list(x = list(at = time (z), rot = 90)), layout = c(1,4), aspect = ""fill"", xlab = """",
	panel = function (x,y, ...) {
		panel.abline (v = xx, col = ""grey"", lty = 3)
		panel.xyplot(x, y, type = ""b"", col = 1)
	})
What I would like to do is the reverse the y-axis on only one of this panels. I have found a related example:

http://r.789695.n4.nabble.com/lattice-limits-in-reversed-order-with-relation-quot-same-quot-td2399883.html

However I cannot get the prepanel to work with my example. I am quite new to lattice, and there is probably something with the panel function I do not understand.

In addition if there is an easy way to get one of the panels as histograms and the others as lines I would be very grateful for tips.

Any help is very much appreciated!



",r,lattice,,,,,open,0,182,11,"How to reverse only one y-axis in xyplot of zoo object I am having some trouble in creating a xyplot of a zoo object.

I have created a simplified example:

    z <- zoo(cbind(a=1:4,b=11:14,c=10:13,d=5:8,e=10:7,f=1:4), 1991:1994)

I am using the following code to create a multipanel xyplot:

    numLbl <- 4
    xx <- seq(from = min(time(z)), to = max(time(z)), length.out = numLbl)
    xyplot(z[,c(1,2,4,6)],scales = list(x = list(at = time (z), rot = 90)), layout = c(1,4), aspect = ""fill"", xlab = """",
	panel = function (x,y, ...) {
		panel.abline (v = xx, col = ""grey"", lty = 3)
		panel.xyplot(x, y, type = ""b"", col = 1)
	})
What I would like to do is the reverse the y-axis on only one of this panels. I have found a related example:

http://r.789695.n4.nabble.com/lattice-limits-in-reversed-order-with-relation-quot-same-quot-td2399883.html

However I cannot get the prepanel to work with my example. I am quite new to lattice, and there is probably something with the panel function I do not understand.

In addition if there is an easy way to get one of the panels as histograms and the others as lines I would be very grateful for tips.

Any help is very much appreciated!



",2
3297491,07/21/2010 08:35:38,366256,06/14/2010 11:03:44,228,21,practically getting started with Sweave,"my question(s) might be less general than the title suggests. I am running R on Mac OS X with a MySQL database to store the data. I have been working with the Komodo / Sciviews-R for some time. Recently I had the need for auto-generated reports and looked into Sweave. I guess StatET / Eclipse appears to be the ""standard"" solution for Sweavers. 

1) Is it reasonable to switch from Komodo to StatET Eclipse? I tried StatET before but chose Komodo over StatET because I liked the calltip / autosuggest and the more convenient config from Komodo so much. 

2) What´s a reasonable workflow to generate Sweave files? Usually I develop my R code first and then care about the report later. I just learned today that there is one file in Sweave that contains R code and Latex code at once and that from this file the .tex document is created. While the example files look handily and can't really imagine how to enter my 250 + lines of R code to a file and mixed it up with Latex.

Is it possible to just enter the qplot() and ggplot() statements to a such a document and source the functionality like database connection and intermediate results somehow?


Or is it just a matter of being used to the mix of Latex and R code?

Thx for any suggestions, hints, links and back-to-the-roots-shout-outs…



",r,latex,sweave,,,,open,0,230,5,"practically getting started with Sweave my question(s) might be less general than the title suggests. I am running R on Mac OS X with a MySQL database to store the data. I have been working with the Komodo / Sciviews-R for some time. Recently I had the need for auto-generated reports and looked into Sweave. I guess StatET / Eclipse appears to be the ""standard"" solution for Sweavers. 

1) Is it reasonable to switch from Komodo to StatET Eclipse? I tried StatET before but chose Komodo over StatET because I liked the calltip / autosuggest and the more convenient config from Komodo so much. 

2) What´s a reasonable workflow to generate Sweave files? Usually I develop my R code first and then care about the report later. I just learned today that there is one file in Sweave that contains R code and Latex code at once and that from this file the .tex document is created. While the example files look handily and can't really imagine how to enter my 250 + lines of R code to a file and mixed it up with Latex.

Is it possible to just enter the qplot() and ggplot() statements to a such a document and source the functionality like database connection and intermediate results somehow?


Or is it just a matter of being used to the mix of Latex and R code?

Thx for any suggestions, hints, links and back-to-the-roots-shout-outs…



",3
11531611,07/17/2012 22:12:53,1533111,07/17/2012 22:05:51,1,0,RNCEP with R project,"When i run this script in R to calculate wind at specific coordinates
Uwnd <- NCEP.interp('uwnd', level=925, lat=lat, lon=long, dt=as.character(dt), reanalysis2 = FALSE)
I started to work until total progress 92% then everything failed and i got this error message
Error in read.table(file = out.temp, sep = "","", skip = 13, header = FALSE,  : 
  no lines available in input
anyone can help",r,,,,,07/18/2012 10:04:58,not constructive,1,62,4,"RNCEP with R project When i run this script in R to calculate wind at specific coordinates
Uwnd <- NCEP.interp('uwnd', level=925, lat=lat, lon=long, dt=as.character(dt), reanalysis2 = FALSE)
I started to work until total progress 92% then everything failed and i got this error message
Error in read.table(file = out.temp, sep = "","", skip = 13, header = FALSE,  : 
  no lines available in input
anyone can help",1
10032079,04/05/2012 16:08:00,1201032,02/10/2012 01:23:04,2100,73,crantastic - packages sorted by number of users,"On <a href=""http://crantastic.org/"">crantastic</a>, I can find packages sorted:

   1. alphabetically: http://crantastic.org/packages
   2. by users rating: http://crantastic.org/popcon

but I cannot browse my way to a page where they are sorted by number of users, which I would find extremely useful.

Did I miss a link somewhere? If there is no such page, can one of you come up with a little script for scraping all the packages data into a data.frame?

Thank you.",r,cran,rcurl,,,04/05/2012 16:26:31,not constructive,1,73,8,"crantastic - packages sorted by number of users On <a href=""http://crantastic.org/"">crantastic</a>, I can find packages sorted:

   1. alphabetically: http://crantastic.org/packages
   2. by users rating: http://crantastic.org/popcon

but I cannot browse my way to a page where they are sorted by number of users, which I would find extremely useful.

Did I miss a link somewhere? If there is no such page, can one of you come up with a little script for scraping all the packages data into a data.frame?

Thank you.",3
10477330,05/07/2012 05:57:58,1379023,05/07/2012 05:45:24,1,0,"Function, Vectors, and Loops in R","I recently began experimenting with R as a language to use for genetic programming. I have slowly but surely been learning more and more about how R works and its best coding practices.
Yet, I have hit a road block. Here is my situation. I have a dataset with roughly 700 rows, each row has 400 or so columns. I have everything setup that a function with a number of parameters the same as the number of columns gets sent as a parameter into an evaluation (fitness scoring) function. I want to go row by row in the dataset and pass the values in each column in a row into the function being evaluated. The first problem was figuring out how to pass in the parameters separately into the function. By ""separately"" I mean that the function expects 400 parameters, not a vector of length 400. To do this I used the following:

    do.call(function,as.list(parameters))

Where parameters is a vector of a month variable (1-12) that is appended to the values in a row in the dataset. This works fine, I just used a for loop to iterate over the 700 rows in the dataset and then another loop for the 12 months and use the above to accumulate a vector of outputs. The problem is this is painfully slow, around 24-28 seconds per function. And I have 100-500 functions sent into this evaluation every generation of evolution. The bottom line is this is not the way to go. Next I attempted to use the sapply method as below.

    outputs <- sapply(1:12,function(m) sapply(rows[1:length(rows)],function(p) do.call(f,as.list(c(p,m)))))

This applied (1-12) as the months and then applied (1-700) as the rows of the dataset. This took just as long. Any ideas on solutions would be helpful. 

Thanks, Isaac

 ",r,loops,genetic,,,,open,0,297,6,"Function, Vectors, and Loops in R I recently began experimenting with R as a language to use for genetic programming. I have slowly but surely been learning more and more about how R works and its best coding practices.
Yet, I have hit a road block. Here is my situation. I have a dataset with roughly 700 rows, each row has 400 or so columns. I have everything setup that a function with a number of parameters the same as the number of columns gets sent as a parameter into an evaluation (fitness scoring) function. I want to go row by row in the dataset and pass the values in each column in a row into the function being evaluated. The first problem was figuring out how to pass in the parameters separately into the function. By ""separately"" I mean that the function expects 400 parameters, not a vector of length 400. To do this I used the following:

    do.call(function,as.list(parameters))

Where parameters is a vector of a month variable (1-12) that is appended to the values in a row in the dataset. This works fine, I just used a for loop to iterate over the 700 rows in the dataset and then another loop for the 12 months and use the above to accumulate a vector of outputs. The problem is this is painfully slow, around 24-28 seconds per function. And I have 100-500 functions sent into this evaluation every generation of evolution. The bottom line is this is not the way to go. Next I attempted to use the sapply method as below.

    outputs <- sapply(1:12,function(m) sapply(rows[1:length(rows)],function(p) do.call(f,as.list(c(p,m)))))

This applied (1-12) as the months and then applied (1-700) as the rows of the dataset. This took just as long. Any ideas on solutions would be helpful. 

Thanks, Isaac

 ",3
4216298,11/18/2010 15:13:53,247707,01/11/2010 00:29:39,29,0,Partial data sets in R,"We are looking to train R with structures like:
age, data1, data2, ... dataN, actions

where N depends on the amount of data we have about a person.

Our goal is to determine how likely is it that another person would generate actions by querying on all the data we have him/her. 


age, data1, data2, ...dataM where M could be bigger or smaller than N.

With complete data-sets we could have used binary logistic regression. But we need to use partial sets.",r,statistics,,,,11/18/2010 19:11:57,not a real question,1,78,5,"Partial data sets in R We are looking to train R with structures like:
age, data1, data2, ... dataN, actions

where N depends on the amount of data we have about a person.

Our goal is to determine how likely is it that another person would generate actions by querying on all the data we have him/her. 


age, data1, data2, ...dataM where M could be bigger or smaller than N.

With complete data-sets we could have used binary logistic regression. But we need to use partial sets.",2
6230706,06/03/2011 17:46:17,599139,02/01/2011 21:53:56,63,2,What's the most useful ggplot2 tip or trick?,"GGplot2 is a ubiquitous and powerful package, but many of the features and functions are not easy to find. I've found a couple of useful functions lately and I'd like to learn some more. It's possible this is a Community Wiki topic.

One item per post.",r,ggplot2,,,,11/18/2011 06:25:06,not constructive,1,45,8,"What's the most useful ggplot2 tip or trick? GGplot2 is a ubiquitous and powerful package, but many of the features and functions are not easy to find. I've found a couple of useful functions lately and I'd like to learn some more. It's possible this is a Community Wiki topic.

One item per post.",2
7175050,08/24/2011 12:00:12,907141,08/23/2011 06:40:14,3,0,R increase efficiency in creating/recording signals,"I was able to create a signal based on dispersion in my data set as follows:

    sig <- ifelse(merged[,1] < merged[,2], 1, ifelse(merged[,1] > merged[,4],-1,0))

This yields 3 states: -1,0,1

I now want to add a secondary condition, which would keep my states (-1 or 1) until another state is triggered. Currently I do this in a for loop:

    for(i in 2:nrow(my.sample))
    {
    	if (my.sample[i-1,6] == -1 && my.sample[i,1]>my.sample[i,3])
    	{ 
             my.sample[i,6] <- -1   
    	}
    
    	if (my.sample[i-1,6] == 1 && my.sample[i,1]<my.sample[i,3])
    	{ 
             my.sample[i,6] <- 1   
    	}    		
    }

The code does exactly what I want, however, performance is an issue during step 2. This is due to the large quantities of data I have. Unfortunately my R skills are still developing, so I would appreciate it if someone could assist in suggesting any improvements in the code that would allow for a more elegant solution.

Ed",r,,,,,08/24/2011 18:44:39,too localized,1,210,6,"R increase efficiency in creating/recording signals I was able to create a signal based on dispersion in my data set as follows:

    sig <- ifelse(merged[,1] < merged[,2], 1, ifelse(merged[,1] > merged[,4],-1,0))

This yields 3 states: -1,0,1

I now want to add a secondary condition, which would keep my states (-1 or 1) until another state is triggered. Currently I do this in a for loop:

    for(i in 2:nrow(my.sample))
    {
    	if (my.sample[i-1,6] == -1 && my.sample[i,1]>my.sample[i,3])
    	{ 
             my.sample[i,6] <- -1   
    	}
    
    	if (my.sample[i-1,6] == 1 && my.sample[i,1]<my.sample[i,3])
    	{ 
             my.sample[i,6] <- 1   
    	}    		
    }

The code does exactly what I want, however, performance is an issue during step 2. This is due to the large quantities of data I have. Unfortunately my R skills are still developing, so I would appreciate it if someone could assist in suggesting any improvements in the code that would allow for a more elegant solution.

Ed",1
7211329,08/26/2011 22:59:11,406278,07/29/2010 22:53:08,458,3,Creating Large Data Frames,"
Let's say that I want to generate a large data frame from scratch.

Using the data.frame function is how I would generally create data frames.
However, df's like the following are extremely error prone and inefficient.

So is there a more efficient way of creating the following data frame.


    df <- data.frame(GOOGLE_CAMPAIGN=c(rep(""Google - Medicare - US"", 928), rep(""MedicareBranded"", 2983),
                                       rep(""Medigap"", 805), rep(""Medigap Branded"", 1914),
                                       rep(""Medicare Typos"", 1353), rep(""Medigap Typos"", 635),
                                       rep(""Phone - MedicareGeneral"", 585),
                                       rep(""Phone - MedicareBranded"", 2967),
                                       rep(""Phone-Medigap"", 812),
                                       rep(""Auto Broad Match"", 27),
                                       rep(""Auto Exact Match"", 80),
                                       rep(""Auto Exact Match"", 875)),                   
                     GOOGLE_AD_GROUP=c(rep(""Medicare"", 928), rep(""MedicareBranded"", 2983),
                                       rep(""Medigap"", 805), rep(""Medigap Branded"", 1914),
                                       rep(""Medicare Typos"", 1353), rep(""Medigap Typos"", 635),
                                       rep(""Phone ads 1-Medicare Terms"",585),
                                       rep(""Ad Group #1"", 2967), rep(""Medigap-phone"", 812),
                                       rep(""Auto Insurance"", 27),
                                       rep(""Auto General"", 80),
                                       rep(""Auto Brand"", 875)))


Yikes, that is some 'bad' code. How can I generate this 'large' data frame in a more efficient manner?",r,data.frame,,,,,open,0,753,4,"Creating Large Data Frames 
Let's say that I want to generate a large data frame from scratch.

Using the data.frame function is how I would generally create data frames.
However, df's like the following are extremely error prone and inefficient.

So is there a more efficient way of creating the following data frame.


    df <- data.frame(GOOGLE_CAMPAIGN=c(rep(""Google - Medicare - US"", 928), rep(""MedicareBranded"", 2983),
                                       rep(""Medigap"", 805), rep(""Medigap Branded"", 1914),
                                       rep(""Medicare Typos"", 1353), rep(""Medigap Typos"", 635),
                                       rep(""Phone - MedicareGeneral"", 585),
                                       rep(""Phone - MedicareBranded"", 2967),
                                       rep(""Phone-Medigap"", 812),
                                       rep(""Auto Broad Match"", 27),
                                       rep(""Auto Exact Match"", 80),
                                       rep(""Auto Exact Match"", 875)),                   
                     GOOGLE_AD_GROUP=c(rep(""Medicare"", 928), rep(""MedicareBranded"", 2983),
                                       rep(""Medigap"", 805), rep(""Medigap Branded"", 1914),
                                       rep(""Medicare Typos"", 1353), rep(""Medigap Typos"", 635),
                                       rep(""Phone ads 1-Medicare Terms"",585),
                                       rep(""Ad Group #1"", 2967), rep(""Medigap-phone"", 812),
                                       rep(""Auto Insurance"", 27),
                                       rep(""Auto General"", 80),
                                       rep(""Auto Brand"", 875)))


Yikes, that is some 'bad' code. How can I generate this 'large' data frame in a more efficient manner?",2
8575877,12/20/2011 13:03:15,1077190,12/02/2011 10:30:05,1,0,Books related to predictive modelling in R,Are there any books where Predictive modelling can be learned using R language,r,,,,,12/20/2011 16:01:35,off topic,1,13,7,Books related to predictive modelling in R Are there any books where Predictive modelling can be learned using R language,1
7918040,10/27/2011 15:08:33,1016752,10/27/2011 15:01:48,1,0,error in readHTMLTable,"I am trying to extract an HTML data but keep getting this error:
Error in htmlParse(doc) : 
  error in creating parser for http://en.wikipedia.org/wiki/World_population

this is the code I am using
library(XML)
url <- 'http://en.wikipedia.org/wiki/World_population'
tbls <- readHTMLTable(url)

I tried to users setInternet2() and set the proxy as it should. nothing helps.
Any ideas?

Thansk,

Tomer",r,,,,,,open,0,48,3,"error in readHTMLTable I am trying to extract an HTML data but keep getting this error:
Error in htmlParse(doc) : 
  error in creating parser for http://en.wikipedia.org/wiki/World_population

this is the code I am using
library(XML)
url <- 'http://en.wikipedia.org/wiki/World_population'
tbls <- readHTMLTable(url)

I tried to users setInternet2() and set the proxy as it should. nothing helps.
Any ideas?

Thansk,

Tomer",1
1256347,08/10/2009 18:08:30,138470,07/15/2009 04:23:30,1,0,"Plot time data in R to various resolutions (to the minute, to the hour, to the second, etc.)","I have some data in CSV like:

    ""Timestamp"", ""Count""
    ""2009-07-20 16:30:45"", 10
    ""2009-07-20 16:30:45"", 15
    ""2009-07-20 16:30:46"", 8
    ""2009-07-20 16:30:46"", 6
    ""2009-07-20 16:30:46"", 8
    ""2009-07-20 16:30:47"", 20

I can read it into R using read.cvs. I'd like to plot:

 1. Number of entries per second, so:
    <pre>
    ""2009-07-20 16:30:45"", 2
    ""2009-07-20 16:30:46"", 3
    ""2009-07-20 16:30:47"", 1
    </pre>
 2. Average value per second:
   <pre>
    ""2009-07-20 16:30:45"", 12.5
    ""2009-07-20 16:30:46"", 7.333
    ""2009-07-20 16:30:47"", 20
   </pre>
 3. Same as 1 & 2 but then by Minute and then by Hour.

Is there some way to do this (collect by second/min/etc & plot) in R? 












",r,plot,time,statistics,,,open,0,149,18,"Plot time data in R to various resolutions (to the minute, to the hour, to the second, etc.) I have some data in CSV like:

    ""Timestamp"", ""Count""
    ""2009-07-20 16:30:45"", 10
    ""2009-07-20 16:30:45"", 15
    ""2009-07-20 16:30:46"", 8
    ""2009-07-20 16:30:46"", 6
    ""2009-07-20 16:30:46"", 8
    ""2009-07-20 16:30:47"", 20

I can read it into R using read.cvs. I'd like to plot:

 1. Number of entries per second, so:
    <pre>
    ""2009-07-20 16:30:45"", 2
    ""2009-07-20 16:30:46"", 3
    ""2009-07-20 16:30:47"", 1
    </pre>
 2. Average value per second:
   <pre>
    ""2009-07-20 16:30:45"", 12.5
    ""2009-07-20 16:30:46"", 7.333
    ""2009-07-20 16:30:47"", 20
   </pre>
 3. Same as 1 & 2 but then by Minute and then by Hour.

Is there some way to do this (collect by second/min/etc & plot) in R? 












",4
9352966,02/19/2012 20:32:49,535458,10/15/2010 17:37:57,370,2,Using character values as object names,"I would like to use the characters in a vector as the names of character objects
aiming to get

    first as say ""d"",""e"",""a"",""t"" etc.

tried this approach but am clearly missing some function to apply to x[i]

    x <- c(""first"",""second"",""third""..)

    for (i in 1:length(x)) {
    x[i] <- sample(letters,4)
    }
TIA",r,vector,,,,,open,0,62,6,"Using character values as object names I would like to use the characters in a vector as the names of character objects
aiming to get

    first as say ""d"",""e"",""a"",""t"" etc.

tried this approach but am clearly missing some function to apply to x[i]

    x <- c(""first"",""second"",""third""..)

    for (i in 1:length(x)) {
    x[i] <- sample(letters,4)
    }
TIA",2
11422031,07/10/2012 21:12:21,1256539,03/08/2012 08:30:14,61,1,Using ggplot in R,"I have a time series cvs file like this (in var there are the values):

Hour,Date,Type,val1,val2,val3,val4,val5,val6.....  
0100,0153,aaa,   
0100,0153,bbb,  
0100,0153,ccc,  
0100,0153,ddd,  
0200,0153,aaa,  
0200,0153,bbb,  
0200,0153,ccc,  
0200,0153,ddd,  
.  
.  
0100,0154,aaa,  
0100,0154,bbb,  
0100,0154,ccc,  
0100,0154,ddd,  
.  
.  

I would like to use ggplot to plot the barplot  mean of the time series of val1,val2,... for each type. One plot for each type except for ccc and ddd that should be considered as one type, so for this I should first sum up the value, then take the mean, then plot). So three plot at the end. 

Thanks for advice.  
",r,ggplot,,,,07/10/2012 22:32:27,not a real question,1,114,4,"Using ggplot in R I have a time series cvs file like this (in var there are the values):

Hour,Date,Type,val1,val2,val3,val4,val5,val6.....  
0100,0153,aaa,   
0100,0153,bbb,  
0100,0153,ccc,  
0100,0153,ddd,  
0200,0153,aaa,  
0200,0153,bbb,  
0200,0153,ccc,  
0200,0153,ddd,  
.  
.  
0100,0154,aaa,  
0100,0154,bbb,  
0100,0154,ccc,  
0100,0154,ddd,  
.  
.  

I would like to use ggplot to plot the barplot  mean of the time series of val1,val2,... for each type. One plot for each type except for ccc and ddd that should be considered as one type, so for this I should first sum up the value, then take the mean, then plot). So three plot at the end. 

Thanks for advice.  
",2
10273540,04/23/2012 00:45:20,1208541,02/14/2012 07:36:37,1,0,Improving R Code for Memory Experiment,"I am working on a data set for my experiment  and I am trying to find out how how many trials resulted in an incorrect action on the part of the participant. In my table (see below) I added three columns at the end: prefix, corrfix, and errfix (0 indicates error and a numerical value indicates a correct action). What I want to start with is seeing how many trials per participant resulted in an incorrect action for the prefix column (i.e., there is no numerical value for that trial, indicating that they did no perform a correct action).

       RECORDING_SESSION_LABEL CURRENT_FIX_INDEX identifier prefix corrfix errfix
       10161      3    1      0     243      0
       10161      1   29      0       0      0
       10161      2   29      0       0      0
       10161      3  29    201       0      0
       10161      4  29    130       0      0
       10161      5  29      0     231      0

I have been working on a code, but it will return multiple lines (of the same trial) when I only want trials where there is no numerical value (each participant should different trials per line) I would appreciate any help or suggestions. I am trying hard to refine my R skills. 

      flag=0
      flag=1
      if(new[i,]$CURRENT_FIX_INDEX==1){
      flag=1
      }
      if (flag==1) {
      if(new[i,]$prefix==0){
      cbind(new$RECORDING_SESSION_LABEL, new$identifier)}
",r,,,,,04/23/2012 11:04:43,too localized,1,414,6,"Improving R Code for Memory Experiment I am working on a data set for my experiment  and I am trying to find out how how many trials resulted in an incorrect action on the part of the participant. In my table (see below) I added three columns at the end: prefix, corrfix, and errfix (0 indicates error and a numerical value indicates a correct action). What I want to start with is seeing how many trials per participant resulted in an incorrect action for the prefix column (i.e., there is no numerical value for that trial, indicating that they did no perform a correct action).

       RECORDING_SESSION_LABEL CURRENT_FIX_INDEX identifier prefix corrfix errfix
       10161      3    1      0     243      0
       10161      1   29      0       0      0
       10161      2   29      0       0      0
       10161      3  29    201       0      0
       10161      4  29    130       0      0
       10161      5  29      0     231      0

I have been working on a code, but it will return multiple lines (of the same trial) when I only want trials where there is no numerical value (each participant should different trials per line) I would appreciate any help or suggestions. I am trying hard to refine my R skills. 

      flag=0
      flag=1
      if(new[i,]$CURRENT_FIX_INDEX==1){
      flag=1
      }
      if (flag==1) {
      if(new[i,]$prefix==0){
      cbind(new$RECORDING_SESSION_LABEL, new$identifier)}
",1
10180631,04/16/2012 19:48:25,1337143,04/16/2012 19:45:30,1,0,Bivariate Poisson Regression in R?,"I found a package 'bivpois' for R which evaluates a model for two related poisson processes (for example, the number of goals by the home and the away team in a soccer game).  However, this package seems to no longer be useable in newer versions of R.

Is there a reasonable way to modify the glm() function to do a similar process, or run this older package on my new version of R?  I have found very little literature on these sorts of processes and have found very little in terms of easy implementation in other statistical packages like STATA.

Any suggestions would be much appreciated.",r,poisson,,,,,open,0,106,5,"Bivariate Poisson Regression in R? I found a package 'bivpois' for R which evaluates a model for two related poisson processes (for example, the number of goals by the home and the away team in a soccer game).  However, this package seems to no longer be useable in newer versions of R.

Is there a reasonable way to modify the glm() function to do a similar process, or run this older package on my new version of R?  I have found very little literature on these sorts of processes and have found very little in terms of easy implementation in other statistical packages like STATA.

Any suggestions would be much appreciated.",2
3365657,07/29/2010 18:08:27,313163,04/09/2010 20:31:46,283,5,Is there a way to make R beep/play a sound at the end of a script?,"When I run R scripts I go do something else on a different desktop. If I don't check frequently, I never know when something is finished. Is there a way to invoke a beep (like a system beep) or get R to play a sound or notify growl via some code at the end of my script?",r,,,,,,open,0,57,16,"Is there a way to make R beep/play a sound at the end of a script? When I run R scripts I go do something else on a different desktop. If I don't check frequently, I never know when something is finished. Is there a way to invoke a beep (like a system beep) or get R to play a sound or notify growl via some code at the end of my script?",1
11167154,06/23/2012 05:30:25,702432,04/11/2011 15:15:31,567,2,"R: Predict (0,1) in logistic regression in glm()","I am trying to model a ""what if"" situation in a binary logit model. I am estimating the probability of passing a test, given the level of difficulty of the test (1=easiest, 5=toughest), with gender as control. (The data is [here][1]). Students are administered a test which is generally tough (""HIGH"" in the data). From this we can estimate the impact of test-difficulty on the likelihood of passing:

    model = glm(PASS ~ as.factor(SEX) + as.factor(HIGH), family=binomial(link=""logit""), data=df)
    summary(model)

We can also get the predicted probabilities of passing with:

    predict.high = predict(model, type=""response"")

The question is, what if the ""LOW"" test were given instead? To get the new probabilities, we can do: 

    newdata = rename.vars(subset(df, select=c(-HIGH)), 'LOW','HIGH')
    predict.low = predict(model, newdata=newdata, type=""response"")

But how do I know how many additional students would have passed in this case? Is there an obvious switch in `glm()` I am not seeing?

  [1]: http://dl.dropbox.com/u/1791181/bayesglm.csv


",r,,,,,,open,0,163,8,"R: Predict (0,1) in logistic regression in glm() I am trying to model a ""what if"" situation in a binary logit model. I am estimating the probability of passing a test, given the level of difficulty of the test (1=easiest, 5=toughest), with gender as control. (The data is [here][1]). Students are administered a test which is generally tough (""HIGH"" in the data). From this we can estimate the impact of test-difficulty on the likelihood of passing:

    model = glm(PASS ~ as.factor(SEX) + as.factor(HIGH), family=binomial(link=""logit""), data=df)
    summary(model)

We can also get the predicted probabilities of passing with:

    predict.high = predict(model, type=""response"")

The question is, what if the ""LOW"" test were given instead? To get the new probabilities, we can do: 

    newdata = rename.vars(subset(df, select=c(-HIGH)), 'LOW','HIGH')
    predict.low = predict(model, newdata=newdata, type=""response"")

But how do I know how many additional students would have passed in this case? Is there an obvious switch in `glm()` I am not seeing?

  [1]: http://dl.dropbox.com/u/1791181/bayesglm.csv


",1
4954507,02/10/2011 07:35:53,513744,11/19/2010 16:27:30,97,0,Calculate the Area under a Curve in R,"I would like to calculate the area under a curve to do integration without defining a function such as in integrate().

My data looks as this:

    Date          Strike     Volatility
    2003-01-01    20         0.2
    2003-01-01    30         0.3
    2003-01-01    40         0.4
    etc.

I plotted plot(strike, volatility) to look at the volatility smile. Is there a way to integrate this plotted ""curve""?

Thanks for help
Dani",r,integration,,,,,open,0,119,8,"Calculate the Area under a Curve in R I would like to calculate the area under a curve to do integration without defining a function such as in integrate().

My data looks as this:

    Date          Strike     Volatility
    2003-01-01    20         0.2
    2003-01-01    30         0.3
    2003-01-01    40         0.4
    etc.

I plotted plot(strike, volatility) to look at the volatility smile. Is there a way to integrate this plotted ""curve""?

Thanks for help
Dani",2
4494294,12/20/2010 21:50:16,369565,06/17/2010 16:37:09,25,0,"R: gplots, barplots: how to fix bar width independent of paper setting?","I use the gplots package to output barplots. I use it inside a for-loop, so rest of the code is omitted to make it more clear:

    library(""gplots"")
    pdf(file = ""/Users/Tim/desktop/pgax.pdf"", onefile = TRUE, paper = ""special"")
    par(mfrow = c(4,2)) #figures arranged in 2 rows and 2 columns
    par(las=2) #perpendicular labels on x-axis
    
    barplot2(expression,ylab = expression(expression),main = graph.header, cex.names =0.85, beside = TRUE, offset = 0, xpd = FALSE,axis.lty = 0, cex.axis = 0.85, plot.ci = TRUE,ci.l = expression - sd.value, ci.u = expression + sd.value, col = colors,width = 1,names.arg = c(etc))

Now when I specify the papersize at a4, and print out in two columns the bars are made so they fill up the full space assigned. If I only have a few bars in each graws, the width is too big compared to the height. I know I should be using `xlimit and width` = amongst and perhaps even the aspect ratio?, but I can't get the results I wanted. And unconvenient way is to specify the height and width output of the paper, and manually I adjust it for the number of bars in the plots each time. But this doesnt seem appropiate. Does someone know a convenient way to fix width bars in my plots?

All help is much appreciated!

",r,graph,,,,,open,0,231,12,"R: gplots, barplots: how to fix bar width independent of paper setting? I use the gplots package to output barplots. I use it inside a for-loop, so rest of the code is omitted to make it more clear:

    library(""gplots"")
    pdf(file = ""/Users/Tim/desktop/pgax.pdf"", onefile = TRUE, paper = ""special"")
    par(mfrow = c(4,2)) #figures arranged in 2 rows and 2 columns
    par(las=2) #perpendicular labels on x-axis
    
    barplot2(expression,ylab = expression(expression),main = graph.header, cex.names =0.85, beside = TRUE, offset = 0, xpd = FALSE,axis.lty = 0, cex.axis = 0.85, plot.ci = TRUE,ci.l = expression - sd.value, ci.u = expression + sd.value, col = colors,width = 1,names.arg = c(etc))

Now when I specify the papersize at a4, and print out in two columns the bars are made so they fill up the full space assigned. If I only have a few bars in each graws, the width is too big compared to the height. I know I should be using `xlimit and width` = amongst and perhaps even the aspect ratio?, but I can't get the results I wanted. And unconvenient way is to specify the height and width output of the paper, and manually I adjust it for the number of bars in the plots each time. But this doesnt seem appropiate. Does someone know a convenient way to fix width bars in my plots?

All help is much appreciated!

",2
11707808,07/29/2012 08:25:05,1342697,04/18/2012 23:29:15,19,0,r programming: what text editor is this from?,"for those of who are familiar with R, I was wondering what text editor was used to make this code. I noticed ***his equal signs are lined up vertically*** and it looks a lot cleaner to read. I haven't seen this in vim, is it maybe emacs? The code is:

    # Function to Get Ad Details by Ad URL
    get_ad_details = function(ad_url){
       require(XML)
       # parse ad url to html tree
       doc = htmlTreeParse(ad_url, useInternalNodes = T)
    
       # extract labels and values using xpath expression
       labels  = xpathSApply(doc, ""//span[contains(@class, 'ad')]/label"", xmlValue)
       values1 = xpathSApply(doc, ""//span[contains(@class, 'ad')]/strong"", xmlValue)
       values2 = xpathSApply(doc, ""//span[contains(@class, 'ad')]//a"", xmlValue)
       values  = c(values1, values2)
    
       # convert to data frame and add labels
       mydf        = as.data.frame(t(values))
       names(mydf) = labels
       return(mydf)
    }",r,text-editor,,,,07/29/2012 12:29:15,not a real question,1,220,8,"r programming: what text editor is this from? for those of who are familiar with R, I was wondering what text editor was used to make this code. I noticed ***his equal signs are lined up vertically*** and it looks a lot cleaner to read. I haven't seen this in vim, is it maybe emacs? The code is:

    # Function to Get Ad Details by Ad URL
    get_ad_details = function(ad_url){
       require(XML)
       # parse ad url to html tree
       doc = htmlTreeParse(ad_url, useInternalNodes = T)
    
       # extract labels and values using xpath expression
       labels  = xpathSApply(doc, ""//span[contains(@class, 'ad')]/label"", xmlValue)
       values1 = xpathSApply(doc, ""//span[contains(@class, 'ad')]/strong"", xmlValue)
       values2 = xpathSApply(doc, ""//span[contains(@class, 'ad')]//a"", xmlValue)
       values  = c(values1, values2)
    
       # convert to data frame and add labels
       mydf        = as.data.frame(t(values))
       names(mydf) = labels
       return(mydf)
    }",2
5869366,05/03/2011 12:21:43,734124,05/02/2011 08:20:01,4,0,Residual Analysis in tobit Modell,"is it possible to do a residual analysis of a tobit model? plot(<modxy>) doesn't work? When it's not possible, what's the reason? I appreciate any thought!",r,statistics,,,,05/03/2011 12:57:11,off topic,1,26,5,"Residual Analysis in tobit Modell is it possible to do a residual analysis of a tobit model? plot(<modxy>) doesn't work? When it's not possible, what's the reason? I appreciate any thought!",2
4173019,11/13/2010 14:33:40,334755,05/06/2010 18:53:39,419,24,Load MacPorts SQLite3 when using RSQLite library,"I have a user defined function in SQLite (an aggregator that calculates the product) and it works fine outside R. But I'm on a Mac some of the time, which requires the MacPorts version of SQLite3 if you'd like to add your own functions/extensions.

Can I pick which SQLite3 that RSQLite loads? I don't see anything in the SQLite documentation.

Also, MacPorts appears to change my `sqlite3` link to the MacPorts installed SQLite3:

    mbp:~ richard$ which sqlite3
    /opt/local/bin/sqlite3

But if I want to load the extension in SQLite3, I have to explicitly can the MacPorts version, like this:

    mbp:~ richard$ /opt/local/bin/sqlite3 temp.sqlite

Is writing my own SQLite functions and combining them with R a lost cause? Thanks!",r,macports,,,,,open,0,122,7,"Load MacPorts SQLite3 when using RSQLite library I have a user defined function in SQLite (an aggregator that calculates the product) and it works fine outside R. But I'm on a Mac some of the time, which requires the MacPorts version of SQLite3 if you'd like to add your own functions/extensions.

Can I pick which SQLite3 that RSQLite loads? I don't see anything in the SQLite documentation.

Also, MacPorts appears to change my `sqlite3` link to the MacPorts installed SQLite3:

    mbp:~ richard$ which sqlite3
    /opt/local/bin/sqlite3

But if I want to load the extension in SQLite3, I have to explicitly can the MacPorts version, like this:

    mbp:~ richard$ /opt/local/bin/sqlite3 temp.sqlite

Is writing my own SQLite functions and combining them with R a lost cause? Thanks!",2
11654515,07/25/2012 16:36:53,783421,06/03/2011 21:50:17,96,2,R packages - could they contain logos of firm that made them and still be in CRAN Packages Repository?,R packages - could they contain logos of firm that made them and still be in CRAN Packages Repository ?  I think about logo on every plot that is produced by functions from that package ?,r,,,,,07/25/2012 17:13:36,off topic,1,37,19,R packages - could they contain logos of firm that made them and still be in CRAN Packages Repository? R packages - could they contain logos of firm that made them and still be in CRAN Packages Repository ?  I think about logo on every plot that is produced by functions from that package ?,1
3644510,09/05/2010 00:23:46,227290,12/08/2009 16:46:07,64,0,Treatment of 'empty' values in R,"This is a newbie question in R. I am importing a csv file into R using sqldf package. I have several missing values for both numeric and string variables. I notice that missing values are left empty in the dataframe (as opposed to being filled with NA or something else). I want to replace the missing values with an user defined value. Obviously, a function like is.na() will not work in this case.
Thank you in advance.
Toy dataframe with three columns.  
A  B  C  
 3      4  
2   4     6   
34 23 43   
2 5   

I want

A  B  C  
 3      4 NA  
2   4     6   
34 23 43   
2 5  NA 




",r,,,,,,open,0,154,6,"Treatment of 'empty' values in R This is a newbie question in R. I am importing a csv file into R using sqldf package. I have several missing values for both numeric and string variables. I notice that missing values are left empty in the dataframe (as opposed to being filled with NA or something else). I want to replace the missing values with an user defined value. Obviously, a function like is.na() will not work in this case.
Thank you in advance.
Toy dataframe with three columns.  
A  B  C  
 3      4  
2   4     6   
34 23 43   
2 5   

I want

A  B  C  
 3      4 NA  
2   4     6   
34 23 43   
2 5  NA 




",1
6603531,07/06/2011 21:53:50,160511,08/21/2009 03:27:54,477,3,R: checking for defined values in subset(),"Trying to get a subset of a data frame based on, to borrow from SQL, values that are not null. Trying something like:

    lately <- subset(data, year > 1997 & myvalue != NA)

But that's not right. Any tips, r'sters?
",r,,,,,,open,0,42,7,"R: checking for defined values in subset() Trying to get a subset of a data frame based on, to borrow from SQL, values that are not null. Trying something like:

    lately <- subset(data, year > 1997 & myvalue != NA)

But that's not right. Any tips, r'sters?
",1
8256630,11/24/2011 11:55:08,236215,12/21/2009 17:39:14,589,1,Identifying the numbers of AR or MA terms in matlab,"How do I identify the order of ARMA model for given time series in matlab. In R, there is a function ar, auto.arima that searches for the optimal number of lags for AR and MA models. But I haven't been able to find a similar function in matlab. The AR function in matlab estimates parameters for a 'specified' number of lags. Any ideas?

Secondly, is there a function similar to arima.sim in matlab to simulate arima processes?
",r,matlab,,,,,open,0,76,10,"Identifying the numbers of AR or MA terms in matlab How do I identify the order of ARMA model for given time series in matlab. In R, there is a function ar, auto.arima that searches for the optimal number of lags for AR and MA models. But I haven't been able to find a similar function in matlab. The AR function in matlab estimates parameters for a 'specified' number of lags. Any ideas?

Secondly, is there a function similar to arima.sim in matlab to simulate arima processes?
",2
11515479,07/17/2012 03:06:16,1268117,03/14/2012 05:14:05,33,0,Trouble with plotting graphs in R,"I have some graphs with latitude and longitude data for each vertex and attributes called ""tobichi"" and ""sea"" for each edge (essentially, whether to include the edge in plotting or not, presence of either indicating a no). I'm using igraph to visualize this and am writing my own functions to do so. Here's my code so far:

    planar.embedding <- function(graph) {
    	edge.incidence <- get.edgelist(graph)
    	edges.x <- vector(length = (2 * ecount(graph)))
    	edges.y <- vector(length = (2 * ecount(graph)))
    	for (i in 1:ecount(graph)) {
    		edges.x[2 * i - 1] <- get.vertex.attribute(graph, name=""lon"", index=edge.incidence[i, 1])
    		edges.y[2 * i - 1] <- get.vertex.attribute(graph, name=""lat"", index=edge.incidence[i, 1])
    		edges.x[2 * i] <- get.vertex.attribute(graph, name=""lon"", index=edge.incidence[i, 2])
    		edges.y[2 * i] <- get.vertex.attribute(graph, name=""lat"", index=edge.incidence[i, 2])
    	}
    	edg <- get.edge.attribute(graph, name=""tobichi"", index=E(graph))
    	c <- get.edge.attribute(graph, name=""sea"", index=E(graph))
    	cedg <- c | edg
    	zedg <- c(2 * which(cedg == TRUE), 2 * which(cedg == TRUE) - 1)
    	edges.x <- edges.x[-zedg]
    	edges.y <- edges.y[-zedg]
    	return(cbind(edges.x, edges.y))
    }

    planar.plot <- function(graph) {
    	plot(x=get.vertex.attribute(graph, name=""lon"", index=V(graph)), y=get.vertex.attribute(graph, name=""lat"", index=V(graph)))
    	lines(x=planar.embedding(graph)[, ""edges.x""], y=planar.embedding(graph)[, ""edges.y""])
    }

Because of how `lines()` works, edges.x and edges.y are twice as long as the number of edges in the graph, having one entry each for source and sink x and y coordinates. Here `edge.incidence[i, 1]` gets the source vertex, and `edge.incidence[i, 2]` gets the sink vertex (of edge i).

The problem is that this works on some graphs but not others. Interestingly enough, when I omit lines 10-15 in `planar.embedding` everything works fine. What could be going wrong here?",r,graph,igraph,,,07/17/2012 11:45:57,too localized,1,316,6,"Trouble with plotting graphs in R I have some graphs with latitude and longitude data for each vertex and attributes called ""tobichi"" and ""sea"" for each edge (essentially, whether to include the edge in plotting or not, presence of either indicating a no). I'm using igraph to visualize this and am writing my own functions to do so. Here's my code so far:

    planar.embedding <- function(graph) {
    	edge.incidence <- get.edgelist(graph)
    	edges.x <- vector(length = (2 * ecount(graph)))
    	edges.y <- vector(length = (2 * ecount(graph)))
    	for (i in 1:ecount(graph)) {
    		edges.x[2 * i - 1] <- get.vertex.attribute(graph, name=""lon"", index=edge.incidence[i, 1])
    		edges.y[2 * i - 1] <- get.vertex.attribute(graph, name=""lat"", index=edge.incidence[i, 1])
    		edges.x[2 * i] <- get.vertex.attribute(graph, name=""lon"", index=edge.incidence[i, 2])
    		edges.y[2 * i] <- get.vertex.attribute(graph, name=""lat"", index=edge.incidence[i, 2])
    	}
    	edg <- get.edge.attribute(graph, name=""tobichi"", index=E(graph))
    	c <- get.edge.attribute(graph, name=""sea"", index=E(graph))
    	cedg <- c | edg
    	zedg <- c(2 * which(cedg == TRUE), 2 * which(cedg == TRUE) - 1)
    	edges.x <- edges.x[-zedg]
    	edges.y <- edges.y[-zedg]
    	return(cbind(edges.x, edges.y))
    }

    planar.plot <- function(graph) {
    	plot(x=get.vertex.attribute(graph, name=""lon"", index=V(graph)), y=get.vertex.attribute(graph, name=""lat"", index=V(graph)))
    	lines(x=planar.embedding(graph)[, ""edges.x""], y=planar.embedding(graph)[, ""edges.y""])
    }

Because of how `lines()` works, edges.x and edges.y are twice as long as the number of edges in the graph, having one entry each for source and sink x and y coordinates. Here `edge.incidence[i, 1]` gets the source vertex, and `edge.incidence[i, 2]` gets the sink vertex (of edge i).

The problem is that this works on some graphs but not others. Interestingly enough, when I omit lines 10-15 in `planar.embedding` everything works fine. What could be going wrong here?",3
5005989,02/15/2011 15:56:53,242673,01/03/2010 12:44:48,50,7,How to download search results on google scholar using r?,"I would like to get, say, first 100 results of a search on google scholar using R. Does anyone know how to do it? To be precise, I just need the name of the paper, authors and citation count.

Thanks in advance,
Manoel

ps.: It is legal to do that, right?

",r,google,,,,,open,0,48,10,"How to download search results on google scholar using r? I would like to get, say, first 100 results of a search on google scholar using R. Does anyone know how to do it? To be precise, I just need the name of the paper, authors and citation count.

Thanks in advance,
Manoel

ps.: It is legal to do that, right?

",2
3615718,09/01/2010 07:06:34,170792,09/09/2009 11:47:11,6124,169,Bootstrapping to compare two groups,"In the following code I use bootstrapping to calculate the C.I. and the p-value under the null hypothesis that two different fertilizers applied to tomato plants have no effect in plants yields (and the alternative being that the ""improved"" fertilizer is better). The first random sample (x) comes from plants where a standard fertilizer has been used, while an ""improved"" one has been used in the plants where the second sample (y) comes from.

    x <- c(11.4,25.3,29.9,16.5,21.1)
    y <- c(23.7,26.6,28.5,14.2,17.9,24.3)
    total <- c(x,y)
    library(boot)
    diff <- function(x,i) mean(x[i[6:11]]) - mean(x[i[1:5]])
    b <- boot(total, diff, R = 10000)

    ci <- boot.ci(b)
    p.value <- sum(b$t>=b$t0)/b$R

What I don't like about the code above is that resampling is done as if there was only one sample of 11 values (separating the first 5 as belonging to sample x leaving the rest to sample y).
Could you show me how this code should be modified in order to draw resamples of size 5 with replacement from the first sample and separate resamples of size 6 from the second sample, so that bootstrap resampling would mimic the “separate samples” design that produced the original data?",r,bootstrapping,,,,,open,0,212,5,"Bootstrapping to compare two groups In the following code I use bootstrapping to calculate the C.I. and the p-value under the null hypothesis that two different fertilizers applied to tomato plants have no effect in plants yields (and the alternative being that the ""improved"" fertilizer is better). The first random sample (x) comes from plants where a standard fertilizer has been used, while an ""improved"" one has been used in the plants where the second sample (y) comes from.

    x <- c(11.4,25.3,29.9,16.5,21.1)
    y <- c(23.7,26.6,28.5,14.2,17.9,24.3)
    total <- c(x,y)
    library(boot)
    diff <- function(x,i) mean(x[i[6:11]]) - mean(x[i[1:5]])
    b <- boot(total, diff, R = 10000)

    ci <- boot.ci(b)
    p.value <- sum(b$t>=b$t0)/b$R

What I don't like about the code above is that resampling is done as if there was only one sample of 11 values (separating the first 5 as belonging to sample x leaving the rest to sample y).
Could you show me how this code should be modified in order to draw resamples of size 5 with replacement from the first sample and separate resamples of size 6 from the second sample, so that bootstrap resampling would mimic the “separate samples” design that produced the original data?",2
2349205,02/27/2010 22:38:35,157872,08/17/2009 16:25:55,134,9,"Can't draw Histogram,  'x' must be numeric","I have a data file with this format:

Weight 	  Industry Type
251,787	  Kellogg  h
253,9601  Kellogg  a
256,0758  Kellogg  h
....

I read the data and try to draw an histogram with this commands:

     ce= read.table(""file.txt"", header= T)
    
     we = ce[,1]
     in = ce[,2]
     ty = ce[,3]

    hist(we)

But I get this error:
Error en hist.default(we) : 'x' must be numeric. What I need to do in order to draw histograms for my three variables ?",r,,,,,,open,0,99,8,"Can't draw Histogram,  'x' must be numeric I have a data file with this format:

Weight 	  Industry Type
251,787	  Kellogg  h
253,9601  Kellogg  a
256,0758  Kellogg  h
....

I read the data and try to draw an histogram with this commands:

     ce= read.table(""file.txt"", header= T)
    
     we = ce[,1]
     in = ce[,2]
     ty = ce[,3]

    hist(we)

But I get this error:
Error en hist.default(we) : 'x' must be numeric. What I need to do in order to draw histograms for my three variables ?",1
9558040,03/04/2012 19:22:54,633739,02/25/2011 07:41:48,45,2,ggplot map with l,"I want to plot a world map using ggplot2 (v.9) which combines two pieces if information.  The following example illustrates:

    library(rgdal)
    library(ggplot2)

    gpclibPermit()
    # From http://thematicmapping.org/downloads/world_borders.php:
    w <- readOGR(dsn=""data"", layer=""TM_WORLD_BORDERS_SIMPL-0.3"")
    world.ggmap <- fortify(world.map, region = ""NAME"")

    n <- length(unique(world.ggmap$id))
    df <- data.frame(id = unique(world.ggmap$id),
                     growth = 4*runif(n),
                     category = factor(sample(1:5, n, replace=T)))

    ## noise
    df[c(sample(1:100,40)),c(""growth"", ""category"")] <- NA


    ggplot(df, aes(map_id = id +
         geom_map(aes(fill = growth, color = category), map =world.ggmap) +
         expand_limits(x = world.ggmap$long, y = world.ggmap$lat) +
         scale_fill_gradient(low = ""red"", high = ""blue"", guide = ""colorbar"")


However, this solution is not a nice way to display both `growth` and `category`.  `Growth` is highly visible, but `category` is almost impossible to see because it is just a border. 

I have tried to increase the size of borders, but without luck (the new geom_map is hard to work with).  Does anyone knows how to increase border size in the above example, *or* even better, a mechanism to display two factors?

A bonus question: country names, such as those used by the maps package (which features USSR!) are the data used in the example is fragile.  I prefer to use ISO 3166-1 alpha-3([1]).  Does anyone know data readily usable with ggplot2 which features ISO-... country names

[1]: https://en.wikipedia.org/wiki/ISO_3166-1_alpha-3

Result:

![result](http://ompldr.org/vY3hsYQ)",r,ggplot2,geospatial,,,,open,0,306,4,"ggplot map with l I want to plot a world map using ggplot2 (v.9) which combines two pieces if information.  The following example illustrates:

    library(rgdal)
    library(ggplot2)

    gpclibPermit()
    # From http://thematicmapping.org/downloads/world_borders.php:
    w <- readOGR(dsn=""data"", layer=""TM_WORLD_BORDERS_SIMPL-0.3"")
    world.ggmap <- fortify(world.map, region = ""NAME"")

    n <- length(unique(world.ggmap$id))
    df <- data.frame(id = unique(world.ggmap$id),
                     growth = 4*runif(n),
                     category = factor(sample(1:5, n, replace=T)))

    ## noise
    df[c(sample(1:100,40)),c(""growth"", ""category"")] <- NA


    ggplot(df, aes(map_id = id +
         geom_map(aes(fill = growth, color = category), map =world.ggmap) +
         expand_limits(x = world.ggmap$long, y = world.ggmap$lat) +
         scale_fill_gradient(low = ""red"", high = ""blue"", guide = ""colorbar"")


However, this solution is not a nice way to display both `growth` and `category`.  `Growth` is highly visible, but `category` is almost impossible to see because it is just a border. 

I have tried to increase the size of borders, but without luck (the new geom_map is hard to work with).  Does anyone knows how to increase border size in the above example, *or* even better, a mechanism to display two factors?

A bonus question: country names, such as those used by the maps package (which features USSR!) are the data used in the example is fragile.  I prefer to use ISO 3166-1 alpha-3([1]).  Does anyone know data readily usable with ggplot2 which features ISO-... country names

[1]: https://en.wikipedia.org/wiki/ISO_3166-1_alpha-3

Result:

![result](http://ompldr.org/vY3hsYQ)",3
10246128,04/20/2012 12:19:37,1233913,02/26/2012 14:24:43,11,0,Error with GSE data set?,"I must start by saying that I'm just beginning to R programing. I'm unable to create expressionset of my data downloaded from GEO.  I get an error:

Error in validObject(.Object) : "" invalid class """"ExpressionSet"""" object: sampleNames differ between assayData and phenoData""


For example I have data set from GSE6887.

Please take a look at the sample data, the phenodata table I made and R-program. I guess that the phenodata should be modified to get this working.

Thanks in advance",r,error-handling,,,,04/21/2012 00:51:55,not a real question,1,77,5,"Error with GSE data set? I must start by saying that I'm just beginning to R programing. I'm unable to create expressionset of my data downloaded from GEO.  I get an error:

Error in validObject(.Object) : "" invalid class """"ExpressionSet"""" object: sampleNames differ between assayData and phenoData""


For example I have data set from GSE6887.

Please take a look at the sample data, the phenodata table I made and R-program. I guess that the phenodata should be modified to get this working.

Thanks in advance",2
10554741,05/11/2012 15:53:33,466857,10/05/2010 13:17:47,17,0,R - Fill in data frame with values from rows above,"Say I have a data frame like this:

    ID,  ID_2, FIRST, VALUE
    -----------------------
    'a', 'aa', TRUE, 2
    'a', 'ab', FALSE, NA
    'a', 'ac', FALSE, NA
    'b', 'aa', TRUE, 5
    'b', 'ab', FALSE, NA

So VALUE is only set for FIRST = TRUE once per ID. ID_2 may be duplicate between IDs, but doesn't have to.

How do I put the numbers from the first rows of each ID into all rows of that ID, such that the VALUE column becomes 2, 2, 2, 5, 5?

I know I could simply loop over all IDs with a for loop, but I am looking for a more efficient way.",r,data.frame,,,,,open,0,126,11,"R - Fill in data frame with values from rows above Say I have a data frame like this:

    ID,  ID_2, FIRST, VALUE
    -----------------------
    'a', 'aa', TRUE, 2
    'a', 'ab', FALSE, NA
    'a', 'ac', FALSE, NA
    'b', 'aa', TRUE, 5
    'b', 'ab', FALSE, NA

So VALUE is only set for FIRST = TRUE once per ID. ID_2 may be duplicate between IDs, but doesn't have to.

How do I put the numbers from the first rows of each ID into all rows of that ID, such that the VALUE column becomes 2, 2, 2, 5, 5?

I know I could simply loop over all IDs with a for loop, but I am looking for a more efficient way.",2
8159173,11/16/2011 21:45:16,1050538,11/16/2011 21:19:36,1,0,Generate a vector graphics format of an R Graph,"I have an R script that generates a graph. I need it to be saved in a vector format (to retain the quality when the image is re-sized). PDF will not be very useful since it is not convenient to manipulate. And unfortunately, the postscript device isnt working for me. Please suggest an alternative format or a fix for the code snippet to generate an eps. 

    postscript(""fig1.eps"")
    #pdf(""fig1.pdf"")
    x <- seq(-pi, pi, len = 65)
    plot(x, sin(x), type = ""l"", ylim = c(-1.2, 1.8), col = 3)
    lines(x, cos(x), pch = 3, col = 4)
    lines(x, tan(x), pch = 4, col = 6)
    title(""legend(..., lty = c(2, -1, 1), pch = c(-1,3,4), merge = TRUE)"", cex.main = 1.1)
    legend(2, 1.9, c(""sin"", ""cos"", ""tan""), col = c(3,4,6), lty = c(1, 1, 1))
    dev.off()
",r,graphics,postscript,s,,11/17/2011 13:54:50,not a real question,1,160,9,"Generate a vector graphics format of an R Graph I have an R script that generates a graph. I need it to be saved in a vector format (to retain the quality when the image is re-sized). PDF will not be very useful since it is not convenient to manipulate. And unfortunately, the postscript device isnt working for me. Please suggest an alternative format or a fix for the code snippet to generate an eps. 

    postscript(""fig1.eps"")
    #pdf(""fig1.pdf"")
    x <- seq(-pi, pi, len = 65)
    plot(x, sin(x), type = ""l"", ylim = c(-1.2, 1.8), col = 3)
    lines(x, cos(x), pch = 3, col = 4)
    lines(x, tan(x), pch = 4, col = 6)
    title(""legend(..., lty = c(2, -1, 1), pch = c(-1,3,4), merge = TRUE)"", cex.main = 1.1)
    legend(2, 1.9, c(""sin"", ""cos"", ""tan""), col = c(3,4,6), lty = c(1, 1, 1))
    dev.off()
",4
8302399,11/28/2011 21:15:03,356790,06/02/2010 18:59:59,2319,36,Call overridden s3 method from subclass (R.methodsS3),"I'm using `setMethodS3` in package R.methodsS3 to create an S3 method.  Lets say I have two classes, `class Parent` and `class Child`.  `class Child` inherits from `class Parent`.  Both have the method `MyMethod()`.  How do I call superclass `MyMethod()` (Parent's `MyMethod`) from Child's `MyMethod()`?  I tried this$MyMethod(), but it calls Child's `MyMethod()`",r,r.oo,,,,,open,0,57,7,"Call overridden s3 method from subclass (R.methodsS3) I'm using `setMethodS3` in package R.methodsS3 to create an S3 method.  Lets say I have two classes, `class Parent` and `class Child`.  `class Child` inherits from `class Parent`.  Both have the method `MyMethod()`.  How do I call superclass `MyMethod()` (Parent's `MyMethod`) from Child's `MyMethod()`?  I tried this$MyMethod(), but it calls Child's `MyMethod()`",2
11579913,07/20/2012 13:00:42,1083757,12/06/2011 15:08:46,40,0,Transcription Factors annotation,"I have a list of genes and I would like to retrieve informations about them, for example if a gene is a transcription factor or not. No other informations I need. Is there a tool (for example an R library) to download computationally the informations i need? I have thousand of genes...

Thanks in advance

Eleonora",r,,,,,07/20/2012 15:45:01,off topic,1,54,3,"Transcription Factors annotation I have a list of genes and I would like to retrieve informations about them, for example if a gene is a transcription factor or not. No other informations I need. Is there a tool (for example an R library) to download computationally the informations i need? I have thousand of genes...

Thanks in advance

Eleonora",1
9480855,02/28/2012 11:14:11,900889,08/18/2011 15:18:34,18,0,R: different result with for-loop and apply ? (paired t-test),"I am using R to run a large number of 2-group t-tests, using input from a delimited table. Following recommendations from here and elsewhere, I tried either 'for-loops' and 'apply' to accomplish that. For 'normal' t.test, both work nicely and give the same results. However, for a paired t-test, the for-look works while the apply-loop does not. Would some expert be to kind to explain what is going on?

My input file looks like this: (the first line is a header line, the data lines have a name, 4 datapoints for group 1 and 4 datapoints for group 2):

    header g1.1 g1.2 g1.3 g1.4 g2.1 g2.2 g2.3 g2.4
    name1  10.1 10.9 10.5 10.4 11.1 12.1 11.9 12.0
    name2  23.2 24.4 24.5 27.2 15.5 16.5 17.7 20.0
    name3  .....
        and so on (overall ~50000 lines)
    
This is the for-loop that works nicely:

    table <- read.table('ttest_in.txt',head=1,sep='\t')
    for(i in 1:nrow(table)) {
       g1<-as.numeric((table)[i,2:5])
       g2<-as.numeric((table)[i,6:9])
       pv <- t.test(g1,g2,paired=TRUE)$p.value
    }

This is the 'apply' version that causes problems

    table <- read.table('ttest_in.txt',head=1,sep='\t')
    mat <- as.matrix(table[,2:9])
    pv.list <- apply(mat,1,function(x){t.test(x[1:4],x[5:8],paired=TRUE)$p.value})

Here is what happens: for some short datasets, both procedures work nicely. However, for a big dataset (which might contain missing values), the for-loop version works fine while the apply version crashes with the message that some 'data are essentially constant'. Obviously, it is not a good idea to crash the entire script just because the t.test doesn't like one piece of data. However, I am really puzzled why the for-loop appears to work on the same data. Being an R newbie, I am not even sure which of the data lines is the one causing the problem - I could find out in the for-loop version, but here there is no error. The 'apply' loop appears like a black box to me.

Any ideas ?



",r,,,,,,open,0,354,10,"R: different result with for-loop and apply ? (paired t-test) I am using R to run a large number of 2-group t-tests, using input from a delimited table. Following recommendations from here and elsewhere, I tried either 'for-loops' and 'apply' to accomplish that. For 'normal' t.test, both work nicely and give the same results. However, for a paired t-test, the for-look works while the apply-loop does not. Would some expert be to kind to explain what is going on?

My input file looks like this: (the first line is a header line, the data lines have a name, 4 datapoints for group 1 and 4 datapoints for group 2):

    header g1.1 g1.2 g1.3 g1.4 g2.1 g2.2 g2.3 g2.4
    name1  10.1 10.9 10.5 10.4 11.1 12.1 11.9 12.0
    name2  23.2 24.4 24.5 27.2 15.5 16.5 17.7 20.0
    name3  .....
        and so on (overall ~50000 lines)
    
This is the for-loop that works nicely:

    table <- read.table('ttest_in.txt',head=1,sep='\t')
    for(i in 1:nrow(table)) {
       g1<-as.numeric((table)[i,2:5])
       g2<-as.numeric((table)[i,6:9])
       pv <- t.test(g1,g2,paired=TRUE)$p.value
    }

This is the 'apply' version that causes problems

    table <- read.table('ttest_in.txt',head=1,sep='\t')
    mat <- as.matrix(table[,2:9])
    pv.list <- apply(mat,1,function(x){t.test(x[1:4],x[5:8],paired=TRUE)$p.value})

Here is what happens: for some short datasets, both procedures work nicely. However, for a big dataset (which might contain missing values), the for-loop version works fine while the apply version crashes with the message that some 'data are essentially constant'. Obviously, it is not a good idea to crash the entire script just because the t.test doesn't like one piece of data. However, I am really puzzled why the for-loop appears to work on the same data. Being an R newbie, I am not even sure which of the data lines is the one causing the problem - I could find out in the for-loop version, but here there is no error. The 'apply' loop appears like a black box to me.

Any ideas ?



",1
8152603,11/16/2011 13:41:44,1049760,11/16/2011 13:34:49,1,0,Interpolating hourly basis,"I have a huge data set in the form of 

               V1           V2    V3     V4 V5 V6
1      201005010000 201005010000  1.68 291.38  1  0
2      201005010000 201005010300  0.93 335.10  1  0
3      201005010000 201005010600  2.25  57.38  1  0
4      201005010000 201005010900  0.43  13.76  1  0
5      201005010000 201005011200  0.74 101.14  1  0

I am interested in interpolating it on an hour basis(it's for avery 3 hours). Data is also being updated after every six hours for next eight days. 
Thanks in advance.

",r,,,,,11/16/2011 14:40:48,not a real question,1,149,3,"Interpolating hourly basis I have a huge data set in the form of 

               V1           V2    V3     V4 V5 V6
1      201005010000 201005010000  1.68 291.38  1  0
2      201005010000 201005010300  0.93 335.10  1  0
3      201005010000 201005010600  2.25  57.38  1  0
4      201005010000 201005010900  0.43  13.76  1  0
5      201005010000 201005011200  0.74 101.14  1  0

I am interested in interpolating it on an hour basis(it's for avery 3 hours). Data is also being updated after every six hours for next eight days. 
Thanks in advance.

",1
8397015,12/06/2011 08:22:52,636467,02/27/2011 13:24:06,58,4,How do I decide which work should be done by R and which work should be done by Java or C?,"I am familiar with programming language such as Java and C and I have some work about machine learning for which I start to learn R. Then my question is how do I decide which work should be done by R and which work should be done by Java or C?

For example, I am trying to visualize a data set by graph in which edges with different edges with different properties have different colors. To accomplish this job, firstly, I use java to build the graph upon the data set and write it to a GML file, then I use R read the file and draw the graph. Is this process reasonable? Should I do all the work with R and why?",r,graph,,,,12/06/2011 12:35:10,not constructive,1,122,21,"How do I decide which work should be done by R and which work should be done by Java or C? I am familiar with programming language such as Java and C and I have some work about machine learning for which I start to learn R. Then my question is how do I decide which work should be done by R and which work should be done by Java or C?

For example, I am trying to visualize a data set by graph in which edges with different edges with different properties have different colors. To accomplish this job, firstly, I use java to build the graph upon the data set and write it to a GML file, then I use R read the file and draw the graph. Is this process reasonable? Should I do all the work with R and why?",2
6581413,07/05/2011 10:45:00,810014,06/22/2011 09:01:54,3,0,cluster analysis of protein sequences using R?,How can I do cluster analysis of protein sequences using R progamming? I am a beginner in R programming language. I don't know anything about it .Please help me to write scripts for this in windows and please suggest good books for R programming.,r,,,,,07/05/2011 11:29:58,off topic,1,44,7,cluster analysis of protein sequences using R? How can I do cluster analysis of protein sequences using R progamming? I am a beginner in R programming language. I don't know anything about it .Please help me to write scripts for this in windows and please suggest good books for R programming.,1
9889722,03/27/2012 12:42:20,876274,08/03/2011 09:17:07,98,0,removing thousand seprator in R,"    Recently I have very strange problem.I used to write.table to the manipulated data into a tab delim text file.After the coman executed and when I opened the text file,every value iconverted in thousand seperator which was not when not when I saw in R.

For an example,in R i get the following values

     159.234789
     160.678151 
     154.878034
     456.126789

After I used write.table,these values are 

     159.234.789
     160.678.151 
     154.878.034
     456.126.789

I dont know what is happening..Could anyone can help me please.

",r,,,,,03/27/2012 15:50:23,too localized,1,114,5,"removing thousand seprator in R     Recently I have very strange problem.I used to write.table to the manipulated data into a tab delim text file.After the coman executed and when I opened the text file,every value iconverted in thousand seperator which was not when not when I saw in R.

For an example,in R i get the following values

     159.234789
     160.678151 
     154.878034
     456.126789

After I used write.table,these values are 

     159.234.789
     160.678.151 
     154.878.034
     456.126.789

I dont know what is happening..Could anyone can help me please.

",1
5285178,03/12/2011 20:11:34,552642,12/23/2010 17:13:18,59,0,R plot implicit function outer command,"I would like to plot an implicit function of x and y:  1-0.125*y^2-x^2=0.005
I know it can be plotted as a contour plot but have trouble with the ""outer"" command 
in the following:
  
   `x<-seq(0.4,1.01,length=1000)  
    y<-seq(0,3,length=1000)  
    z<-outer(x,y,FUN=""1-0.125*y^2-x^2=0.005"")  
    contour(x,y,z,levels=0,drawpoints=FALSE)`

I read the FAQ (7.17) regarding the ""outer"" command and the need to vectorize the function but am still in a quandry.  

Any help would be greatly appreciated.
Thanks,
Carey



",r,,,,,,open,0,84,6,"R plot implicit function outer command I would like to plot an implicit function of x and y:  1-0.125*y^2-x^2=0.005
I know it can be plotted as a contour plot but have trouble with the ""outer"" command 
in the following:
  
   `x<-seq(0.4,1.01,length=1000)  
    y<-seq(0,3,length=1000)  
    z<-outer(x,y,FUN=""1-0.125*y^2-x^2=0.005"")  
    contour(x,y,z,levels=0,drawpoints=FALSE)`

I read the FAQ (7.17) regarding the ""outer"" command and the need to vectorize the function but am still in a quandry.  

Any help would be greatly appreciated.
Thanks,
Carey



",1
9298653,02/15/2012 17:57:18,1148098,01/13/2012 16:35:24,2,1,unable to use mod_R,"I am trying to use rapache. I have this line in httpd.conf file
    
     LoadModule R_module modules/mod_R.so
when I start restart httpd, I see this error in error_log
     Fatal error: unable to open the base package

I did install R, rapache and libapreq2. I did also try to put the R library in ld.so.conf, it did not work.",r,,,,,02/15/2012 22:44:30,off topic,1,67,4,"unable to use mod_R I am trying to use rapache. I have this line in httpd.conf file
    
     LoadModule R_module modules/mod_R.so
when I start restart httpd, I see this error in error_log
     Fatal error: unable to open the base package

I did install R, rapache and libapreq2. I did also try to put the R library in ld.so.conf, it did not work.",1
11394376,07/09/2012 11:45:04,1171168,01/26/2012 11:34:32,18,0,Maximum Likelihood Estimation of Inverse Gamma Distribution in R or RPy,"I am trying to fit a three parameter inverse gamma distribution to my data in either R or Python. I would like to do this using maximum likelihood estimation (MLE). 

The pdf of the three parameter inverse gamma is given by:

![enter image description here][1]

*Where Γ is the gamma function, ρ is the shape, α is the scale and s is the location parameter*
 
I haven't spotted an R package that can perform MLE to this distribution directly (if you know of one, please let me know!). So I think this leaves either:

- *(A) working out the log-likelihood function of the formula* 
- *(B) transforming the data to a gamma distribution. However, this distribution only has two parameters so I'm not clear on how I would calculate the third parameter (I'm not a very mathematical person!).*

Any help on a method to use MLE to fit an inverse gamma distribution to my data would be much appreciated! Many thanks in advance. 


  [1]: http://i.stack.imgur.com/nOrUa.jpg",r,pdf,curve-fitting,rpy2,gamma-distribution,07/09/2012 15:33:58,off topic,1,163,11,"Maximum Likelihood Estimation of Inverse Gamma Distribution in R or RPy I am trying to fit a three parameter inverse gamma distribution to my data in either R or Python. I would like to do this using maximum likelihood estimation (MLE). 

The pdf of the three parameter inverse gamma is given by:

![enter image description here][1]

*Where Γ is the gamma function, ρ is the shape, α is the scale and s is the location parameter*
 
I haven't spotted an R package that can perform MLE to this distribution directly (if you know of one, please let me know!). So I think this leaves either:

- *(A) working out the log-likelihood function of the formula* 
- *(B) transforming the data to a gamma distribution. However, this distribution only has two parameters so I'm not clear on how I would calculate the third parameter (I'm not a very mathematical person!).*

Any help on a method to use MLE to fit an inverse gamma distribution to my data would be much appreciated! Many thanks in advance. 


  [1]: http://i.stack.imgur.com/nOrUa.jpg",5
10288235,04/23/2012 21:00:20,843419,07/13/2011 19:47:52,8,1,apsrtable for sarlm class,"I love `apsrtable()`, and have found it somewhat simple to extend to other classes (in particular, I have adapted it for `mlogit` objects. But for some reason, the `apsrtableSummary.sarlm()` function doesn't work quite like other hacks I have written.

Typically, we need to redefine the coefficients matrix so that `apsrtable()` knows where to find it. The code for this is 

    ""apsrtableSummary.sarlm"" <- function (x){
      s <- summary(x)
      s$coefficients <- s$Coef
      return(s)
    }

We also need to redefine the `modelInfo` for the new class, like this:

    setMethod(""modelInfo"", ""summary.sarlm"", function(x){
      env <- sys.parent()
      digits <- evalq(digits, envir=env)
      model.info <- list(
        ""$\\rho$"" = formatC(x$rho, format=""f"", digits=digits),
        ""$p(\\rho)$"" = formatC(x$LR1$p.value, format=""f"", digits=digits),
        ""$N$"" = length(x$fitted.values),
        ""AIC"" = formatC(AIC(x), format=""f"", digits=digits),
        ""\\mathcal{L}"" = formatC(x$LL, format=""f"", digits=digits)
      )
      class(model.info) <- ""model.info""
      return(model.info)
    })

After defining these two functions however, a call to `apsrtable()` doesn't print the coefficients (MWE using example from `lagsarlm` in `spdep` package).

    library(spdep)
    library(apsrtable)
    data(oldcol)
    COL.lag.eig <- lagsarlm(CRIME ~ INC + HOVAL, data=COL.OLD,
                             nb2listw(COL.nb, style=""W""), method=""eigen"")
    summary(COL.lag.eig)
    # Load functions above
    apsrtable(COL.lag.eig)
   
    ## OUTPUT ##
    \begin{table}[!ht]
    \caption{}
    \label{} 
    \begin{tabular}{ l D{.}{.}{2} } 
    \hline 
      & \multicolumn{ 1 }{ c }{ Model 1 } \\ \hline
     %                           & Model 1 \\
     $\rho$.rho                 & 0.43   \\ 
    $p(\rho)$.Likelihood ratio & 0.00   \\ 
    $N$                         & 49     \\ 
    AIC                         & 374.78 \\ 
    \mathcal{L}                & -182.39 \\ \hline
     \multicolumn{2}{l}{\footnotesize{Standard errors in parentheses}}\\
    \multicolumn{2}{l}{\footnotesize{$^*$ indicates significance at $p< 0.05 $}} 
    \end{tabular} 
     \end{table}

 
As you can see, everything works out great except for that the coefficients and standard errors are not there. It's clear that the summary redefinition works, because

    apsrtableSummary(COL.lag.eig)$coefficients
                  Estimate Std. Error   z value     Pr(>|z|)
    (Intercept) 45.0792505 7.17734654  6.280768 3.369041e-10
    INC         -1.0316157 0.30514297 -3.380762 7.228517e-04
    HOVAL       -0.2659263 0.08849862 -3.004863 2.657002e-03

 I've pulled my hair out for several days trying to find a way out of this. Any tips?",r,latex,xtable,,,,open,0,669,4,"apsrtable for sarlm class I love `apsrtable()`, and have found it somewhat simple to extend to other classes (in particular, I have adapted it for `mlogit` objects. But for some reason, the `apsrtableSummary.sarlm()` function doesn't work quite like other hacks I have written.

Typically, we need to redefine the coefficients matrix so that `apsrtable()` knows where to find it. The code for this is 

    ""apsrtableSummary.sarlm"" <- function (x){
      s <- summary(x)
      s$coefficients <- s$Coef
      return(s)
    }

We also need to redefine the `modelInfo` for the new class, like this:

    setMethod(""modelInfo"", ""summary.sarlm"", function(x){
      env <- sys.parent()
      digits <- evalq(digits, envir=env)
      model.info <- list(
        ""$\\rho$"" = formatC(x$rho, format=""f"", digits=digits),
        ""$p(\\rho)$"" = formatC(x$LR1$p.value, format=""f"", digits=digits),
        ""$N$"" = length(x$fitted.values),
        ""AIC"" = formatC(AIC(x), format=""f"", digits=digits),
        ""\\mathcal{L}"" = formatC(x$LL, format=""f"", digits=digits)
      )
      class(model.info) <- ""model.info""
      return(model.info)
    })

After defining these two functions however, a call to `apsrtable()` doesn't print the coefficients (MWE using example from `lagsarlm` in `spdep` package).

    library(spdep)
    library(apsrtable)
    data(oldcol)
    COL.lag.eig <- lagsarlm(CRIME ~ INC + HOVAL, data=COL.OLD,
                             nb2listw(COL.nb, style=""W""), method=""eigen"")
    summary(COL.lag.eig)
    # Load functions above
    apsrtable(COL.lag.eig)
   
    ## OUTPUT ##
    \begin{table}[!ht]
    \caption{}
    \label{} 
    \begin{tabular}{ l D{.}{.}{2} } 
    \hline 
      & \multicolumn{ 1 }{ c }{ Model 1 } \\ \hline
     %                           & Model 1 \\
     $\rho$.rho                 & 0.43   \\ 
    $p(\rho)$.Likelihood ratio & 0.00   \\ 
    $N$                         & 49     \\ 
    AIC                         & 374.78 \\ 
    \mathcal{L}                & -182.39 \\ \hline
     \multicolumn{2}{l}{\footnotesize{Standard errors in parentheses}}\\
    \multicolumn{2}{l}{\footnotesize{$^*$ indicates significance at $p< 0.05 $}} 
    \end{tabular} 
     \end{table}

 
As you can see, everything works out great except for that the coefficients and standard errors are not there. It's clear that the summary redefinition works, because

    apsrtableSummary(COL.lag.eig)$coefficients
                  Estimate Std. Error   z value     Pr(>|z|)
    (Intercept) 45.0792505 7.17734654  6.280768 3.369041e-10
    INC         -1.0316157 0.30514297 -3.380762 7.228517e-04
    HOVAL       -0.2659263 0.08849862 -3.004863 2.657002e-03

 I've pulled my hair out for several days trying to find a way out of this. Any tips?",3
7398898,09/13/2011 08:30:31,941999,09/13/2011 07:14:50,1,0,R quantmod and xts code help,"I need help with the following code:

    library(xts)
    library(quantmod)
    
    # quickly re-source this file
    s <- function() source('meanrev.R')
    
    checkPair <- function(sym1, sym2, dateFilter='::')
    {
      t.xts <- getCombined(sym1, sym2, dateFilter=dateFilter)
    
      cat(""Date range is"", format(start(t.xts)), ""to"", format(end(t.xts)), ""\n"")

I get stuck here:


      # Build linear model
      m <- buildLM(t.xts)
    
      # Note beta -- http://en.wikipedia.org/wiki/Beta_(finance)
      beta <- getBeta(m)
      cat(""Assumed hedge ratio is"", beta, ""\n"")
    
      # Build spread
      sprd <- buildSpread(t.xts, beta)
    
      # Test cointegration
      ht <- testCoint(sprd)
      cat(""PP p-value is"", as.double(ht$p.value), ""\n"")
    
      if (as.double(ht$p.value) < 0.05)
      {
        cat(""The spread is likely mean-reverting.\n"")
      }
      else
      {
        cat(""The spread is not mean-reverting.\n"")
      }
    }
    
    getCombined <- function(sym1, sym2, dateFilter='::')
    {
      # Grab historical data for both symbols
      one <- getSymbols(sym1, auto.assign=FALSE)
      two <- getSymbols(sym2, auto.assign=FALSE)
    
      # Give columns more usable names
      colnames(one) <- c('Open', 'High', 'Low', 'Close', 'Volume', 'Adjusted')
      colnames(two) <- c('Open', 'High', 'Low', 'Close', 'Volume', 'Adjusted')
    
      # Build combined object
      return(merge(one$Close, two$Close, all=FALSE)[dateFilter])
    }
    
    buildLM <- function(combined)
    {
      return(lm(Close ~ Close.1 + 0, combined))
    }
    
    getBeta <- function(m)
    {
      return(as.double(coef(m)[1]))
    }
    
    buildSpread <- function(combined, beta)
    {
      return(combined$Close - beta*combined$Close.1)
    }
    
    testCoint <- function(sprd)
    {
      return(PP.test(sprd, lshort = FALSE))
    }


This is for a school project. Any help would be great. Thank you. ",r,xts,quantmod,,,10/30/2011 15:52:14,not a real question,1,483,6,"R quantmod and xts code help I need help with the following code:

    library(xts)
    library(quantmod)
    
    # quickly re-source this file
    s <- function() source('meanrev.R')
    
    checkPair <- function(sym1, sym2, dateFilter='::')
    {
      t.xts <- getCombined(sym1, sym2, dateFilter=dateFilter)
    
      cat(""Date range is"", format(start(t.xts)), ""to"", format(end(t.xts)), ""\n"")

I get stuck here:


      # Build linear model
      m <- buildLM(t.xts)
    
      # Note beta -- http://en.wikipedia.org/wiki/Beta_(finance)
      beta <- getBeta(m)
      cat(""Assumed hedge ratio is"", beta, ""\n"")
    
      # Build spread
      sprd <- buildSpread(t.xts, beta)
    
      # Test cointegration
      ht <- testCoint(sprd)
      cat(""PP p-value is"", as.double(ht$p.value), ""\n"")
    
      if (as.double(ht$p.value) < 0.05)
      {
        cat(""The spread is likely mean-reverting.\n"")
      }
      else
      {
        cat(""The spread is not mean-reverting.\n"")
      }
    }
    
    getCombined <- function(sym1, sym2, dateFilter='::')
    {
      # Grab historical data for both symbols
      one <- getSymbols(sym1, auto.assign=FALSE)
      two <- getSymbols(sym2, auto.assign=FALSE)
    
      # Give columns more usable names
      colnames(one) <- c('Open', 'High', 'Low', 'Close', 'Volume', 'Adjusted')
      colnames(two) <- c('Open', 'High', 'Low', 'Close', 'Volume', 'Adjusted')
    
      # Build combined object
      return(merge(one$Close, two$Close, all=FALSE)[dateFilter])
    }
    
    buildLM <- function(combined)
    {
      return(lm(Close ~ Close.1 + 0, combined))
    }
    
    getBeta <- function(m)
    {
      return(as.double(coef(m)[1]))
    }
    
    buildSpread <- function(combined, beta)
    {
      return(combined$Close - beta*combined$Close.1)
    }
    
    testCoint <- function(sprd)
    {
      return(PP.test(sprd, lshort = FALSE))
    }


This is for a school project. Any help would be great. Thank you. ",3
11738409,07/31/2012 10:49:24,1565339,07/31/2012 09:55:01,1,0,Multiple definition error building RInside samples on Windows 7 x64,"I'm trying to build rinside_sample1.cpp from RInside\examples\standard directory on Windows 7 x64 using g++ from RTools in the following way:

    set RCPP=%R_HOME%\library\Rcpp
    set RINSIDE=%R_HOME%\library\RInside
    g++ -c -m64 rinside_sample1.cpp -I %RINSIDE%\include -I %RCPP%\include -I %R_HOME%\include
    g++ -m64 rinside_sample1.o -o rinside_sample1.exe -L %RINSIDE%\libs\x64 -l RInside -L %RCPP%\libs\x64 -l Rcpp -L %R_HOME%\bin\x64 -l R

The linkage results into the multiple definition error:

    d000026.o:(.idata$5+0x0): multiple definition of `__imp__ZTVN4Rcpp7RObjectE'
    d000019.o:(.idata$5+0x0): first defined here
    d000026.o:(.idata$6+0x0): multiple definition of `__nm__ZTVN4Rcpp7RObjectE'
    d000019.o:(.idata$6+0x0): first defined here
    collect2: ld returned 1 exit status

**However the similar building process for rinside_sample0.cpp succeedes. Does anyone have an idea of the solution?**

It seems that the problem with rinside_sample1.cpp arises because of Rcpp::NumericMatrix and Rcpp::NumericVector usage.

rinside_sample0.cpp code:

    // -*- mode: C++; c-indent-level: 4; c-basic-offset: 4;  tab-width: 8; -*-
    //
    // Simple example showing how to do the standard 'hello, world' using embedded R
    //
    // Copyright (C) 2009 Dirk Eddelbuettel 
    // Copyright (C) 2010 Dirk Eddelbuettel and Romain Francois
    //
    // GPL'ed 
    
    #include <RInside.h>                    // for the embedded R via RInside
    
    int main(int argc, char *argv[]) {
    
        RInside R(argc, argv);              // create an embedded R instance 
    
        R[""txt""] = ""Hello, world!\n"";	// assign a char* (string) to 'txt'
    
        R.parseEvalQ(""cat(txt)"");           // eval the init string, ignoring any returns
    
        exit(0);
    }

rinside_sample1.cpp code:

    // -*- mode: C++; c-indent-level: 4; c-basic-offset: 4; tab-width: 8; -*-
    //
    // Simple example with data in C++ that is passed to R, processed and a result is extracted
    //
    // Copyright (C) 2009         Dirk Eddelbuettel 
    // Copyright (C) 2010 - 2011  Dirk Eddelbuettel and Romain Francois
    //
    // GPL'ed 
    
    #include <RInside.h>                            // for the embedded R via RInside
    
    Rcpp::NumericMatrix createMatrix(const int n) {
        Rcpp::NumericMatrix M(n,n);
        for (int i=0; i<n; i++) {
            for (int j=0; j<n; j++) {
                M(i,j) = i*10 + j; 
            }
        }
        return(M);
    }
    
    int main(int argc, char *argv[]) {
    
        RInside R(argc, argv);                      // create an embedded R instance 
        
        const int mdim = 4;                         // let the matrices be 4 by 4; create, fill 
        R[""M""] = createMatrix(mdim);                // then assign data Matrix to R's 'M' var
    
        std::string str = 
            ""cat('Running ls()\n'); print(ls()); ""
            ""cat('Showing M\n'); print(M); ""
            ""cat('Showing colSums()\n'); Z <- colSums(M); print(Z); ""
            ""Z"";                     // returns Z
    
        
        Rcpp::NumericVector v = R.parseEval(str);   // eval string, Z then assigned to num. vec              
    
        for (int i=0; i< v.size(); i++) {           // show the result
            std::cout << ""In C++ element "" << i << "" is "" << v[i] << std::endl;
        }
        exit(0);
    }

",r,rcpp,rinside,,,,open,0,962,10,"Multiple definition error building RInside samples on Windows 7 x64 I'm trying to build rinside_sample1.cpp from RInside\examples\standard directory on Windows 7 x64 using g++ from RTools in the following way:

    set RCPP=%R_HOME%\library\Rcpp
    set RINSIDE=%R_HOME%\library\RInside
    g++ -c -m64 rinside_sample1.cpp -I %RINSIDE%\include -I %RCPP%\include -I %R_HOME%\include
    g++ -m64 rinside_sample1.o -o rinside_sample1.exe -L %RINSIDE%\libs\x64 -l RInside -L %RCPP%\libs\x64 -l Rcpp -L %R_HOME%\bin\x64 -l R

The linkage results into the multiple definition error:

    d000026.o:(.idata$5+0x0): multiple definition of `__imp__ZTVN4Rcpp7RObjectE'
    d000019.o:(.idata$5+0x0): first defined here
    d000026.o:(.idata$6+0x0): multiple definition of `__nm__ZTVN4Rcpp7RObjectE'
    d000019.o:(.idata$6+0x0): first defined here
    collect2: ld returned 1 exit status

**However the similar building process for rinside_sample0.cpp succeedes. Does anyone have an idea of the solution?**

It seems that the problem with rinside_sample1.cpp arises because of Rcpp::NumericMatrix and Rcpp::NumericVector usage.

rinside_sample0.cpp code:

    // -*- mode: C++; c-indent-level: 4; c-basic-offset: 4;  tab-width: 8; -*-
    //
    // Simple example showing how to do the standard 'hello, world' using embedded R
    //
    // Copyright (C) 2009 Dirk Eddelbuettel 
    // Copyright (C) 2010 Dirk Eddelbuettel and Romain Francois
    //
    // GPL'ed 
    
    #include <RInside.h>                    // for the embedded R via RInside
    
    int main(int argc, char *argv[]) {
    
        RInside R(argc, argv);              // create an embedded R instance 
    
        R[""txt""] = ""Hello, world!\n"";	// assign a char* (string) to 'txt'
    
        R.parseEvalQ(""cat(txt)"");           // eval the init string, ignoring any returns
    
        exit(0);
    }

rinside_sample1.cpp code:

    // -*- mode: C++; c-indent-level: 4; c-basic-offset: 4; tab-width: 8; -*-
    //
    // Simple example with data in C++ that is passed to R, processed and a result is extracted
    //
    // Copyright (C) 2009         Dirk Eddelbuettel 
    // Copyright (C) 2010 - 2011  Dirk Eddelbuettel and Romain Francois
    //
    // GPL'ed 
    
    #include <RInside.h>                            // for the embedded R via RInside
    
    Rcpp::NumericMatrix createMatrix(const int n) {
        Rcpp::NumericMatrix M(n,n);
        for (int i=0; i<n; i++) {
            for (int j=0; j<n; j++) {
                M(i,j) = i*10 + j; 
            }
        }
        return(M);
    }
    
    int main(int argc, char *argv[]) {
    
        RInside R(argc, argv);                      // create an embedded R instance 
        
        const int mdim = 4;                         // let the matrices be 4 by 4; create, fill 
        R[""M""] = createMatrix(mdim);                // then assign data Matrix to R's 'M' var
    
        std::string str = 
            ""cat('Running ls()\n'); print(ls()); ""
            ""cat('Showing M\n'); print(M); ""
            ""cat('Showing colSums()\n'); Z <- colSums(M); print(Z); ""
            ""Z"";                     // returns Z
    
        
        Rcpp::NumericVector v = R.parseEval(str);   // eval string, Z then assigned to num. vec              
    
        for (int i=0; i< v.size(); i++) {           // show the result
            std::cout << ""In C++ element "" << i << "" is "" << v[i] << std::endl;
        }
        exit(0);
    }

",3
10039167,04/06/2012 04:37:32,815646,06/25/2011 20:14:30,1672,27,start a new R session in knitr,"How can I start a new `R` session in `knitr`? I would rather start a new session rather than use something like `rm(list=ls())` because it is not equivalent.

    <<myname>>=
    #some R code
    @
    <<another_chunk>>=
    #start a new R session
    #more R code
    @",r,knitr,,,,,open,0,64,7,"start a new R session in knitr How can I start a new `R` session in `knitr`? I would rather start a new session rather than use something like `rm(list=ls())` because it is not equivalent.

    <<myname>>=
    #some R code
    @
    <<another_chunk>>=
    #start a new R session
    #more R code
    @",2
10610067,05/15/2012 23:12:46,1372997,05/03/2012 15:56:40,6,0,Installing packages upon starting R session,"I am fairly new to R programming. I am trying to customize my R setup so that when an R session is started a few packages are installed at the beginning. I know that there is a .First() function that I can write in the Rprofile.site file. However, upon adding my install package code inside the .First() function, the package does not get installed. 

Furthermore, it seems to go into a loop of trying to create the package and it creates a lock file in the library folder in R. This causes my computer to really slow down (almost to the point where it is frozen) because it keeps trying to install that package.

Here is the code that I have added to the end of the Rprofile.site file. 

    .First <- function() {
      install.packages(""customPackage.tar.gz"", repos=NULL, type=""source"")
      cat(""\nWelcome to R on "", date(), ""\n"") 
    }

I even tried adding the install.packages line just by itself in the file (without having a .First() function) to no avail. 

The customPackage.tar.gz refers to a package I built up using existing code that I have written up. Since this is a custom package, the repos is NULL. If I do not include this line in my .First() function and just run the command after launching the R session, the package gets installed just fine in the R/R-2.15.0/library folder. 

There are several custom packages that I need to have installed upon the beginning of an R session, and that's why it is important that I add all these installation lines of code in the Rprofile.site file. Any ideas on how I can do this? Everywhere I have looked online on customizing the Rprofile.site file shows examples of just using libraries that already exist (For example, library(R2HTML)), but nothing for installing new libraries. Thanks for the help!",r,customization,,,,,open,0,316,6,"Installing packages upon starting R session I am fairly new to R programming. I am trying to customize my R setup so that when an R session is started a few packages are installed at the beginning. I know that there is a .First() function that I can write in the Rprofile.site file. However, upon adding my install package code inside the .First() function, the package does not get installed. 

Furthermore, it seems to go into a loop of trying to create the package and it creates a lock file in the library folder in R. This causes my computer to really slow down (almost to the point where it is frozen) because it keeps trying to install that package.

Here is the code that I have added to the end of the Rprofile.site file. 

    .First <- function() {
      install.packages(""customPackage.tar.gz"", repos=NULL, type=""source"")
      cat(""\nWelcome to R on "", date(), ""\n"") 
    }

I even tried adding the install.packages line just by itself in the file (without having a .First() function) to no avail. 

The customPackage.tar.gz refers to a package I built up using existing code that I have written up. Since this is a custom package, the repos is NULL. If I do not include this line in my .First() function and just run the command after launching the R session, the package gets installed just fine in the R/R-2.15.0/library folder. 

There are several custom packages that I need to have installed upon the beginning of an R session, and that's why it is important that I add all these installation lines of code in the Rprofile.site file. Any ideas on how I can do this? Everywhere I have looked online on customizing the Rprofile.site file shows examples of just using libraries that already exist (For example, library(R2HTML)), but nothing for installing new libraries. Thanks for the help!",2
2856140,05/18/2010 09:53:42,322912,04/22/2010 04:40:10,52,0,tidy/efficient function writing in R,"Excuse my ignorance, as I'm not a computer engineer but with roots in biology. I have become a great fan of pre-allocating objects (kudos to SO and R inferno by Patrick Burns) and would like to improve my coding habits. In lieu of this fact, I've been thinking about writing more efficient functions and have the following question.

Is there any benefits in removing variables that will be overwritten at the start of the next loop, or is this just a waste of time? For the sake of argument, let's assume that the size of old and new variables is very similar or identical.",r,memory-efficient,,,,,open,0,103,5,"tidy/efficient function writing in R Excuse my ignorance, as I'm not a computer engineer but with roots in biology. I have become a great fan of pre-allocating objects (kudos to SO and R inferno by Patrick Burns) and would like to improve my coding habits. In lieu of this fact, I've been thinking about writing more efficient functions and have the following question.

Is there any benefits in removing variables that will be overwritten at the start of the next loop, or is this just a waste of time? For the sake of argument, let's assume that the size of old and new variables is very similar or identical.",2
10537274,05/10/2012 15:24:45,1169210,01/25/2012 12:37:27,86,2,R storing models by an index,"Using the package earth, I have a number of models and I would like to call a model up by its index number
I have tried using a list but this does not work.

a <- list(a)
a[1] <- earth(Volume ~ ., data = trees1)

a[2] <- earth(Volume ~ ., data = trees2)

a[3] <- earth(Volume ~ ., data = trees3)

I would be grateful for your help.
",r,,,,,,open,0,62,6,"R storing models by an index Using the package earth, I have a number of models and I would like to call a model up by its index number
I have tried using a list but this does not work.

a <- list(a)
a[1] <- earth(Volume ~ ., data = trees1)

a[2] <- earth(Volume ~ ., data = trees2)

a[3] <- earth(Volume ~ ., data = trees3)

I would be grateful for your help.
",1
11327724,07/04/2012 10:58:29,1496589,07/02/2012 16:16:55,18,0,"(Plotting) ""Average"" of several density functions","I have a data set grouped by a factor (like e.g. cyl in the built in mtcars in R). I plotted the estimated density funtions (desityplot) for each factor using the lattice package, e.g.

    library(""lattice"")
    densityplot( ~mtcars$mpg[mtcars$cyl==4] )
    densityplot( ~mtcars$mpg[mtcars$cyl==6] )
    densityplot( ~mtcars$mpg[mtcars$cyl==8] )

Now I would like to create a densityplot() for the whole dataset but the estimation should disregard the different number of datapoints available for each factor, e.g.

    densityplot( ~mtcars$mpg )

is not what I want, because there are more data points with cyl=8 than data points with cyl=6. Is it possible to estimate the density separately for each factor and than calculate some sort of ""average"" density? If not, what is the appropriate method for my purpose?",r,lattice,,,,07/05/2012 15:02:20,off topic,1,134,6,"(Plotting) ""Average"" of several density functions I have a data set grouped by a factor (like e.g. cyl in the built in mtcars in R). I plotted the estimated density funtions (desityplot) for each factor using the lattice package, e.g.

    library(""lattice"")
    densityplot( ~mtcars$mpg[mtcars$cyl==4] )
    densityplot( ~mtcars$mpg[mtcars$cyl==6] )
    densityplot( ~mtcars$mpg[mtcars$cyl==8] )

Now I would like to create a densityplot() for the whole dataset but the estimation should disregard the different number of datapoints available for each factor, e.g.

    densityplot( ~mtcars$mpg )

is not what I want, because there are more data points with cyl=8 than data points with cyl=6. Is it possible to estimate the density separately for each factor and than calculate some sort of ""average"" density? If not, what is the appropriate method for my purpose?",2
9592509,03/06/2012 21:52:15,913071,08/25/2011 22:34:44,40,0,How do I reshape these data in R?,"So -- I'm working with a df that's got these groups of repeated observations indexed by an id, like so:

    id | x1 | x2 | y1 | y2
    1    a    b    c    2
    1    a    b    d    3
    1    a    b    e    4
    2    ...
    2    ...
    ...
i.e., all the variables within each group are identical, save for y1 and y2 (generally speaking, y2 'modifies' y1.) All these variables that I've listed here are factors. What I'd like to do is to turn each one of these groups into something that resembles the following:

    id | x1 | x2 | y1' | y2' | y3' 
    1    a    b    c-2   d-3   e-4
    2    ...

where the y1's (y1-prime) are concatenations of adjacent values of y1 and y2, with a dash in between. However, the number of y1's differs from id-group to id-group, but I'd be happy with a very wide data frame that allows for these extras as a solution. Anyhow, I've (rather futilely, I must confess) tried melting and casting these data with reshape2, but at this point, I'm not sure whether I'm not going about this right, or that package just isn't a fit for what I'm trying to do here. Any advice would be appreciated -- thanks!",r,reshape,,,,,open,0,298,8,"How do I reshape these data in R? So -- I'm working with a df that's got these groups of repeated observations indexed by an id, like so:

    id | x1 | x2 | y1 | y2
    1    a    b    c    2
    1    a    b    d    3
    1    a    b    e    4
    2    ...
    2    ...
    ...
i.e., all the variables within each group are identical, save for y1 and y2 (generally speaking, y2 'modifies' y1.) All these variables that I've listed here are factors. What I'd like to do is to turn each one of these groups into something that resembles the following:

    id | x1 | x2 | y1' | y2' | y3' 
    1    a    b    c-2   d-3   e-4
    2    ...

where the y1's (y1-prime) are concatenations of adjacent values of y1 and y2, with a dash in between. However, the number of y1's differs from id-group to id-group, but I'd be happy with a very wide data frame that allows for these extras as a solution. Anyhow, I've (rather futilely, I must confess) tried melting and casting these data with reshape2, but at this point, I'm not sure whether I'm not going about this right, or that package just isn't a fit for what I'm trying to do here. Any advice would be appreciated -- thanks!",2
8005417,11/04/2011 05:59:14,986817,10/09/2011 22:13:11,382,5,mmap and csv files,"I am trying to understand how to use the package `mmap` 
to access large csv files. More precisely, I'd like to

1. Create a `mmap` object from a `csv` file with `mmap.csv()`;
2. Save the file created by `mmap.csv()` containing the data in binary format;
3. Be able to ""map the binary data back to R"" using the function `mmap()`.

Achieving 1. and 2. is easy: just use `mmap.cv()` and save the `tempfile()`
contains the binary data, or modify `mmap.cv()` to accept an extra parameter
as output file (and modify the line `tmpstruct <- tempfile()` accordingly).
What I am having trouble with is 3. In particular, I need to construct a 
C-struct for the records in the binary data from the `mmap` object. 
Here is a simple reproducible example:

    # create mmap object with its file
    library(mmap)
    data(cars)

    m <- as.mmap(cars, file=""cars.Rmap"")
    colnames(m) <- colnames(cars)
    str(m) 
    munmap(m)

The information from `str()` can be used to construct the C-struct
`record.struct` that allows mapping the binary file `cars.Rmap` 
via the function mmap. 

    # load from disk
    record.struct <- struct(speed = integer(),  # int32()
                            dist  = integer()   # int32())
                            )
    mm <- mmap(""temp.Rmap"", mode=record.struct)


*How can one construct `record.struct` directly 
from the mmap object `m`?* ",r,mmap,,,,,open,0,283,4,"mmap and csv files I am trying to understand how to use the package `mmap` 
to access large csv files. More precisely, I'd like to

1. Create a `mmap` object from a `csv` file with `mmap.csv()`;
2. Save the file created by `mmap.csv()` containing the data in binary format;
3. Be able to ""map the binary data back to R"" using the function `mmap()`.

Achieving 1. and 2. is easy: just use `mmap.cv()` and save the `tempfile()`
contains the binary data, or modify `mmap.cv()` to accept an extra parameter
as output file (and modify the line `tmpstruct <- tempfile()` accordingly).
What I am having trouble with is 3. In particular, I need to construct a 
C-struct for the records in the binary data from the `mmap` object. 
Here is a simple reproducible example:

    # create mmap object with its file
    library(mmap)
    data(cars)

    m <- as.mmap(cars, file=""cars.Rmap"")
    colnames(m) <- colnames(cars)
    str(m) 
    munmap(m)

The information from `str()` can be used to construct the C-struct
`record.struct` that allows mapping the binary file `cars.Rmap` 
via the function mmap. 

    # load from disk
    record.struct <- struct(speed = integer(),  # int32()
                            dist  = integer()   # int32())
                            )
    mm <- mmap(""temp.Rmap"", mode=record.struct)


*How can one construct `record.struct` directly 
from the mmap object `m`?* ",2
7960738,10/31/2011 22:52:41,623506,02/18/2011 17:05:48,68,1,Importing mgcv fails because Rlapack.dll cannot be found,"I want to link to the R statistical package in IronPython by using the [R.NET](http://rdotnet.codeplex.com/) library. It's been working fine, but now I need to use R's [mgcv](http://cran.r-project.org/web/packages/mgcv/index.html) library.

Importing `mgcv` fails (import is done with the command `rdn.r.EagerEvaluate(""library(mgcv)"")`, where `rdn` is an IronPython object that wraps the R.NET library). When the import fails, Windows opens a dialog box that says: ""The program can't start because Rlapack.dll is missing from your computer. Try reinstalling the program to fix this problem.""

Of course, R never would have worked in the first place if Rlapack.dll was missing, so what is going on? ",r,ironpython,,,,,open,0,99,8,"Importing mgcv fails because Rlapack.dll cannot be found I want to link to the R statistical package in IronPython by using the [R.NET](http://rdotnet.codeplex.com/) library. It's been working fine, but now I need to use R's [mgcv](http://cran.r-project.org/web/packages/mgcv/index.html) library.

Importing `mgcv` fails (import is done with the command `rdn.r.EagerEvaluate(""library(mgcv)"")`, where `rdn` is an IronPython object that wraps the R.NET library). When the import fails, Windows opens a dialog box that says: ""The program can't start because Rlapack.dll is missing from your computer. Try reinstalling the program to fix this problem.""

Of course, R never would have worked in the first place if Rlapack.dll was missing, so what is going on? ",2
5325005,03/16/2011 12:07:30,402681,07/26/2010 19:45:21,121,4,Control size of the output of seqLogo?,"I am using the `seqLogo` package to draw some sequence Logos.  I need to make the logos wider, the default drawing makes the logos into square graphs.  Is there a way to do this?",r,bioinformatics,sequence-diagram,logo,,,open,0,36,7,"Control size of the output of seqLogo? I am using the `seqLogo` package to draw some sequence Logos.  I need to make the logos wider, the default drawing makes the logos into square graphs.  Is there a way to do this?",4
4286188,11/26/2010 14:41:29,521469,11/26/2010 14:30:33,1,0,[R] Split matrix and rejoin,"This is my first post. Apologies in advance if my question is dumb. I'm new to programming.

Ok, So I have a a matrix(eBpvalues) in R that has 152720 rows and 2 columns.
I want to split into 10 separate matrices containing 15272 rows each.

I have tried this with:

>newmx <-split(as.data.frame(eBpvalues), rep(1:10, each = 15272)))

> summary(newmx)
   Length Class      Mode  
1  2      data.frame list    
2  2      data.frame list  
3  2      data.frame list  
4  2      data.frame list  
5  2      data.frame list  
6  2      data.frame list  
7  2      data.frame list  
8  2      data.frame list  
9  2      data.frame list  
10 2      data.frame list  

How would I go about joining these matrices side-by-side so I have a new matrix with 20 columns and 15272 rows?

Cheers,

Neil









",r,matrix,split,,,,open,0,196,5,"[R] Split matrix and rejoin This is my first post. Apologies in advance if my question is dumb. I'm new to programming.

Ok, So I have a a matrix(eBpvalues) in R that has 152720 rows and 2 columns.
I want to split into 10 separate matrices containing 15272 rows each.

I have tried this with:

>newmx <-split(as.data.frame(eBpvalues), rep(1:10, each = 15272)))

> summary(newmx)
   Length Class      Mode  
1  2      data.frame list    
2  2      data.frame list  
3  2      data.frame list  
4  2      data.frame list  
5  2      data.frame list  
6  2      data.frame list  
7  2      data.frame list  
8  2      data.frame list  
9  2      data.frame list  
10 2      data.frame list  

How would I go about joining these matrices side-by-side so I have a new matrix with 20 columns and 15272 rows?

Cheers,

Neil









",3
4559255,12/30/2010 01:10:00,59087,01/26/2009 19:01:48,4578,194,Concatenate data frames,"## Background
Distinguish between model values and predicted values.

## Problem
Consider the following code:

    library( 'gam' )

    slope = 0.55
    amplitude = 0.22
    frequency = 3
    noise = 0.75
    x <- 1:200
    y <- (slope * x / 100) + (amplitude * sin( frequency * x / 100 ))
    ynoise <- y + (noise * runif( length( x ) ))
    
    gam.object <- gam( ynoise ~ s( x ) )
    p <- predict( gam.object, data.frame( x = 1:210 ) )
    
    df <- data.frame( value=p, model='y' )

## Question
What is the R syntax to set some `model` rows of the data frame (`df`) to `'n'`?:

    df[201:210,2] <- 'n'

Doesn't work, nor do any of the variations I have tried.

## Related
http://stat.ethz.ch/R-manual/R-patched/library/base/html/Extract.data.frame.html

Thank you!",r,data.frame,,,,,open,0,159,3,"Concatenate data frames ## Background
Distinguish between model values and predicted values.

## Problem
Consider the following code:

    library( 'gam' )

    slope = 0.55
    amplitude = 0.22
    frequency = 3
    noise = 0.75
    x <- 1:200
    y <- (slope * x / 100) + (amplitude * sin( frequency * x / 100 ))
    ynoise <- y + (noise * runif( length( x ) ))
    
    gam.object <- gam( ynoise ~ s( x ) )
    p <- predict( gam.object, data.frame( x = 1:210 ) )
    
    df <- data.frame( value=p, model='y' )

## Question
What is the R syntax to set some `model` rows of the data frame (`df`) to `'n'`?:

    df[201:210,2] <- 'n'

Doesn't work, nor do any of the variations I have tried.

## Related
http://stat.ethz.ch/R-manual/R-patched/library/base/html/Extract.data.frame.html

Thank you!",2
11739335,07/31/2012 11:42:58,1565505,07/31/2012 11:01:59,1,0,te( ) interactions and AIC model selection with GA,"I'm working with a time-series of several years and to analyze it, I’m using GAM smoothers from the package mgcv. I’m constructing models where zooplankton biomass (bm) is the dependent variable and the continuous explanatory variables are: 
-time in Julian days (t), to creat a long-term linear trend 
-Julian days of the year (t_year) to create an annual cycle 
- Mean temperature of Winter (temp_W), Temperature of September (temp_sept) or Chla. 

Questions: 


1) To introduce a tensor product modifying the annual cycle in my model, I tried 2 different approaches: 

a) gam( bm ~ t + te (t_year, temp_W, temp_sept, k = c( 5,30 ), d = ( 1,2), bs = c(  “cc”,”cr” ) ), data = data ) 

b) gam( bm ~ t + te ( t_year, temp_W, temp_sept, k = 5, bs = c( “cc”,”cr”,”cr” ) ), data = data ) 

Here is my problem: when I’m using just 2 variables (e.g., t_year and temp_W) for the tensor product, I can understand pretty well how the interpolation works and visualize it with vis.gam() as a 3d plot or a contour one. But with 3 variables is difficult to me to understand how it works. Besides, I don’t which one is the proper way to construct it, a) or b). Finally, when I plot a) or b) as vis.gam (model_name , view= c(“t_year”, “temp_W”)), How should I interpret the plot? The effect of temp_W on the annual cycle after considering already the effect of temp_sept or just the individual effect of Temp_W on the annual cycle?

 
2) I’m trying to do a model selection using AIC criteria. I have several questions about it: 

Should I use always the same type of smoothing basis (bs), the same type of smoother ( e.g te) and the same dimension of the basis (k)? Example: 

Option 1: 

a) mod1 <- gam(bm ~ t, data = data ) 

b) mod2 <- gam( bm ~ te ( t, k = 5, bs = “cr” ), data = data ) 

c) mod3 <- gam( bm ~ te ( t_year, k = 5, bs = “cc”), data = data ) 

d) mod4 <- gam( bm ~ te ( t_year, temp_W, k = 5, bs = c( “cc”,”cr” ) ), data = data ) 

e) mod5 <- gam( bm ~ te ( t_year, temp_W, temp_sept, k = 5, bs = c( “cc”,”cr”,”cr” ) ), data = data ). 

Here the limitation for k = 5, is due to mod5, I don’t use s () because in mod4 and mod5 te () is used and finally, I always use “cr” and “cc”. 

Option 2: 

a) mod1 <- gam( bm ~ t, data = data ) 

b) mod2 <- gam( bm ~ s ( t, k = 13, bs = “cr” ), data = data ) 

c) mod3 <- gam( bm ~ s( t_year, k = 13, bs = “cc” ), data = data ) 

d) mod4 <- gam( bm ~ te( t_year, temp_W, k = 11, bs = c( “cc”,”cr” ) ), data = data) 

e) mod5 <- gam( bm ~ te( t_year, temp_W, temp_sept, k = 5, bs = c( “cc”,”cr”,”cr” ) ), data = data ). 

I can get lower AIC for each of the models with Option 2, but are they comparable when I use AIC criteria? Is it therefore the proper way to do it as in Option 1? 

AIC (mod1, mod2, mod3, mod4, mod5). 

Thank you in advance, 
Best regards, 
Ricardo González-Gil ",r,interaction,selectionmodel,gam,,07/31/2012 13:15:42,off topic,1,580,9,"te( ) interactions and AIC model selection with GA I'm working with a time-series of several years and to analyze it, I’m using GAM smoothers from the package mgcv. I’m constructing models where zooplankton biomass (bm) is the dependent variable and the continuous explanatory variables are: 
-time in Julian days (t), to creat a long-term linear trend 
-Julian days of the year (t_year) to create an annual cycle 
- Mean temperature of Winter (temp_W), Temperature of September (temp_sept) or Chla. 

Questions: 


1) To introduce a tensor product modifying the annual cycle in my model, I tried 2 different approaches: 

a) gam( bm ~ t + te (t_year, temp_W, temp_sept, k = c( 5,30 ), d = ( 1,2), bs = c(  “cc”,”cr” ) ), data = data ) 

b) gam( bm ~ t + te ( t_year, temp_W, temp_sept, k = 5, bs = c( “cc”,”cr”,”cr” ) ), data = data ) 

Here is my problem: when I’m using just 2 variables (e.g., t_year and temp_W) for the tensor product, I can understand pretty well how the interpolation works and visualize it with vis.gam() as a 3d plot or a contour one. But with 3 variables is difficult to me to understand how it works. Besides, I don’t which one is the proper way to construct it, a) or b). Finally, when I plot a) or b) as vis.gam (model_name , view= c(“t_year”, “temp_W”)), How should I interpret the plot? The effect of temp_W on the annual cycle after considering already the effect of temp_sept or just the individual effect of Temp_W on the annual cycle?

 
2) I’m trying to do a model selection using AIC criteria. I have several questions about it: 

Should I use always the same type of smoothing basis (bs), the same type of smoother ( e.g te) and the same dimension of the basis (k)? Example: 

Option 1: 

a) mod1 <- gam(bm ~ t, data = data ) 

b) mod2 <- gam( bm ~ te ( t, k = 5, bs = “cr” ), data = data ) 

c) mod3 <- gam( bm ~ te ( t_year, k = 5, bs = “cc”), data = data ) 

d) mod4 <- gam( bm ~ te ( t_year, temp_W, k = 5, bs = c( “cc”,”cr” ) ), data = data ) 

e) mod5 <- gam( bm ~ te ( t_year, temp_W, temp_sept, k = 5, bs = c( “cc”,”cr”,”cr” ) ), data = data ). 

Here the limitation for k = 5, is due to mod5, I don’t use s () because in mod4 and mod5 te () is used and finally, I always use “cr” and “cc”. 

Option 2: 

a) mod1 <- gam( bm ~ t, data = data ) 

b) mod2 <- gam( bm ~ s ( t, k = 13, bs = “cr” ), data = data ) 

c) mod3 <- gam( bm ~ s( t_year, k = 13, bs = “cc” ), data = data ) 

d) mod4 <- gam( bm ~ te( t_year, temp_W, k = 11, bs = c( “cc”,”cr” ) ), data = data) 

e) mod5 <- gam( bm ~ te( t_year, temp_W, temp_sept, k = 5, bs = c( “cc”,”cr”,”cr” ) ), data = data ). 

I can get lower AIC for each of the models with Option 2, but are they comparable when I use AIC criteria? Is it therefore the proper way to do it as in Option 1? 

AIC (mod1, mod2, mod3, mod4, mod5). 

Thank you in advance, 
Best regards, 
Ricardo González-Gil ",4
7370348,09/10/2011 07:44:42,937964,09/10/2011 07:44:42,1,0,Extracting specific information from web pages using R,"I am researching various websites and would like to know if there is any code that can extract specific information from these sites. I have seen a few questions and answers around the RCurl and RTidyHTML packages. However, these do not work for me as they are customized for the sites in question. For instance I need to extract the product categories & sub-categories from the site : http://www.20north.com. I would need to extract and make a data frame comprising of only data about the categories such as ""Handbags & Accessories"",""Clothing & Accessories"",etc. Any help I can get would be greatly appreciated!

Thanks in advance!",r,web-scraping,,,,09/10/2011 12:08:55,not a real question,1,104,8,"Extracting specific information from web pages using R I am researching various websites and would like to know if there is any code that can extract specific information from these sites. I have seen a few questions and answers around the RCurl and RTidyHTML packages. However, these do not work for me as they are customized for the sites in question. For instance I need to extract the product categories & sub-categories from the site : http://www.20north.com. I would need to extract and make a data frame comprising of only data about the categories such as ""Handbags & Accessories"",""Clothing & Accessories"",etc. Any help I can get would be greatly appreciated!

Thanks in advance!",2
10089142,04/10/2012 12:50:03,1140326,01/10/2012 07:33:38,98,2,Converting hours and minutes column in dataframe in time format,"I have a dataframe containing 3 columns including minutes and hours.
I want to convert these columsn namely mintutes and column in time format


given Data drame

     Score Hour Min
      10   10    56
      23   17     01

desired
 
      score time
       10    10:56:00
       23    17:01:00



        ",r,,,,,,open,0,97,10,"Converting hours and minutes column in dataframe in time format I have a dataframe containing 3 columns including minutes and hours.
I want to convert these columsn namely mintutes and column in time format


given Data drame

     Score Hour Min
      10   10    56
      23   17     01

desired
 
      score time
       10    10:56:00
       23    17:01:00



        ",1
10462957,05/05/2012 14:51:41,1376922,05/05/2012 14:34:13,1,0,Interpret knn.cv (R) results after applying on data set,"I have encountered a problem while using the k-nearest neighbors algorithm (with cross validation) on a data set in R, the knn.cv from the FNN package.
The data set consists of 4601 email cases with 58 attributes, with the 57 depending on character or word frequencies in the emails(numerical, range [0,100] ) , and the last one indicating if it is spam (value 1) or ham (value 0).
After indicating train and cl variables and using 10 neighbors, running the package presents a list of all the emails with values like 7.4032e+00 at each column, which i don't know how to use.
I need to find the percentage of spam and ham the package classifies and compare it with the correct percentage.
Anyone knows how to interpret these results?
Thanks in advance",r,dataset,knn,,,,open,0,127,9,"Interpret knn.cv (R) results after applying on data set I have encountered a problem while using the k-nearest neighbors algorithm (with cross validation) on a data set in R, the knn.cv from the FNN package.
The data set consists of 4601 email cases with 58 attributes, with the 57 depending on character or word frequencies in the emails(numerical, range [0,100] ) , and the last one indicating if it is spam (value 1) or ham (value 0).
After indicating train and cl variables and using 10 neighbors, running the package presents a list of all the emails with values like 7.4032e+00 at each column, which i don't know how to use.
I need to find the percentage of spam and ham the package classifies and compare it with the correct percentage.
Anyone knows how to interpret these results?
Thanks in advance",3
5198201,03/04/2011 19:14:55,621833,02/17/2011 17:29:10,26,1,Methods and R packages for feature selection for nonparametric regression or density estimation.,"A newbie question here.  I am currently performing a nonparametric regression using the np package in R.  I have 7 features and using a brute force approach I identified the best 3.  But, soon I will have many more than 7 features!

My question is what are the current best methods for feature selection for nonparametric regression.  And which if any packages implement the methods.  Thank you.

",r,feature-detection,kernel-density,,,03/04/2011 22:21:26,off topic,1,71,13,"Methods and R packages for feature selection for nonparametric regression or density estimation. A newbie question here.  I am currently performing a nonparametric regression using the np package in R.  I have 7 features and using a brute force approach I identified the best 3.  But, soon I will have many more than 7 features!

My question is what are the current best methods for feature selection for nonparametric regression.  And which if any packages implement the methods.  Thank you.

",3
5749058,04/21/2011 19:48:54,302058,03/25/2010 20:23:52,106,0,Extend memory size limit in R,"I have a R program that combines 10 files each file is of size 296MB and I have increased the memory size to 8GB (Size of RAM)

--max-mem-size=8192M

, and when I ran this program I got a error saying

In type.convert(data[[i]], as.is = as.is[i], dec = dec, na.strings = character(0L)) :
  Reached total allocation of 7646Mb: see help(memory.size) 

Here is my R program 
---------------------------------
S1 <- read.csv2(""C:/Sim_Omega3_results/sim_omega3_1_400.txt"");
S2 <- read.csv2(""C:/Sim_Omega3_results/sim_omega3_401_800.txt"");
S3 <- read.csv2(""C:/Sim_Omega3_results/sim_omega3_801_1200.txt"");
S4 <- read.csv2(""C:/Sim_Omega3_results/sim_omega3_1201_1600.txt"");
S5 <- read.csv2(""C:/Sim_Omega3_results/sim_omega3_1601_2000.txt"");
S6 <- read.csv2(""C:/Sim_Omega3_results/sim_omega3_2001_2400.txt"");
S7 <- read.csv2(""C:/Sim_Omega3_results/sim_omega3_2401_2800.txt"");
S8 <- read.csv2(""C:/Sim_Omega3_results/sim_omega3_2801_3200.txt"");
S9 <- read.csv2(""C:/Sim_Omega3_results/sim_omega3_3201_3600.txt"");
S10 <- read.csv2(""C:/Sim_Omega3_results/sim_omega3_3601_4000.txt"");
options(max.print=154.8E10);
combine_result <- rbind(S1,S2,S3,S4,S5,S6,S7,S8,S9,S10)
write.table(combine_result,file=""C:/sim_omega3_1_4000.txt"",sep="";"",row.names=FALSE,col.names=TRUE, quote = FALSE);
----------------------------------------------------------------------------------

Can anyone, help me with this

Thanks,

Shruti.",r,,,,,,open,0,93,6,"Extend memory size limit in R I have a R program that combines 10 files each file is of size 296MB and I have increased the memory size to 8GB (Size of RAM)

--max-mem-size=8192M

, and when I ran this program I got a error saying

In type.convert(data[[i]], as.is = as.is[i], dec = dec, na.strings = character(0L)) :
  Reached total allocation of 7646Mb: see help(memory.size) 

Here is my R program 
---------------------------------
S1 <- read.csv2(""C:/Sim_Omega3_results/sim_omega3_1_400.txt"");
S2 <- read.csv2(""C:/Sim_Omega3_results/sim_omega3_401_800.txt"");
S3 <- read.csv2(""C:/Sim_Omega3_results/sim_omega3_801_1200.txt"");
S4 <- read.csv2(""C:/Sim_Omega3_results/sim_omega3_1201_1600.txt"");
S5 <- read.csv2(""C:/Sim_Omega3_results/sim_omega3_1601_2000.txt"");
S6 <- read.csv2(""C:/Sim_Omega3_results/sim_omega3_2001_2400.txt"");
S7 <- read.csv2(""C:/Sim_Omega3_results/sim_omega3_2401_2800.txt"");
S8 <- read.csv2(""C:/Sim_Omega3_results/sim_omega3_2801_3200.txt"");
S9 <- read.csv2(""C:/Sim_Omega3_results/sim_omega3_3201_3600.txt"");
S10 <- read.csv2(""C:/Sim_Omega3_results/sim_omega3_3601_4000.txt"");
options(max.print=154.8E10);
combine_result <- rbind(S1,S2,S3,S4,S5,S6,S7,S8,S9,S10)
write.table(combine_result,file=""C:/sim_omega3_1_4000.txt"",sep="";"",row.names=FALSE,col.names=TRUE, quote = FALSE);
----------------------------------------------------------------------------------

Can anyone, help me with this

Thanks,

Shruti.",1
8040608,11/07/2011 18:03:42,986662,10/09/2011 18:53:24,25,0,R: quit !incomplete.cases() function without loosing workspace,"I startet the `!incomplete.cases()` function to identify missing data and made a typing-error. So I wanted to restart it, but it doesn´t stop. I´ve tried `quit(save=""default"", status = 0, runlast = TRUE)` but that doesn´t work either. No reaction. Any ideas how I can stop the function without loosing the script/data saved in the workspace?",r,function,process,stop,,12/06/2011 04:27:43,too localized,1,55,7,"R: quit !incomplete.cases() function without loosing workspace I startet the `!incomplete.cases()` function to identify missing data and made a typing-error. So I wanted to restart it, but it doesn´t stop. I´ve tried `quit(save=""default"", status = 0, runlast = TRUE)` but that doesn´t work either. No reaction. Any ideas how I can stop the function without loosing the script/data saved in the workspace?",4
9803952,03/21/2012 11:42:51,1240108,02/29/2012 11:27:13,24,0,HoltWinter model,"0 down vote favorite
share [g+] share [fb] share [tw]
	

I have a time series object vels of type zoo:

2011-05-01 00:00:00 7.52

2011-05-01 00:10:00 7.69

2011-05-01 00:20:00 7.67

2011-05-01 00:30:00 7.52

2011-05-01 00:40:00 7.38

2011-05-01 00:50:00 7.56

2011-05-01 01:00:00 7.41

2011-05-01 01:10:00 7.11


I would like to fit a HoltWinter model but I see that the fitting is no correct. 
lines(HoltWinters(vels)$fitted,col=""red"")
It tells me that x and y differ.
Furthermore If I do: 
lines(HoltWinters(vels)[1:1000]$fitted,col=""red"")
It plots the line but with no correct fitting. 



",r,,,,,03/22/2012 10:47:30,off topic,1,71,2,"HoltWinter model 0 down vote favorite
share [g+] share [fb] share [tw]
	

I have a time series object vels of type zoo:

2011-05-01 00:00:00 7.52

2011-05-01 00:10:00 7.69

2011-05-01 00:20:00 7.67

2011-05-01 00:30:00 7.52

2011-05-01 00:40:00 7.38

2011-05-01 00:50:00 7.56

2011-05-01 01:00:00 7.41

2011-05-01 01:10:00 7.11


I would like to fit a HoltWinter model but I see that the fitting is no correct. 
lines(HoltWinters(vels)$fitted,col=""red"")
It tells me that x and y differ.
Furthermore If I do: 
lines(HoltWinters(vels)[1:1000]$fitted,col=""red"")
It plots the line but with no correct fitting. 



",1
4875719,02/02/2011 14:39:26,584879,01/21/2011 18:34:23,33,1,Kullback-Leibler divergence - interpretation,"I have another question about the Kullback-Leibler divergence. 

Can someone explain me why the ""distance"" between the blue density and the red density is smaller than the ""distance"" between the pink density and the red density?

Best regards,
Marco 

![enter image description here][1]


  [1]: http://i.stack.imgur.com/PDzAR.jpg",r,,,,,02/03/2011 04:03:59,off topic,1,44,4,"Kullback-Leibler divergence - interpretation I have another question about the Kullback-Leibler divergence. 

Can someone explain me why the ""distance"" between the blue density and the red density is smaller than the ""distance"" between the pink density and the red density?

Best regards,
Marco 

![enter image description here][1]


  [1]: http://i.stack.imgur.com/PDzAR.jpg",1
10071944,04/09/2012 10:33:35,1318686,04/07/2012 06:34:31,11,0,ggplot2 geom_point with binned x-axis for binary data,"I am trying to create a scatterplot with binned x-axis for binary data. When I use geom_point with binary y, the plot is pretty useless (see figure 1). As shown in figure 2 I want to bin the data based on the values of the x-axis and then plot the avg x and avg y within each bins as using geom_point (mapping the the number of obs in each bin to the size of the point). I can do this by aggregating the data but I was wondering whether ggplot can it directly. I played around with stat_bindot etc but wasn't able to find a solution. Any ideas? Below is some code. 

Thanks!


    # simulate data
    n=1000
    y=rbinom(n,1,0.5)
    x=runif(n)
    data=data.frame(x,y)

    # figure 1 - geom_point with binary data, pretty useless!
    ggplot(data,aes(x=x,y=y)) + geom_point() + ylim(0,1)

    # let's create an aggregated dataset with bins
    bin=cut(data$x,seq(0,1,0.05))
    # I am sure the aggregation can be done in a better way...
    data.bin=aggregate(data,list(bin),function(x) { return(c(mean(x),length(x)))})

    # figure 2 - geom_point with binned x-axis, much nicer!
    ggplot(data.bin,aes(x=x[,1],y=y[,1],size=x[,2])) + geom_point() + ylim(0,1)
",r,ggplot2,,,,,open,0,214,8,"ggplot2 geom_point with binned x-axis for binary data I am trying to create a scatterplot with binned x-axis for binary data. When I use geom_point with binary y, the plot is pretty useless (see figure 1). As shown in figure 2 I want to bin the data based on the values of the x-axis and then plot the avg x and avg y within each bins as using geom_point (mapping the the number of obs in each bin to the size of the point). I can do this by aggregating the data but I was wondering whether ggplot can it directly. I played around with stat_bindot etc but wasn't able to find a solution. Any ideas? Below is some code. 

Thanks!


    # simulate data
    n=1000
    y=rbinom(n,1,0.5)
    x=runif(n)
    data=data.frame(x,y)

    # figure 1 - geom_point with binary data, pretty useless!
    ggplot(data,aes(x=x,y=y)) + geom_point() + ylim(0,1)

    # let's create an aggregated dataset with bins
    bin=cut(data$x,seq(0,1,0.05))
    # I am sure the aggregation can be done in a better way...
    data.bin=aggregate(data,list(bin),function(x) { return(c(mean(x),length(x)))})

    # figure 2 - geom_point with binned x-axis, much nicer!
    ggplot(data.bin,aes(x=x[,1],y=y[,1],size=x[,2])) + geom_point() + ylim(0,1)
",2
10117805,04/12/2012 05:20:06,908777,08/24/2011 02:27:42,31,1,$ operator is invalid for atomic vectors in HH R,"I am trying to generate this graph. 
[Using this package][1]

Following is in datafile 	
         RAND,PREF,SEA,SN

	Cummulative,Q1,68,1238
	Current,Q1,67,1243
	Cummulative,Q2,70,1238
	Current,Q2,69,1243
	Cummulative,Q3,75,1238
	Current,Q3,75,1243
	Cummulative,Q4,78,1238
	Current,Q4,81,1243
	Cummulative,Q5,71,1238
	Current,Q5,68,1243
	Cummulative,Q6,77,1238
	Current,Q6,76,1243
	Cummulative,Q7,78,1238
	Current,Q7,80,1243
	Cummulative,Q8,78,1238
	Current,Q8,81,1243
	Cummulative,Q9,69,1238
	Current,Q9,68,1243
	Cummulative,Q10,69,1238
	Current,Q10,68,1243
	Cummulative,Q11,73,1238
	Current,Q11,74,1243
	Cummulative,Q12,77,1238
	Current,Q12,79,1243
	Cummulative,Q13,74,1238
	Current,Q13,73,1243
	Cummulative,Q14,76,1238
	Current,Q14,75,1243
	Cummulative,Q15,71,1238
	Current,Q15,72,1243
	Cummulative,Q16,63,1238
	Current,Q16,67,1243
	Cummulative,Q17,71,1238
	Current,Q17,70,1243


code:


	eqdata <- read.table(datafile , header = T,sep="","")
	#eqdata <- as.data.frame(eqdata)
	eqdata1 <- hh(eqdata)
	
	aed <- logrelrisk(eqdata1)
	
	p <- ae.dotplot(aed, A.name=""TREATMENT A (N=216)"",B.name=""TREATMENT B (N=431)"")
Output:
Error in ae$SAE : $ operator is invalid for atomic vectors
Calls: logrelrisk

any help much appreciated. 

  [1]: http://rgm2.lab.nig.ac.jp/RGM2/func.php?rd_id=HH:ae.dotplot",r,rscript,,,,,open,0,60,10,"$ operator is invalid for atomic vectors in HH R I am trying to generate this graph. 
[Using this package][1]

Following is in datafile 	
         RAND,PREF,SEA,SN

	Cummulative,Q1,68,1238
	Current,Q1,67,1243
	Cummulative,Q2,70,1238
	Current,Q2,69,1243
	Cummulative,Q3,75,1238
	Current,Q3,75,1243
	Cummulative,Q4,78,1238
	Current,Q4,81,1243
	Cummulative,Q5,71,1238
	Current,Q5,68,1243
	Cummulative,Q6,77,1238
	Current,Q6,76,1243
	Cummulative,Q7,78,1238
	Current,Q7,80,1243
	Cummulative,Q8,78,1238
	Current,Q8,81,1243
	Cummulative,Q9,69,1238
	Current,Q9,68,1243
	Cummulative,Q10,69,1238
	Current,Q10,68,1243
	Cummulative,Q11,73,1238
	Current,Q11,74,1243
	Cummulative,Q12,77,1238
	Current,Q12,79,1243
	Cummulative,Q13,74,1238
	Current,Q13,73,1243
	Cummulative,Q14,76,1238
	Current,Q14,75,1243
	Cummulative,Q15,71,1238
	Current,Q15,72,1243
	Cummulative,Q16,63,1238
	Current,Q16,67,1243
	Cummulative,Q17,71,1238
	Current,Q17,70,1243


code:


	eqdata <- read.table(datafile , header = T,sep="","")
	#eqdata <- as.data.frame(eqdata)
	eqdata1 <- hh(eqdata)
	
	aed <- logrelrisk(eqdata1)
	
	p <- ae.dotplot(aed, A.name=""TREATMENT A (N=216)"",B.name=""TREATMENT B (N=431)"")
Output:
Error in ae$SAE : $ operator is invalid for atomic vectors
Calls: logrelrisk

any help much appreciated. 

  [1]: http://rgm2.lab.nig.ac.jp/RGM2/func.php?rd_id=HH:ae.dotplot",2
9587376,03/06/2012 16:04:54,395744,07/19/2010 10:52:51,1056,3,R: Missing stacked values in barplot() with log scale,"A have a matrix with 3 rows and N columns, however some of the values are missing.
For a certain column, one or more values are 0 and those I would like.

I would like to plot the bars for each column, so that the values for rows are stacked. I also want to avoid that zero-value artifacts - bars of very small height which represent 0 or 1.

How to achieve this in R, preferably by using core packages / barplot?",r,plot,logging,missing,zero,03/07/2012 20:31:15,not a real question,1,79,9,"R: Missing stacked values in barplot() with log scale A have a matrix with 3 rows and N columns, however some of the values are missing.
For a certain column, one or more values are 0 and those I would like.

I would like to plot the bars for each column, so that the values for rows are stacked. I also want to avoid that zero-value artifacts - bars of very small height which represent 0 or 1.

How to achieve this in R, preferably by using core packages / barplot?",5
8279593,11/26/2011 15:48:47,570103,01/10/2011 16:31:31,21,1,R - Error in TeachingDemos output,"I was trying to generate some plots for t, z and chi-square test. I found an example with TeachingDemos package. The link [here][1]. The codes and corresponding error is as follows,

    > library(TeachingDemos)
    > z.ex <- z.test(rnorm(25,100,5),99,5)
    > z.ex
    	One Sample z-test
    data:  rnorm(25, 100, 5) 
    z = 3.0926, n = 25, Std. Dev. = 5, Std. Dev. of the sample mean = 1, p-value
    = 0.001984
    alternative hypothesis: true mean is not equal to 99 
    95 percent confidence interval:
     100.1326 104.0525 
    sample estimates:
    mean of rnorm(25, 100, 5) 
                     102.0926 
    > plot(z.ex)
    Error in xy.coords(x, y, xlabel, ylabel, log) : 
      'x' is a list, but does not have components 'x' and 'y'
    > 

I am using, R 2.13 on a Ubuntu 11.10 laptop. 
Anybody can tell me what should I do to plot what I wanted?

_Have a nice weekend.

_HM



  [1]: http://www.rforge.net/doc/packages/NCStats/plot.htest.html",r,,,,,,open,0,220,6,"R - Error in TeachingDemos output I was trying to generate some plots for t, z and chi-square test. I found an example with TeachingDemos package. The link [here][1]. The codes and corresponding error is as follows,

    > library(TeachingDemos)
    > z.ex <- z.test(rnorm(25,100,5),99,5)
    > z.ex
    	One Sample z-test
    data:  rnorm(25, 100, 5) 
    z = 3.0926, n = 25, Std. Dev. = 5, Std. Dev. of the sample mean = 1, p-value
    = 0.001984
    alternative hypothesis: true mean is not equal to 99 
    95 percent confidence interval:
     100.1326 104.0525 
    sample estimates:
    mean of rnorm(25, 100, 5) 
                     102.0926 
    > plot(z.ex)
    Error in xy.coords(x, y, xlabel, ylabel, log) : 
      'x' is a list, but does not have components 'x' and 'y'
    > 

I am using, R 2.13 on a Ubuntu 11.10 laptop. 
Anybody can tell me what should I do to plot what I wanted?

_Have a nice weekend.

_HM



  [1]: http://www.rforge.net/doc/packages/NCStats/plot.htest.html",1
7599146,09/29/2011 14:49:07,805808,06/19/2011 23:14:52,3495,116,Testing if rows of a matrix or data frame are sorted in R,"What is an efficient way to test if rows in a matrix are sorted?

I am porting some code that uses [`issorted(,'rows')` from Matlab](http://www.mathworks.com/help/techdoc/ref/issorted.html).  As it seems that `is.unsorted` does not extend beyond vectors, I'm writing or looking for something else.  The naive method is to check that the sorted version of the matrix (or data frame) is the same as the original, but that's obviously inefficient.  

NB: For sorting, a la [`sortrows()` in Matlab](http://www.mathworks.com/help/techdoc/ref/sortrows.html), my code essentially uses `SortedDF <- DF[do.call(order, DF),]` (it's wrapped in a larger function that converts matrices to data frames, passes parameters to `order`, etc.).  I wouldn't be surprised if there are faster implementations (data table comes to mind).",r,sorting,matlab,,,,open,0,117,13,"Testing if rows of a matrix or data frame are sorted in R What is an efficient way to test if rows in a matrix are sorted?

I am porting some code that uses [`issorted(,'rows')` from Matlab](http://www.mathworks.com/help/techdoc/ref/issorted.html).  As it seems that `is.unsorted` does not extend beyond vectors, I'm writing or looking for something else.  The naive method is to check that the sorted version of the matrix (or data frame) is the same as the original, but that's obviously inefficient.  

NB: For sorting, a la [`sortrows()` in Matlab](http://www.mathworks.com/help/techdoc/ref/sortrows.html), my code essentially uses `SortedDF <- DF[do.call(order, DF),]` (it's wrapped in a larger function that converts matrices to data frames, passes parameters to `order`, etc.).  I wouldn't be surprised if there are faster implementations (data table comes to mind).",3
9777888,03/19/2012 21:01:55,1279507,03/19/2012 20:44:05,1,0,a simple way of balancing linear equations?,"I'm looking to take a linear equation, such as 

0.4x + 0.2y + 0.4z = 1

where each coefficient is non-negative. Then, I want to tweak one variable by ± 0.1 (easy enough). The problem is that the equation will now be unbalanced in that 1.1 != 1. Does anyone have a simple method of adjusting the other coefficients so that these equations be balanced. 

0.4x + 0.2y + 0.4z = 1
0.5x + 0.15y + 0.35z = 1

It is important that each coefficient is adjusted by a minimal amount (so that the entire unbalance of 0.1 is not dumped onto one coefficient), because I'm intending to use this function in a genetic algorithm and I need to keep the other variables pretty constant. Does anyone have any ideas (preferably in pseudo code)??

Thanks",r,equation,linear,,,03/20/2012 16:44:27,not a real question,1,131,7,"a simple way of balancing linear equations? I'm looking to take a linear equation, such as 

0.4x + 0.2y + 0.4z = 1

where each coefficient is non-negative. Then, I want to tweak one variable by ± 0.1 (easy enough). The problem is that the equation will now be unbalanced in that 1.1 != 1. Does anyone have a simple method of adjusting the other coefficients so that these equations be balanced. 

0.4x + 0.2y + 0.4z = 1
0.5x + 0.15y + 0.35z = 1

It is important that each coefficient is adjusted by a minimal amount (so that the entire unbalance of 0.1 is not dumped onto one coefficient), because I'm intending to use this function in a genetic algorithm and I need to keep the other variables pretty constant. Does anyone have any ideas (preferably in pseudo code)??

Thanks",3
2665499,04/19/2010 06:21:35,170792,09/09/2009 11:47:11,2424,93,Error closing R commander when package rgl is loaded,"    library(ca)
    # Loading required package: rgl
    library(Rcmdr)
    # R Commander starts
    
    # When trying to close R Commander window
    Error in unloadNamespace(""rgl"") : name space 'rgl' is still used by: 'ca'

What is the suggested way to close R Commander in such a situation?
Thank you",r,libraries,,,,,open,0,67,9,"Error closing R commander when package rgl is loaded     library(ca)
    # Loading required package: rgl
    library(Rcmdr)
    # R Commander starts
    
    # When trying to close R Commander window
    Error in unloadNamespace(""rgl"") : name space 'rgl' is still used by: 'ca'

What is the suggested way to close R Commander in such a situation?
Thank you",2
6985844,08/08/2011 17:11:05,633426,02/25/2011 01:51:48,50,0,Deleting Specific Variables in R,"How can I delete certain variables that are saved in session in R?

Thank you.",r,variables,delete,,,06/07/2012 13:19:43,not a real question,1,14,5,"Deleting Specific Variables in R How can I delete certain variables that are saved in session in R?

Thank you.",3
7467066,09/19/2011 06:11:31,912429,08/25/2011 15:17:36,22,0,how to present the POSIXlt format time using gvisMotionChart?,"The googleVis package of R software is surprisingly good. However, I am puzzled by one problem of gvisMotionChart about the timevar, because the time in my data set is POSIXlt format, such as:

""2009-07-02 19:00:00"" ""2009-07-02 20:00:00"" ""2009-07-02 21:00:00"" ""2009-07-02 22:00:00"" ""2009-07-02 23:00:00"" .

Because the  time unit is hour, if I transform the  POSIXlt format to date format(using as.date), the hours information will be deleted and make the data nonsense. 

So my question is: how to present the  POSIXlt format time using gvisMotionChart?",r,,,,,,open,0,85,9,"how to present the POSIXlt format time using gvisMotionChart? The googleVis package of R software is surprisingly good. However, I am puzzled by one problem of gvisMotionChart about the timevar, because the time in my data set is POSIXlt format, such as:

""2009-07-02 19:00:00"" ""2009-07-02 20:00:00"" ""2009-07-02 21:00:00"" ""2009-07-02 22:00:00"" ""2009-07-02 23:00:00"" .

Because the  time unit is hour, if I transform the  POSIXlt format to date format(using as.date), the hours information will be deleted and make the data nonsense. 

So my question is: how to present the  POSIXlt format time using gvisMotionChart?",1
11478459,07/13/2012 21:12:36,811115,06/22/2011 20:28:20,106,0,Where is the RCPP package in R?,I can't find the rcpp package on any CRAN mirror. Has it been removed? I need this package to use the forecast package.,r,,,,,07/13/2012 21:29:06,too localized,1,23,7,Where is the RCPP package in R? I can't find the rcpp package on any CRAN mirror. Has it been removed? I need this package to use the forecast package.,1
7101909,08/18/2011 03:02:22,482711,10/21/2010 08:10:49,552,6,programming habits: Why we should specify argument names when calling a function,"Some people in my environment rely on the argument order to call a function

for instance

    function1(arg1, arg2, arg3, arg4, arg5)
instead of 

    function1(arg1=arg1, arg2=arg2, arg3=arg3, arg4=arg4, arg5=arg5)

I think it is error prone.

Is there a rule/convention/document for that?

I can see at least 2 situations where it is not great

 1. If we want to add a new argument, we are forced to add it at the end of the list, which may not great for the common sense (as I like to group arguments that goes together)
 2. The arguments with default have to be put at the end of the list, otherwise you have to input it even if you use the default value.

Any ideas on that?
",r,argument-passing,,,,08/18/2011 14:24:49,not constructive,1,123,12,"programming habits: Why we should specify argument names when calling a function Some people in my environment rely on the argument order to call a function

for instance

    function1(arg1, arg2, arg3, arg4, arg5)
instead of 

    function1(arg1=arg1, arg2=arg2, arg3=arg3, arg4=arg4, arg5=arg5)

I think it is error prone.

Is there a rule/convention/document for that?

I can see at least 2 situations where it is not great

 1. If we want to add a new argument, we are forced to add it at the end of the list, which may not great for the common sense (as I like to group arguments that goes together)
 2. The arguments with default have to be put at the end of the list, otherwise you have to input it even if you use the default value.

Any ideas on that?
",2
7298722,09/04/2011 10:58:44,122792,06/14/2009 17:52:19,707,4,Has anybody gotten Vim-R working properly under windows?,"I'm constantly getting this error message when I run VIM on a .R file:

    ""SAGB.R"" 515L, 26020C
    Please, install the 'screen' application to enable the Vim-R-plugin.
    Press <Enter> to continue.

This is after I already included the let vimrplugin_screenplugy = 0 line into my _vimrc (see here): 

     set nocompatible
     source $VIMRUNTIME/vimrc_example.vim
     source $VIMRUNTIME/mswin.vim
     syntax enable
     filetype plugin on
     filetype indent on
     set number
     let vimrplugin_screenplugin = 0

Before I put that line in I was getting a longer error message, namely: 



    ""SAGB.R"" 515L, 26020C
    Please, either install the 'tmux' application (recommended) or put 'let vimrplugin_screenplugin = 0' in your vimrc to enable the Vim-R-plugin.
    Please, either install the screen plugin (http://www.vim.org/scripts/script.php?script_id=2711) (recommended) or put 'let vimrplugin_screenplugin = 0' in your vimrc.
    Press <Enter> to continue.

Both of these apps are Linux only, if I am correct?

so basically I have done everything that it says on the [tin][1]. I have Python 2.7 with pywin32 installed as suggested, I have all the lines it wants in the _vimrc file, and still have that error. 

Thanks for any help/pointers. FWIW am running Win 7 under VMWare on OS/X Lion. Cannot run native R on OS/X because I access software that is only available on Windows. I can see the plugin in the c:\program files\vim\vimfiles\r-plugin directory. 


  [1]: http://www.vim.org/scripts/script.php?script_id=2628",r,vim,,,,09/04/2011 23:40:51,off topic,1,268,8,"Has anybody gotten Vim-R working properly under windows? I'm constantly getting this error message when I run VIM on a .R file:

    ""SAGB.R"" 515L, 26020C
    Please, install the 'screen' application to enable the Vim-R-plugin.
    Press <Enter> to continue.

This is after I already included the let vimrplugin_screenplugy = 0 line into my _vimrc (see here): 

     set nocompatible
     source $VIMRUNTIME/vimrc_example.vim
     source $VIMRUNTIME/mswin.vim
     syntax enable
     filetype plugin on
     filetype indent on
     set number
     let vimrplugin_screenplugin = 0

Before I put that line in I was getting a longer error message, namely: 



    ""SAGB.R"" 515L, 26020C
    Please, either install the 'tmux' application (recommended) or put 'let vimrplugin_screenplugin = 0' in your vimrc to enable the Vim-R-plugin.
    Please, either install the screen plugin (http://www.vim.org/scripts/script.php?script_id=2711) (recommended) or put 'let vimrplugin_screenplugin = 0' in your vimrc.
    Press <Enter> to continue.

Both of these apps are Linux only, if I am correct?

so basically I have done everything that it says on the [tin][1]. I have Python 2.7 with pywin32 installed as suggested, I have all the lines it wants in the _vimrc file, and still have that error. 

Thanks for any help/pointers. FWIW am running Win 7 under VMWare on OS/X Lion. Cannot run native R on OS/X because I access software that is only available on Windows. I can see the plugin in the c:\program files\vim\vimfiles\r-plugin directory. 


  [1]: http://www.vim.org/scripts/script.php?script_id=2628",2
10462342,05/05/2012 13:29:27,961264,09/23/2011 13:35:04,530,1,R: unlist a list and build an array (with dimnames as specified)?,"I have a list `res.raw` which I would like to convert to an array of lists `res` as indicated below. The problem is that I do not get the correct order of the variables in the array. How can I build `res` from `res.raw` in the correct (given) order (as given by `dim` and `dimnames`?

    N <- 2
    nd <- length(d <- c(2, 5, 10))
    nt <- length(t <- c(0.25))
    nm <- length(m <- c(""m1"", ""m2"", ""m3"", ""m4""))
    nil <- length(il <- c(FALSE, TRUE))

    g <- expand.grid(il=il, t=t, d=d, stringsAsFactors=FALSE)[,3:1]
    ng <- nrow(g)

    require(foreach)
    res.raw <- foreach(l=1:ng) %dopar% {
        res. <- lapply(1:N, function(x) list(res.m1=list(value=l, foo=l),
                                             res.m2=list(value=l+1, foo=l+1),
                                             res.m3=list(value=l+2, foo=l+2),
                                             res.m4=list(value=l+3, foo=l+3)))
        names(res.) <- 1:N
        res.
    }
    names(res.raw) <- apply(g, 1, paste, collapse="".""))

    ## str(res.raw)
    ## List of 6 => d*t*il
    ##  $ :List of 2 => N
    ##   ..$ 1:List of 4 => m's
    ##   .. ..$ res.m1:List of 2
    ##   .. .. ..$ value: int 1
    ##   .. .. ..$ foo  : int 1

    ## str(unlist(res.raw, recursive=FALSE))
    ## List of 12 => d*t*il*N (or possibly different order)
    ##  $ 1:List of 4 => m's
    ##   ..$ res.m1:List of 2
    ##   .. ..$ value: int 1
    ##   .. ..$ foo  : int 1

    ## str(unlist(unlist(res.raw, recursive=FALSE), recursive=FALSE))
    ## List of 48 => d*t*il*N*m (or possibly different order)
    ##  $ 1.res.m1:List of 2
    ##   ..$ value: int 1
    ##   ..$ foo  : int 1

    ul <- function(x) unlist(x, recursive=FALSE)
    res <- array(ul(ul(res.raw)), dim=c(nd, nt, nil, N, nm),
                 dimnames=list(d=d, t=t, il=il, N=1:N, m=m))
    ## => wrong order
",r,,,,,,open,0,544,12,"R: unlist a list and build an array (with dimnames as specified)? I have a list `res.raw` which I would like to convert to an array of lists `res` as indicated below. The problem is that I do not get the correct order of the variables in the array. How can I build `res` from `res.raw` in the correct (given) order (as given by `dim` and `dimnames`?

    N <- 2
    nd <- length(d <- c(2, 5, 10))
    nt <- length(t <- c(0.25))
    nm <- length(m <- c(""m1"", ""m2"", ""m3"", ""m4""))
    nil <- length(il <- c(FALSE, TRUE))

    g <- expand.grid(il=il, t=t, d=d, stringsAsFactors=FALSE)[,3:1]
    ng <- nrow(g)

    require(foreach)
    res.raw <- foreach(l=1:ng) %dopar% {
        res. <- lapply(1:N, function(x) list(res.m1=list(value=l, foo=l),
                                             res.m2=list(value=l+1, foo=l+1),
                                             res.m3=list(value=l+2, foo=l+2),
                                             res.m4=list(value=l+3, foo=l+3)))
        names(res.) <- 1:N
        res.
    }
    names(res.raw) <- apply(g, 1, paste, collapse="".""))

    ## str(res.raw)
    ## List of 6 => d*t*il
    ##  $ :List of 2 => N
    ##   ..$ 1:List of 4 => m's
    ##   .. ..$ res.m1:List of 2
    ##   .. .. ..$ value: int 1
    ##   .. .. ..$ foo  : int 1

    ## str(unlist(res.raw, recursive=FALSE))
    ## List of 12 => d*t*il*N (or possibly different order)
    ##  $ 1:List of 4 => m's
    ##   ..$ res.m1:List of 2
    ##   .. ..$ value: int 1
    ##   .. ..$ foo  : int 1

    ## str(unlist(unlist(res.raw, recursive=FALSE), recursive=FALSE))
    ## List of 48 => d*t*il*N*m (or possibly different order)
    ##  $ 1.res.m1:List of 2
    ##   ..$ value: int 1
    ##   ..$ foo  : int 1

    ul <- function(x) unlist(x, recursive=FALSE)
    res <- array(ul(ul(res.raw)), dim=c(nd, nt, nil, N, nm),
                 dimnames=list(d=d, t=t, il=il, N=1:N, m=m))
    ## => wrong order
",1
11596747,07/21/2012 23:59:23,954721,09/20/2011 12:12:56,300,2,Issues with XTS and multiple large files - Causing R to Crash,"I am using XTS and am loading multiple files, between 100 and 1000.  Each file is anywhere between 50k to 300k lines in size.

I am using the latest version of R 2.15.1 on Windows 7 64 bit.  I have also tried on Ubuntu Linux with R version 2.14.X

It seems that sometimes R will crash, other times it will do just fine.  Very difficult to isolate what is happening here.  Has anyone else had these sort of difficulties?",r,xts,,,,07/23/2012 21:34:52,too localized,1,81,12,"Issues with XTS and multiple large files - Causing R to Crash I am using XTS and am loading multiple files, between 100 and 1000.  Each file is anywhere between 50k to 300k lines in size.

I am using the latest version of R 2.15.1 on Windows 7 64 bit.  I have also tried on Ubuntu Linux with R version 2.14.X

It seems that sometimes R will crash, other times it will do just fine.  Very difficult to isolate what is happening here.  Has anyone else had these sort of difficulties?",2
8735365,01/04/2012 23:01:51,173292,09/14/2009 17:48:23,1804,52,Capture last output as an R object,"I'm working with R in ESS and just made the stupid mistake of running a long running function without assigning the result to a variable. So, it just printed out the result, a long string of output that looks like:

    [[1]]
    1 FALSE
    [[2]]
    1 TRUE
    [[3]]
    1 TRUE
    [[4]]
    1 TRUE 

Is there any way to coerce this printed output into an R object? Either within R, or using emacs (M-x undo-my-stupid-mistake)?",r,ess,,,,,open,0,97,7,"Capture last output as an R object I'm working with R in ESS and just made the stupid mistake of running a long running function without assigning the result to a variable. So, it just printed out the result, a long string of output that looks like:

    [[1]]
    1 FALSE
    [[2]]
    1 TRUE
    [[3]]
    1 TRUE
    [[4]]
    1 TRUE 

Is there any way to coerce this printed output into an R object? Either within R, or using emacs (M-x undo-my-stupid-mistake)?",2
10161807,04/15/2012 11:55:52,636656,02/27/2011 17:23:21,7352,178,Reshape in the middle,"As part of piloting a survey, I presented each Turker with sets of choices amongst four alternatives.  The data looks like this:

    > so
      WorkerId pio_1_1 pio_1_2 pio_1_3 pio_1_4 pio_2_1 pio_2_2 pio_2_3 pio_2_4
    1        1     Yes      No      No      No      No      No     Yes      No
    2        2      No     Yes      No      No     Yes      No     Yes      No
    3        3     Yes     Yes      No      No     Yes      No     Yes      No

I'd like it to look like this:

    WorkerId set pio1 pio2 pio3 pio4
           1   1  Yes   No   No   No
           1   2   No   No  Yes   No
    ...

I can kludge through this by a number of means, none of which seem very elegant:

 - Swapping the order of the numbers with regexes and backreferencing and then using reshape()
 - Writing my own little function to parse out the first digit between the underscores and then reshape it long
 - Splitting and then stacking the columns (relies on the ordering being right)

But it seems to me that all of these ignore the idea that data in what you might call ""double wide"" format has its own structure.  I'd love to use the reshape2 package for this, but despite the data having been produced with cast() I don't see any options that would help me truly melt this data.frame back.

Suggestions welcome.

    so <- structure(list(WorkerId = 1:3, pio_1_1 = structure(c(2L, 1L, 
    2L), .Label = c(""No"", ""Yes""), class = ""factor""), pio_1_2 = structure(c(1L, 
    2L, 2L), .Label = c(""No"", ""Yes""), class = ""factor""), pio_1_3 = structure(c(1L, 
    1L, 1L), .Label = c(""No"", ""Yes""), class = ""factor""), pio_1_4 = structure(c(1L, 
    1L, 1L), .Label = ""No"", class = ""factor""), pio_2_1 = structure(c(1L, 
    2L, 2L), .Label = c(""No"", ""Yes""), class = ""factor""), pio_2_2 = structure(c(1L, 
    1L, 1L), .Label = c(""No"", ""Yes""), class = ""factor""), pio_2_3 = structure(c(2L, 
    2L, 2L), .Label = c(""No"", ""Yes""), class = ""factor""), pio_2_4 = structure(c(1L, 
    1L, 1L), .Label = ""No"", class = ""factor"")), .Names = c(""WorkerId"", 
    ""pio_1_1"", ""pio_1_2"", ""pio_1_3"", ""pio_1_4"", ""pio_2_1"", ""pio_2_2"", 
    ""pio_2_3"", ""pio_2_4""), row.names = c(NA, 3L), class = ""data.frame"")",r,reshape,,,,,open,0,564,4,"Reshape in the middle As part of piloting a survey, I presented each Turker with sets of choices amongst four alternatives.  The data looks like this:

    > so
      WorkerId pio_1_1 pio_1_2 pio_1_3 pio_1_4 pio_2_1 pio_2_2 pio_2_3 pio_2_4
    1        1     Yes      No      No      No      No      No     Yes      No
    2        2      No     Yes      No      No     Yes      No     Yes      No
    3        3     Yes     Yes      No      No     Yes      No     Yes      No

I'd like it to look like this:

    WorkerId set pio1 pio2 pio3 pio4
           1   1  Yes   No   No   No
           1   2   No   No  Yes   No
    ...

I can kludge through this by a number of means, none of which seem very elegant:

 - Swapping the order of the numbers with regexes and backreferencing and then using reshape()
 - Writing my own little function to parse out the first digit between the underscores and then reshape it long
 - Splitting and then stacking the columns (relies on the ordering being right)

But it seems to me that all of these ignore the idea that data in what you might call ""double wide"" format has its own structure.  I'd love to use the reshape2 package for this, but despite the data having been produced with cast() I don't see any options that would help me truly melt this data.frame back.

Suggestions welcome.

    so <- structure(list(WorkerId = 1:3, pio_1_1 = structure(c(2L, 1L, 
    2L), .Label = c(""No"", ""Yes""), class = ""factor""), pio_1_2 = structure(c(1L, 
    2L, 2L), .Label = c(""No"", ""Yes""), class = ""factor""), pio_1_3 = structure(c(1L, 
    1L, 1L), .Label = c(""No"", ""Yes""), class = ""factor""), pio_1_4 = structure(c(1L, 
    1L, 1L), .Label = ""No"", class = ""factor""), pio_2_1 = structure(c(1L, 
    2L, 2L), .Label = c(""No"", ""Yes""), class = ""factor""), pio_2_2 = structure(c(1L, 
    1L, 1L), .Label = c(""No"", ""Yes""), class = ""factor""), pio_2_3 = structure(c(2L, 
    2L, 2L), .Label = c(""No"", ""Yes""), class = ""factor""), pio_2_4 = structure(c(1L, 
    1L, 1L), .Label = ""No"", class = ""factor"")), .Names = c(""WorkerId"", 
    ""pio_1_1"", ""pio_1_2"", ""pio_1_3"", ""pio_1_4"", ""pio_2_1"", ""pio_2_2"", 
    ""pio_2_3"", ""pio_2_4""), row.names = c(NA, 3L), class = ""data.frame"")",2
6483202,06/26/2011 09:38:06,816025,06/26/2011 09:38:06,1,0,"optimization using ""nlminb""","im now performing Location Model using non-parametric smoothing to estimate the paramneters.....one of the smoothed paramater is the lamdha that i have to optimize...

  so in that case, i decide to use ""nlminb function"" to achieve it.....

  however, my programing give me the same ""$par"" value even though it was iterate 150 time and make 200 evaluation (by default)..... which is it choose ""the start value as $par"" (that is 0.000001 ...... i think, there must be something wrong with my written program.... 
  
 my programing look like:-   (note: w is the parameter that i want to optimize and LOO is 
                              stand for leave-one-out


BEGIN

Myfunc <- function(w, n1, n2, v1, v2, g)
{  ## open  loop for main function

## DATA generation
        # generate data from group 1 and 2
        # for each group: discretise the continuous to binary
        # newdata <- combine the groups 1 and 2

## MODEL construction
     countError <- 0
        n <- nrow(newdata)

       for (k in 1:n)
       {# open loop for leave-one-out
             # construct model based on n-1 object using smoothing method
                 # classify omitted object
                countError <- countError + countE
       }   # close loop for LOO process

          Error <- countError / n     # error rate counted from LOO procedure 
         
     return(Error)           # The Average ERROR Rate from LOO procedure 

}       # close loop for Myfunc

library(stats)
  nlminb(start=0.000001, Myfunc, lower=0.000001, upper=0.999999, 
                 control=list(eval.max=100, iter.max=100))

END



could someone help me......


your concerns and guidances is highly appreciated and really100 needed...... 

Hashibah,
Statistic PhD Student

    



 ",r,,,,,,open,0,435,3,"optimization using ""nlminb"" im now performing Location Model using non-parametric smoothing to estimate the paramneters.....one of the smoothed paramater is the lamdha that i have to optimize...

  so in that case, i decide to use ""nlminb function"" to achieve it.....

  however, my programing give me the same ""$par"" value even though it was iterate 150 time and make 200 evaluation (by default)..... which is it choose ""the start value as $par"" (that is 0.000001 ...... i think, there must be something wrong with my written program.... 
  
 my programing look like:-   (note: w is the parameter that i want to optimize and LOO is 
                              stand for leave-one-out


BEGIN

Myfunc <- function(w, n1, n2, v1, v2, g)
{  ## open  loop for main function

## DATA generation
        # generate data from group 1 and 2
        # for each group: discretise the continuous to binary
        # newdata <- combine the groups 1 and 2

## MODEL construction
     countError <- 0
        n <- nrow(newdata)

       for (k in 1:n)
       {# open loop for leave-one-out
             # construct model based on n-1 object using smoothing method
                 # classify omitted object
                countError <- countError + countE
       }   # close loop for LOO process

          Error <- countError / n     # error rate counted from LOO procedure 
         
     return(Error)           # The Average ERROR Rate from LOO procedure 

}       # close loop for Myfunc

library(stats)
  nlminb(start=0.000001, Myfunc, lower=0.000001, upper=0.999999, 
                 control=list(eval.max=100, iter.max=100))

END



could someone help me......


your concerns and guidances is highly appreciated and really100 needed...... 

Hashibah,
Statistic PhD Student

    



 ",1
2593412,04/07/2010 14:47:20,142068,07/21/2009 14:53:05,14,0,Writing a script for reading many .csv files with similar filenames,"I have several .csv files with similar filenames except a numeric month (i.e. 03_data.csv, 04_data.csv, 05_data.csv, etc.) that I'd like to read into R.

I have two questions:

 - Is there a function in R similar to
   MATLAB's varname and assignin that
   will let me create/declare a variable name
   within a function or loop that will allow me to 
   read the respective .csv file - i.e.
   03_data.csv into 03_data data.frame,
   etc.? I want to write a quick loop to
   do this because the filenames are
   similar.
 - As an alternative, is it better to
   create one dataframe with the first
   file and then append the rest using a
   for loop? How would I do that?
",r,matlab,,,,,open,0,137,11,"Writing a script for reading many .csv files with similar filenames I have several .csv files with similar filenames except a numeric month (i.e. 03_data.csv, 04_data.csv, 05_data.csv, etc.) that I'd like to read into R.

I have two questions:

 - Is there a function in R similar to
   MATLAB's varname and assignin that
   will let me create/declare a variable name
   within a function or loop that will allow me to 
   read the respective .csv file - i.e.
   03_data.csv into 03_data data.frame,
   etc.? I want to write a quick loop to
   do this because the filenames are
   similar.
 - As an alternative, is it better to
   create one dataframe with the first
   file and then append the rest using a
   for loop? How would I do that?
",2
11320575,07/03/2012 23:28:19,870178,07/29/2011 23:23:11,48,9,Why doesn't position_dodge work with geom_text,Why is it so hard to get good labels for individual points for a scatterplot in ggplot2. It seems like a simple `position dodge` would accomplish 90% of good labelling.,r,ggplot2,,,,07/04/2012 19:42:32,not a real question,1,30,6,Why doesn't position_dodge work with geom_text Why is it so hard to get good labels for individual points for a scatterplot in ggplot2. It seems like a simple `position dodge` would accomplish 90% of good labelling.,2
9846253,03/23/2012 20:42:43,1289120,03/23/2012 20:24:29,1,0,micorarray data into the R CMA package,"I am trying to classify some of my microarray samples using several classification methods available through the CMA (Classifiction for MicroArray) package in R. However, I keeping getting an error in function when I try to use the GeneSelection at the end:

Error in function (classes, fdef, mtable) : unable to find an inherited method for function ""GeneSelection"", for signature ""matrix"", ""data.frame"", ""missing""

Any help would be greatly appreciated. I am still trying to learn R.

Thanks in advance.

Below are my codes:
Loading Packages

library(limma)

library(CMA)
Reading the target files

targets<-readTargets(""ClassificationSample.txt"")
Reading the data

wtfun <- function(x) { okAboveBG <- x[,""rIsWellAboveBG""]==1 & x[,""gIsWellAboveBG""]==1 okSaturated <- x[,""rIsSaturated""]==0 & x[,""gIsSaturated""]==0 okPopnOutlier <- x[,""rIsFeatPopnOL""]==0 & x[,""gIsFeatPopnOL""]==0 okNonUnifOutlier <- x[,""rIsFeatNonUnifOL""]==0 &
x[,""gIsFeatNonUnifOL""]==0 as.numeric(okAboveBG & okSaturated & okPopnOutlier & okNonUnifOutlier) }

RG <- read.maimages(targets,source=""agilent"",columns= list( R = ""rMeanSignal"", G = ""gMeanSignal"", Rb = ""rBGUsed"", Gb = ""gBGUsed"", Rb.real = ""rBGMeanSignal"", Gb.real = ""gBGMeanSignal"",logratio=""LogRatio""), annotation = c (""FeatureNum"",""Row"",""Col"",""ProbeName"",""ControlType"",""GeneName"", ""Description"",""SystematicName""))
Extract out the log ratios

array1<-RG$logratio[,1] array2<-RG$logratio[,2] array3<-RG$logratio[,3] array4<-RG$logratio[,4] array5<-RG$logratio[,5] array6<-RG$logratio[,6] array7<-RG$logratio[,7] array8<-RG$logratio[,8] array9<-RG$logratio[,9] array10<-RG$logratio[,10] array11<-RG$logratio[,11] array12<-RG$logratio[,12] array13<-RG$logratio[,13] array14<-RG$logratio[,14] array15<-RG$logratio[,15] array16<-RG$logratio[,16] array17<-RG$logratio[,17] array18<-RG$logratio[,18] array19<-RG$logratio[,19] array20<-RG$logratio[,20] array21<-RG$logratio[,21] array22<-RG$logratio[,22] array23<-RG$logratio[,23] array24<-RG$logratio[,24] array25<-RG$logratio[,25] array26<-RG$logratio[,26] array27<-RG$logratio[,27] array28<-RG$logratio[,28] array29<-RG$logratio[,29] array30<-RG$logratio[,30] array31<-RG$logratio[,31] array32<-RG$logratio[,32] array33<-RG$logratio[,33] array34<-RG$logratio[,34] array35<-RG$logratio[,35] array36<-RG$logratio[,36] array37<-RG$logratio[,37] array38<-RG$logratio[,38] array39<-RG$logratio[,39] array40<-RG$logratio[,40] array41<-RG$logratio[,41] array42<-RG$logratio[,42] array43<-RG$logratio[,43] array44<-RG$logratio[,44] array45<-RG$logratio[,45] array46<-RG$logratio[,46] array47<-RG$logratio[,47] array48<-RG$logratio[,48] array49<-RG$logratio[,49] array50<-RG$logratio[,50]
Create a DATA frame with the log ratios and the controls

RG2<-data.frame(cbind(array1,array2,array3,array4,array5,array6,array7,array8,array9,array10,array11,array12,array13,array14, array15,array16,array17,array18,array19,array20,array21,array22,array23,array24,array25,array26, array27,array28,array29,array30,array31,array32,array33,array34,array35,array36,array37,array38,array39,array40,array41,array42, array43,array44,array45,array46,array47,array48,array49,array50,controls=RG$control[,1]))
Extract out the non-control values

isGene<-RG$genes$ControlType==0

array1real<-RG2$array1[isGene] array2real<-RG2$array2[isGene] array3real<-RG2$array3[isGene] array4real<-RG2$array4[isGene] array5real<-RG2$array5[isGene] array6real<-RG2$array6[isGene] array7real<-RG2$array7[isGene] array8real<-RG2$array8[isGene] array9real<-RG2$array9[isGene] array10real<-RG2$array10[isGene] array11real<-RG2$array11[isGene] array12real<-RG2$array12[isGene] array13real<-RG2$array13[isGene] array14real<-RG2$array14[isGene] array15real<-RG2$array15[isGene] array16real<-RG2$array16[isGene] array17real<-RG2$array17[isGene] array18real<-RG2$array18[isGene] array19real<-RG2$array19[isGene] array20real<-RG2$array20[isGene] array21real<-RG2$array21[isGene] array22real<-RG2$array22[isGene] array23real<-RG2$array23[isGene] array24real<-RG2$array24[isGene] array25real<-RG2$array25[isGene] array26real<-RG2$array26[isGene] array27real<-RG2$array27[isGene] array28real<-RG2$array28[isGene] array29real<-RG2$array29[isGene] array30real<-RG2$array30[isGene] array31real<-RG2$array31[isGene] array32real<-RG2$array32[isGene] array33real<-RG2$array33[isGene] array34real<-RG2$array34[isGene] array35real<-RG2$array35[isGene] array36real<-RG2$array36[isGene] array37real<-RG2$array37[isGene] array38real<-RG2$array38[isGene] array39real<-RG2$array39[isGene] array40real<-RG2$array40[isGene] array41real<-RG2$array41[isGene] array42real<-RG2$array42[isGene] array43real<-RG2$array43[isGene] array44real<-RG2$array44[isGene] array45real<-RG2$array45[isGene] array46real<-RG2$array46[isGene] array47real<-RG2$array47[isGene] array48real<-RG2$array48[isGene] array49real<-RG2$array49[isGene] array50real<-RG2$array50[isGene]
Create a Data Frame with the non-control values, the array names are to the left of the '=' sign

RG3<-data.frame(array1=array1real, array2=array2real, array3=array3real,array4=array4real, array5=array5real, array6=array6real, array7=array7real, array8=array8real, array9=array9real, array10=array10real, array11=array11real, array12=array12real, array13=array13real, array14=array14real, array15=array15real, array16=array16real, array17=array17real, array18=array18real, array19=array19real, array20=array20real, array21=array21real, array22=array22real, array23=array23real, array24=array24real, array25=array25real, array26=array26real, array27=array27real, array28=array28real, array29=array29real, array30=array30real, array31=array31real, array32=array32real, array33=array33real, array34=array34real, array35=array35real, array36=array36real, array37=array37real, array38=array38real, array39=array39real, array40=array40real, array41=array41real, array42=array42real, array43=array43real, array44=array44real, array45=array45real, array46=array46real, array47=array47real, array48=array48real, array49=array49real, array50=array50real)
Assign Row Names

rownames(RG3)<-paste(RG$genes$FeatureNum[isGene],RG$genes$GeneName[isGene],sep=""_"")
Assign Column names

colnames(RG3)<-targets$Cy3
Using CMV

TestY<-(RG3[0,])

TestX<-as.matrix(RG3[-1,])

looRG3<-GenerateLearningsets(y=TestY, method=""LOOCV"")

varsel_loo<-GeneSelection(TestX, TestY, learningsets=looRG3, method=""f.test"")
",r,,,,,03/29/2012 06:17:41,not a real question,1,335,7,"micorarray data into the R CMA package I am trying to classify some of my microarray samples using several classification methods available through the CMA (Classifiction for MicroArray) package in R. However, I keeping getting an error in function when I try to use the GeneSelection at the end:

Error in function (classes, fdef, mtable) : unable to find an inherited method for function ""GeneSelection"", for signature ""matrix"", ""data.frame"", ""missing""

Any help would be greatly appreciated. I am still trying to learn R.

Thanks in advance.

Below are my codes:
Loading Packages

library(limma)

library(CMA)
Reading the target files

targets<-readTargets(""ClassificationSample.txt"")
Reading the data

wtfun <- function(x) { okAboveBG <- x[,""rIsWellAboveBG""]==1 & x[,""gIsWellAboveBG""]==1 okSaturated <- x[,""rIsSaturated""]==0 & x[,""gIsSaturated""]==0 okPopnOutlier <- x[,""rIsFeatPopnOL""]==0 & x[,""gIsFeatPopnOL""]==0 okNonUnifOutlier <- x[,""rIsFeatNonUnifOL""]==0 &
x[,""gIsFeatNonUnifOL""]==0 as.numeric(okAboveBG & okSaturated & okPopnOutlier & okNonUnifOutlier) }

RG <- read.maimages(targets,source=""agilent"",columns= list( R = ""rMeanSignal"", G = ""gMeanSignal"", Rb = ""rBGUsed"", Gb = ""gBGUsed"", Rb.real = ""rBGMeanSignal"", Gb.real = ""gBGMeanSignal"",logratio=""LogRatio""), annotation = c (""FeatureNum"",""Row"",""Col"",""ProbeName"",""ControlType"",""GeneName"", ""Description"",""SystematicName""))
Extract out the log ratios

array1<-RG$logratio[,1] array2<-RG$logratio[,2] array3<-RG$logratio[,3] array4<-RG$logratio[,4] array5<-RG$logratio[,5] array6<-RG$logratio[,6] array7<-RG$logratio[,7] array8<-RG$logratio[,8] array9<-RG$logratio[,9] array10<-RG$logratio[,10] array11<-RG$logratio[,11] array12<-RG$logratio[,12] array13<-RG$logratio[,13] array14<-RG$logratio[,14] array15<-RG$logratio[,15] array16<-RG$logratio[,16] array17<-RG$logratio[,17] array18<-RG$logratio[,18] array19<-RG$logratio[,19] array20<-RG$logratio[,20] array21<-RG$logratio[,21] array22<-RG$logratio[,22] array23<-RG$logratio[,23] array24<-RG$logratio[,24] array25<-RG$logratio[,25] array26<-RG$logratio[,26] array27<-RG$logratio[,27] array28<-RG$logratio[,28] array29<-RG$logratio[,29] array30<-RG$logratio[,30] array31<-RG$logratio[,31] array32<-RG$logratio[,32] array33<-RG$logratio[,33] array34<-RG$logratio[,34] array35<-RG$logratio[,35] array36<-RG$logratio[,36] array37<-RG$logratio[,37] array38<-RG$logratio[,38] array39<-RG$logratio[,39] array40<-RG$logratio[,40] array41<-RG$logratio[,41] array42<-RG$logratio[,42] array43<-RG$logratio[,43] array44<-RG$logratio[,44] array45<-RG$logratio[,45] array46<-RG$logratio[,46] array47<-RG$logratio[,47] array48<-RG$logratio[,48] array49<-RG$logratio[,49] array50<-RG$logratio[,50]
Create a DATA frame with the log ratios and the controls

RG2<-data.frame(cbind(array1,array2,array3,array4,array5,array6,array7,array8,array9,array10,array11,array12,array13,array14, array15,array16,array17,array18,array19,array20,array21,array22,array23,array24,array25,array26, array27,array28,array29,array30,array31,array32,array33,array34,array35,array36,array37,array38,array39,array40,array41,array42, array43,array44,array45,array46,array47,array48,array49,array50,controls=RG$control[,1]))
Extract out the non-control values

isGene<-RG$genes$ControlType==0

array1real<-RG2$array1[isGene] array2real<-RG2$array2[isGene] array3real<-RG2$array3[isGene] array4real<-RG2$array4[isGene] array5real<-RG2$array5[isGene] array6real<-RG2$array6[isGene] array7real<-RG2$array7[isGene] array8real<-RG2$array8[isGene] array9real<-RG2$array9[isGene] array10real<-RG2$array10[isGene] array11real<-RG2$array11[isGene] array12real<-RG2$array12[isGene] array13real<-RG2$array13[isGene] array14real<-RG2$array14[isGene] array15real<-RG2$array15[isGene] array16real<-RG2$array16[isGene] array17real<-RG2$array17[isGene] array18real<-RG2$array18[isGene] array19real<-RG2$array19[isGene] array20real<-RG2$array20[isGene] array21real<-RG2$array21[isGene] array22real<-RG2$array22[isGene] array23real<-RG2$array23[isGene] array24real<-RG2$array24[isGene] array25real<-RG2$array25[isGene] array26real<-RG2$array26[isGene] array27real<-RG2$array27[isGene] array28real<-RG2$array28[isGene] array29real<-RG2$array29[isGene] array30real<-RG2$array30[isGene] array31real<-RG2$array31[isGene] array32real<-RG2$array32[isGene] array33real<-RG2$array33[isGene] array34real<-RG2$array34[isGene] array35real<-RG2$array35[isGene] array36real<-RG2$array36[isGene] array37real<-RG2$array37[isGene] array38real<-RG2$array38[isGene] array39real<-RG2$array39[isGene] array40real<-RG2$array40[isGene] array41real<-RG2$array41[isGene] array42real<-RG2$array42[isGene] array43real<-RG2$array43[isGene] array44real<-RG2$array44[isGene] array45real<-RG2$array45[isGene] array46real<-RG2$array46[isGene] array47real<-RG2$array47[isGene] array48real<-RG2$array48[isGene] array49real<-RG2$array49[isGene] array50real<-RG2$array50[isGene]
Create a Data Frame with the non-control values, the array names are to the left of the '=' sign

RG3<-data.frame(array1=array1real, array2=array2real, array3=array3real,array4=array4real, array5=array5real, array6=array6real, array7=array7real, array8=array8real, array9=array9real, array10=array10real, array11=array11real, array12=array12real, array13=array13real, array14=array14real, array15=array15real, array16=array16real, array17=array17real, array18=array18real, array19=array19real, array20=array20real, array21=array21real, array22=array22real, array23=array23real, array24=array24real, array25=array25real, array26=array26real, array27=array27real, array28=array28real, array29=array29real, array30=array30real, array31=array31real, array32=array32real, array33=array33real, array34=array34real, array35=array35real, array36=array36real, array37=array37real, array38=array38real, array39=array39real, array40=array40real, array41=array41real, array42=array42real, array43=array43real, array44=array44real, array45=array45real, array46=array46real, array47=array47real, array48=array48real, array49=array49real, array50=array50real)
Assign Row Names

rownames(RG3)<-paste(RG$genes$FeatureNum[isGene],RG$genes$GeneName[isGene],sep=""_"")
Assign Column names

colnames(RG3)<-targets$Cy3
Using CMV

TestY<-(RG3[0,])

TestX<-as.matrix(RG3[-1,])

looRG3<-GenerateLearningsets(y=TestY, method=""LOOCV"")

varsel_loo<-GeneSelection(TestX, TestY, learningsets=looRG3, method=""f.test"")
",1
10042258,04/06/2012 10:27:43,271678,02/12/2010 08:12:28,1060,20,Generate a list of expression literals from an integer sequence,"I would like to map a sequence of integers to a sequence of expression literals in order to use the latter as tick mark labels in a plot, e.g.

    lbls <- lapply(-2:2, function(i) expression(i * pi))
    plot(...)
    axis(1, at=seq(-2,2)*pi, labels=lbls)

So far I've tried all variations of `bquote`, `substitute`, `expression` etc. that I could think of, but apparently I must have missed something.
Also, the FAQ and related SO questions & answers didn't fully solve this for me.

How would I do it correctly (I want `axis` to render `pi` as the greek letter and have `-2` ... `2` substituted for `i` in the above example)?",r,plot,expression,,,,open,0,112,10,"Generate a list of expression literals from an integer sequence I would like to map a sequence of integers to a sequence of expression literals in order to use the latter as tick mark labels in a plot, e.g.

    lbls <- lapply(-2:2, function(i) expression(i * pi))
    plot(...)
    axis(1, at=seq(-2,2)*pi, labels=lbls)

So far I've tried all variations of `bquote`, `substitute`, `expression` etc. that I could think of, but apparently I must have missed something.
Also, the FAQ and related SO questions & answers didn't fully solve this for me.

How would I do it correctly (I want `axis` to render `pi` as the greek letter and have `-2` ... `2` substituted for `i` in the above example)?",3
11490482,07/15/2012 08:21:05,1526637,07/15/2012 08:15:05,1,0,"R: Filling read.table() with the arguments 'sep', 'dec' and 'na.strings'","I'm supposed to modify the following argument in R:

> con1 <- textConnection(object = c(""id wert gruppe"",
+                                   ""10 2.7 A"",
+                                   ""11 3.82 B"",
+                                   ""15 4 B"",
+                                   ""13 -7.9 C""))
> df1 <- read.table(file = con1, header = TRUE)
> df1
  id  wert gruppe
1 10  2.70      A
2 11  3.82      B
3 15  4.00      B
4 13 -7.90      C

But whenever I insert one argument of the above, e.g. sep, the following happens to me:

Either this:
> df1 <- read.table(file = con1, header = TRUE, sep="","")
> 
> df1
  id.wert.gruppe
1       10 2.7 A
2      11 3.82 B
3         15 4 B
4      13 -7.9 C

Or this:
> df2 <- read.table(file = con1, header = TRUE, sep="","")
Fehler in read.table(file = con1, header = TRUE, sep = "","") : 
  keine Zeilen im Input verfügbar

What do I do wrong?
",r,rstudio,read.table,,,07/16/2012 14:56:59,too localized,1,310,10,"R: Filling read.table() with the arguments 'sep', 'dec' and 'na.strings' I'm supposed to modify the following argument in R:

> con1 <- textConnection(object = c(""id wert gruppe"",
+                                   ""10 2.7 A"",
+                                   ""11 3.82 B"",
+                                   ""15 4 B"",
+                                   ""13 -7.9 C""))
> df1 <- read.table(file = con1, header = TRUE)
> df1
  id  wert gruppe
1 10  2.70      A
2 11  3.82      B
3 15  4.00      B
4 13 -7.90      C

But whenever I insert one argument of the above, e.g. sep, the following happens to me:

Either this:
> df1 <- read.table(file = con1, header = TRUE, sep="","")
> 
> df1
  id.wert.gruppe
1       10 2.7 A
2      11 3.82 B
3         15 4 B
4      13 -7.9 C

Or this:
> df2 <- read.table(file = con1, header = TRUE, sep="","")
Fehler in read.table(file = con1, header = TRUE, sep = "","") : 
  keine Zeilen im Input verfügbar

What do I do wrong?
",3
11287570,07/02/2012 03:46:09,1474184,06/22/2012 07:58:56,56,0,adapting a quantstrat demo,"I am kind of new to using R and would need your help. I am trying to learn how to use quantstrat. I understand well how the demo's work but would like to modify it to my needs and have no clue how to proceed... Basicly, the demo on the basis of an indicator (moving average in my example) will buy x stocks if the signal happens and sell those x stocks when the opposite signal occurs. I would like to adapt the code so I initialy invest for example 100 $ and than following the first trade, I reinvest the 100$ + profits (or - losses)in the second trade. I continuely reinvest the money I have left ... is it possible to code this? Here is the demo code:

    library(zoo)
    library(tseries)
    library(quantmod)
    library(blotter)
    library(quantstrat)
    library(PerformanceAnalytics)

    ticker=""MRK""
    tickerY=""MRK""
    total_hist.start = as.Date(""2006-06-22"")
    total_hist.end   = as.Date(""2008-06-20"")
    total_hist = total_hist.end - total_hist.start

    currency(""USD"")
    stock(ticker,currency=""USD"",multiplier=1)
  
    getSymbols(tickerY,from=total_hist.start,to=total_hist.end,to.assign=TRUE)

    Initialisé les Portfolios et Accompt
    init.date = initDate=total_hist.start-1
    strat.name<- ""MyStrat""
    port.name <- ""MyPort""
    acct.name <- ""MyAcct""

    TradeSize = 1000
    initEq=as.numeric( TradeSize*max(Ad(get(ticker)) ) )

    port <- initPortf(port.name,ticker,initDate=init.date)
    acct <- initAcct(acct.name,portfolios=port.name, initDate=init.date, initEq=initEq)
    ords <- initOrders(portfolio=port.name,initDate=init.date)
    strat<- strategy(strat.name)

    strat<-add.indicator(strategy = strat,name =""SMA"",arguments=list(x = quote(Ad(mktdata)), n=50), label=""SMA50"")

    strat <- add.signal(strat,name=""sigCrossover"",arguments =   
    list(columns=c(""Adjusted"",""SMA50""),relationship=""gt""),label=""px.gt.SMA50"")
    strat <- add.signal(strat,name=""sigCrossover"",arguments = 
    list(columns=c(""Adjusted"",""SMA50""),relationship=""lt""),label=""px.lt.SMA50"")

    # go long when px > MA50
    strat <- add.rule(strategy = strat, name='ruleSignal', arguments = list(sigcol=""px.gt.SMA50"", sigval=TRUE,      
    orderqty=TradeSize, ordertype='market', orderside='long', pricemethod='market'), type='enter', path.dep=TRUE ) 
    #exit when px<SMA50
    strat <- add.rule(strategy = strat, name='ruleSignal', arguments = list(sigcol=""px.lt.SMA50"", sigval=TRUE,  
    orderqty='all',   ordertype='market', orderside='long', pricemethod='market'), type='exit', path.dep=TRUE) 

    out<-try(applyStrategy(strategy=strat, portfolios=port.name))

Thank you very much for the help I really appreciate it !",r,quantitative-finance,,,,07/02/2012 12:27:46,too localized,1,393,4,"adapting a quantstrat demo I am kind of new to using R and would need your help. I am trying to learn how to use quantstrat. I understand well how the demo's work but would like to modify it to my needs and have no clue how to proceed... Basicly, the demo on the basis of an indicator (moving average in my example) will buy x stocks if the signal happens and sell those x stocks when the opposite signal occurs. I would like to adapt the code so I initialy invest for example 100 $ and than following the first trade, I reinvest the 100$ + profits (or - losses)in the second trade. I continuely reinvest the money I have left ... is it possible to code this? Here is the demo code:

    library(zoo)
    library(tseries)
    library(quantmod)
    library(blotter)
    library(quantstrat)
    library(PerformanceAnalytics)

    ticker=""MRK""
    tickerY=""MRK""
    total_hist.start = as.Date(""2006-06-22"")
    total_hist.end   = as.Date(""2008-06-20"")
    total_hist = total_hist.end - total_hist.start

    currency(""USD"")
    stock(ticker,currency=""USD"",multiplier=1)
  
    getSymbols(tickerY,from=total_hist.start,to=total_hist.end,to.assign=TRUE)

    Initialisé les Portfolios et Accompt
    init.date = initDate=total_hist.start-1
    strat.name<- ""MyStrat""
    port.name <- ""MyPort""
    acct.name <- ""MyAcct""

    TradeSize = 1000
    initEq=as.numeric( TradeSize*max(Ad(get(ticker)) ) )

    port <- initPortf(port.name,ticker,initDate=init.date)
    acct <- initAcct(acct.name,portfolios=port.name, initDate=init.date, initEq=initEq)
    ords <- initOrders(portfolio=port.name,initDate=init.date)
    strat<- strategy(strat.name)

    strat<-add.indicator(strategy = strat,name =""SMA"",arguments=list(x = quote(Ad(mktdata)), n=50), label=""SMA50"")

    strat <- add.signal(strat,name=""sigCrossover"",arguments =   
    list(columns=c(""Adjusted"",""SMA50""),relationship=""gt""),label=""px.gt.SMA50"")
    strat <- add.signal(strat,name=""sigCrossover"",arguments = 
    list(columns=c(""Adjusted"",""SMA50""),relationship=""lt""),label=""px.lt.SMA50"")

    # go long when px > MA50
    strat <- add.rule(strategy = strat, name='ruleSignal', arguments = list(sigcol=""px.gt.SMA50"", sigval=TRUE,      
    orderqty=TradeSize, ordertype='market', orderside='long', pricemethod='market'), type='enter', path.dep=TRUE ) 
    #exit when px<SMA50
    strat <- add.rule(strategy = strat, name='ruleSignal', arguments = list(sigcol=""px.lt.SMA50"", sigval=TRUE,  
    orderqty='all',   ordertype='market', orderside='long', pricemethod='market'), type='exit', path.dep=TRUE) 

    out<-try(applyStrategy(strategy=strat, portfolios=port.name))

Thank you very much for the help I really appreciate it !",2
3580532,08/27/2010 00:26:23,160511,08/21/2009 03:27:54,300,3,R: read contents of text file as a query?,"Using R, I just want to read the contents of a file into a variable like:

    query <- read_file_contents('biglongquery.sql')

As to avoid putting, well, big long queries in the R script itself. I do _not_ want to read in data like CSV (e.g. `read.tables`), etc- just the raw text.",r,,,,,,open,0,51,9,"R: read contents of text file as a query? Using R, I just want to read the contents of a file into a variable like:

    query <- read_file_contents('biglongquery.sql')

As to avoid putting, well, big long queries in the R script itself. I do _not_ want to read in data like CSV (e.g. `read.tables`), etc- just the raw text.",1
9756595,03/18/2012 06:53:23,656208,03/12/2011 00:49:52,150,3,Generating different plots from arbitrary 32/64-bit input in R (ex. random integers),"I've compiled a dataset of 32-bit and 63-bit integer output from a PRNG I need to test. I would like to know how I can pass these samples to R and produce (auto)correlation and distribution plots, as well as a scatter plot. Optionally, I would really like to see a programmatic way to plot these into a 3D attractor for phase space analysis, though this could be a MATLAB question then... unless R can deal with that. I'm interested on showcasing visual detection of patterns in the PRNG algorithm.",r,matlab,random,plot,entropy,03/23/2012 12:37:54,not a real question,1,89,12,"Generating different plots from arbitrary 32/64-bit input in R (ex. random integers) I've compiled a dataset of 32-bit and 63-bit integer output from a PRNG I need to test. I would like to know how I can pass these samples to R and produce (auto)correlation and distribution plots, as well as a scatter plot. Optionally, I would really like to see a programmatic way to plot these into a 3D attractor for phase space analysis, though this could be a MATLAB question then... unless R can deal with that. I'm interested on showcasing visual detection of patterns in the PRNG algorithm.",5
2076370,01/16/2010 06:03:53,160314,08/20/2009 19:43:30,803,23,Most underused data visualization,"Histograms and scatterplots are great methods of visualizing data and the relationship between variables, but recently I have been wondering about what visualization techniques I am missing. What do you think is the most underused type of plot?

Answers should:

 1. Not be very commonly used in
    practice.
 2. Be understandable without a great deal
    of background discussion.
 3. Be applicable in many common situations.
 4. Include reproducible code to create
    an example (preferably in R). A linked image would be
    nice.

",r,plot,ggplot2,visualization,graphics,12/21/2011 14:20:29,not constructive,1,93,4,"Most underused data visualization Histograms and scatterplots are great methods of visualizing data and the relationship between variables, but recently I have been wondering about what visualization techniques I am missing. What do you think is the most underused type of plot?

Answers should:

 1. Not be very commonly used in
    practice.
 2. Be understandable without a great deal
    of background discussion.
 3. Be applicable in many common situations.
 4. Include reproducible code to create
    an example (preferably in R). A linked image would be
    nice.

",5
4566864,12/30/2010 22:47:12,363268,06/10/2010 09:11:30,145,1,Reshaping data into panel format in R,"I have a quite long and (for me) complex question. I have voting data from the Council of the European Union, where the voting behavior for each country has been coded according to a nominal scale:
   

    0: yes
    1: no
    2: Abstention
    3: no with a negative statement
    4: Abstention with a negative statement
    5: yes with a negative statement
 

The data is in the following format (see the end of the post for a dump of 20 observations from the data set):

    Country1 Country2 Country3 ... Date
    1        0        0        ... 2004-12-12
    1        2        0        ... 2003-02-14
    2        0        1        ... 2004-05-22
    ...      ...      ...      ... ...

First of all I would like to aggregate the data into monthly intervals, where for each month we have a sum of how many 0,1,2 etc there were for each country. Ideally the data should look like this:

    Month    Country   sum of 0s    sum of 1s   Sum of 2s
    January  Country1  2            0           1  
    January  Country2  4            0           0
    ...      ...       ...          ...         ...

Once this has been done I would like to put the data into panel format like this:

    Country   Month    sum of 0s   sum of 1s   sum of 2s
    Country1  January  2           0           1 
    Country1  February 0           1           3
    ...       ...      ...         ...         ...
    Country2  January  4           0           0
    Country2  February 2           2           0
    ...       ...      ...         ...         ...

I am sorry if this is a very time consuming question, but I have been playing around with aggregate, by and different apply functions forever, without being able to get the desired result. Any help will be greatly appreciated!

best, Thomas

20 observations from the data set (output from the dput() function):

        structure(list(Recitals = c(29L, 13L, 2L, 20L, 10L, 18L, 29L, 
    6L, 4L, 16L, 7L, 6L, 12L, 23L, 6L, 10L, 2L, 6L, 9L, 8L, 7L), 
        Voting_Rule = structure(c(4L, 4L, 5L, 5L, 5L, 5L, 5L, 4L, 
        4L, 4L, 5L, 4L, 5L, 4L, 5L, 4L, 4L, 5L, 4L, 4L, 4L), .Label = c(""0"", 
        ""Qualified Majority"", ""Simple Majority"", ""Unanimity"", ""Qualified majority"", 
        ""Simple majority""), class = ""factor""), Belgium = c(0L, 0L, 
        0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L), Denmark = c(0L, 0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), 
        Czech.Republic = c(0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), Germany = c(0L, 
        0L, 0L, 0L, 0L, 4L, 0L, 0L, 0L, 0L, 0L, 0L, 2L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L), Estonia = c(0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L
        ), Greece = c(0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 2L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), Spain = c(0L, 0L, 
        0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L), France = c(0L, 0L, 0L, 3L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), Ireland = c(0L, 
        0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L), Italy = c(0L, 0L, 0L, 0L, 0L, 0L, 5L, 
        0L, 0L, 0L, 0L, 0L, 2L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), 
        Cyprus = c(0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), Latvia = c(0L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L), Lithuania = c(0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), Luxembourg = c(0L, 
        0L, 0L, 0L, 0L, 4L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L), Hungary = c(0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L
        ), Malta = c(0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), Netherlands = c(0L, 
        0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L), Austria = c(0L, 0L, 0L, 1L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L
        ), Poland = c(0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), Portugal = c(0L, 
        0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L), Slovenia = c(0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L
        ), Slovakia = c(0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), Finland = c(0L, 
        0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L), Sweden = c(0L, 0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L, 2L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), 
        UK = c(0L, 0L, 0L, 0L, 0L, 0L, 5L, 0L, 0L, 0L, 5L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), Dates = structure(c(12716, 
        12716, 12716, 12674, 12674, 12698, 12705, 12724, 12738, 12738, 
        12716, 12741, 12744, 12754, 12754, 12758, 12758, 12758, 12759, 
        12759, 12759), class = ""Date"")), .Names = c(""Recitals"", ""Voting_Rule"", 
    ""Belgium"", ""Denmark"", ""Czech.Republic"", ""Germany"", ""Estonia"", 
    ""Greece"", ""Spain"", ""France"", ""Ireland"", ""Italy"", ""Cyprus"", ""Latvia"", 
    ""Lithuania"", ""Luxembourg"", ""Hungary"", ""Malta"", ""Netherlands"", 
    ""Austria"", ""Poland"", ""Portugal"", ""Slovenia"", ""Slovakia"", ""Finland"", 
    ""Sweden"", ""UK"", ""Dates""), row.names = c(752L, 753L, 762L, 774L, 
    775L, 776L, 777L, 780L, 789L, 790L, 793L, 794L, 797L, 816L, 817L, 
    818L, 819L, 820L, 824L, 825L, 826L), class = ""data.frame"")


 ",r,data,panel,aggregate,reshape,,open,0,1802,7,"Reshaping data into panel format in R I have a quite long and (for me) complex question. I have voting data from the Council of the European Union, where the voting behavior for each country has been coded according to a nominal scale:
   

    0: yes
    1: no
    2: Abstention
    3: no with a negative statement
    4: Abstention with a negative statement
    5: yes with a negative statement
 

The data is in the following format (see the end of the post for a dump of 20 observations from the data set):

    Country1 Country2 Country3 ... Date
    1        0        0        ... 2004-12-12
    1        2        0        ... 2003-02-14
    2        0        1        ... 2004-05-22
    ...      ...      ...      ... ...

First of all I would like to aggregate the data into monthly intervals, where for each month we have a sum of how many 0,1,2 etc there were for each country. Ideally the data should look like this:

    Month    Country   sum of 0s    sum of 1s   Sum of 2s
    January  Country1  2            0           1  
    January  Country2  4            0           0
    ...      ...       ...          ...         ...

Once this has been done I would like to put the data into panel format like this:

    Country   Month    sum of 0s   sum of 1s   sum of 2s
    Country1  January  2           0           1 
    Country1  February 0           1           3
    ...       ...      ...         ...         ...
    Country2  January  4           0           0
    Country2  February 2           2           0
    ...       ...      ...         ...         ...

I am sorry if this is a very time consuming question, but I have been playing around with aggregate, by and different apply functions forever, without being able to get the desired result. Any help will be greatly appreciated!

best, Thomas

20 observations from the data set (output from the dput() function):

        structure(list(Recitals = c(29L, 13L, 2L, 20L, 10L, 18L, 29L, 
    6L, 4L, 16L, 7L, 6L, 12L, 23L, 6L, 10L, 2L, 6L, 9L, 8L, 7L), 
        Voting_Rule = structure(c(4L, 4L, 5L, 5L, 5L, 5L, 5L, 4L, 
        4L, 4L, 5L, 4L, 5L, 4L, 5L, 4L, 4L, 5L, 4L, 4L, 4L), .Label = c(""0"", 
        ""Qualified Majority"", ""Simple Majority"", ""Unanimity"", ""Qualified majority"", 
        ""Simple majority""), class = ""factor""), Belgium = c(0L, 0L, 
        0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L), Denmark = c(0L, 0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), 
        Czech.Republic = c(0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), Germany = c(0L, 
        0L, 0L, 0L, 0L, 4L, 0L, 0L, 0L, 0L, 0L, 0L, 2L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L), Estonia = c(0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L
        ), Greece = c(0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 2L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), Spain = c(0L, 0L, 
        0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L), France = c(0L, 0L, 0L, 3L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), Ireland = c(0L, 
        0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L), Italy = c(0L, 0L, 0L, 0L, 0L, 0L, 5L, 
        0L, 0L, 0L, 0L, 0L, 2L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), 
        Cyprus = c(0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), Latvia = c(0L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L), Lithuania = c(0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), Luxembourg = c(0L, 
        0L, 0L, 0L, 0L, 4L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L), Hungary = c(0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L
        ), Malta = c(0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), Netherlands = c(0L, 
        0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L), Austria = c(0L, 0L, 0L, 1L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L
        ), Poland = c(0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), Portugal = c(0L, 
        0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L), Slovenia = c(0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L
        ), Slovakia = c(0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), Finland = c(0L, 
        0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L), Sweden = c(0L, 0L, 0L, 0L, 0L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L, 2L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), 
        UK = c(0L, 0L, 0L, 0L, 0L, 0L, 5L, 0L, 0L, 0L, 5L, 0L, 0L, 
        0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), Dates = structure(c(12716, 
        12716, 12716, 12674, 12674, 12698, 12705, 12724, 12738, 12738, 
        12716, 12741, 12744, 12754, 12754, 12758, 12758, 12758, 12759, 
        12759, 12759), class = ""Date"")), .Names = c(""Recitals"", ""Voting_Rule"", 
    ""Belgium"", ""Denmark"", ""Czech.Republic"", ""Germany"", ""Estonia"", 
    ""Greece"", ""Spain"", ""France"", ""Ireland"", ""Italy"", ""Cyprus"", ""Latvia"", 
    ""Lithuania"", ""Luxembourg"", ""Hungary"", ""Malta"", ""Netherlands"", 
    ""Austria"", ""Poland"", ""Portugal"", ""Slovenia"", ""Slovakia"", ""Finland"", 
    ""Sweden"", ""UK"", ""Dates""), row.names = c(752L, 753L, 762L, 774L, 
    775L, 776L, 777L, 780L, 789L, 790L, 793L, 794L, 797L, 816L, 817L, 
    818L, 819L, 820L, 824L, 825L, 826L), class = ""data.frame"")


 ",5
9805989,03/21/2012 13:51:07,84378,03/29/2009 22:24:01,8879,256,R: Set time value into data frame cell,"I'm trying to set a time value into a data frame:

    ps = data.frame(t(rep(NA, 2)))
    ps[1,1] = strptime('10:30:00', '%H:%M:%S')

but I get the error:

    provided 9 variables to replace 1 variables

since a time value is a list (?) in R it thinks I'm trying to set 9 columns, when I really just want to set the one column to that class.

What can I do to make this set properly?",r,data.frame,,,,,open,0,77,8,"R: Set time value into data frame cell I'm trying to set a time value into a data frame:

    ps = data.frame(t(rep(NA, 2)))
    ps[1,1] = strptime('10:30:00', '%H:%M:%S')

but I get the error:

    provided 9 variables to replace 1 variables

since a time value is a list (?) in R it thinks I'm trying to set 9 columns, when I really just want to set the one column to that class.

What can I do to make this set properly?",2
10570966,05/13/2012 10:20:46,1356848,04/25/2012 17:18:19,1,0,how to replace a single value in a file using R?,"I am trying to replace a value of 528.8933 to -9999 in my file

    conne <- file(""C:\\PHD\\72005ready\\latitude\\latitudefloat.bin"", ""rb"")
       a<- readBin(conne, integer(), size=2,  n=360*720, signed=F)  
               a[a == 528.8933] <- -9999

I used the code give above.But when I checked the results have not been canged.
Any help please",r,,,,,,open,0,71,11,"how to replace a single value in a file using R? I am trying to replace a value of 528.8933 to -9999 in my file

    conne <- file(""C:\\PHD\\72005ready\\latitude\\latitudefloat.bin"", ""rb"")
       a<- readBin(conne, integer(), size=2,  n=360*720, signed=F)  
               a[a == 528.8933] <- -9999

I used the code give above.But when I checked the results have not been canged.
Any help please",1
3485456,08/14/2010 22:54:33,143377,07/23/2009 03:25:55,2159,58,Useful little functions in R?,"What are the functions that you wrote, don't quite deserve a package, but you wish to share?

I will throw in some of mine:


    destring <- function(x) {
        ## convert factor to strings
        if (is.character(x)) {
            as.numeric(x)
        } else if (is.factor(x)) {
            as.numeric(levels(x))[x]
        } else if (is.numeric(x)) {
            x
        } else {
            stop(""could not convert to numeric"")
        }
    }
    
    pad0 <- function(x,mx=NULL,fill=0) {
      ## pad numeric vars to strings of specified size
      lx <- nchar(as.character(x))
      mx.calc <- max(lx,na.rm=TRUE)
      if (!is.null(mx)) {
        if (mx<mx.calc) {
          stop(""number of maxchar is too small"")
        }
      } else {
        mx <- mx.calc
      }
      px <- mx-lx
      paste(sapply(px,function(x) paste(rep(fill,x),collapse="""")),x,sep="""")
    }
    
    
    .eval <- function(evaltext,envir=sys.frame()) {
      ## evaluate a string as R code
      eval(parse(text=evaltext), envir=envir)
    }
    
    
    trim <-  function (s)
    {
      ## trim white space/tabs
      s <- sub(""^\t+"","""", s)
      s <- sub(""^ +"", """", s)
      s <- sub("" +$"", """", s)
      s
    }

",r,,,,,11/18/2011 04:48:31,not constructive,1,383,5,"Useful little functions in R? What are the functions that you wrote, don't quite deserve a package, but you wish to share?

I will throw in some of mine:


    destring <- function(x) {
        ## convert factor to strings
        if (is.character(x)) {
            as.numeric(x)
        } else if (is.factor(x)) {
            as.numeric(levels(x))[x]
        } else if (is.numeric(x)) {
            x
        } else {
            stop(""could not convert to numeric"")
        }
    }
    
    pad0 <- function(x,mx=NULL,fill=0) {
      ## pad numeric vars to strings of specified size
      lx <- nchar(as.character(x))
      mx.calc <- max(lx,na.rm=TRUE)
      if (!is.null(mx)) {
        if (mx<mx.calc) {
          stop(""number of maxchar is too small"")
        }
      } else {
        mx <- mx.calc
      }
      px <- mx-lx
      paste(sapply(px,function(x) paste(rep(fill,x),collapse="""")),x,sep="""")
    }
    
    
    .eval <- function(evaltext,envir=sys.frame()) {
      ## evaluate a string as R code
      eval(parse(text=evaltext), envir=envir)
    }
    
    
    trim <-  function (s)
    {
      ## trim white space/tabs
      s <- sub(""^\t+"","""", s)
      s <- sub(""^ +"", """", s)
      s <- sub("" +$"", """", s)
      s
    }

",1
5945932,05/10/2011 06:11:18,680958,03/28/2011 20:59:41,1,0,comparing a huge vector to a fixed number,"I want assess how many components of  v=rnorm(10^8) are less than or equal to 0.5. So I wrote this 
v=rnorm(10^8)

sum(v<=0.5)

Unfortunately, I'm getting this message

> v=rnorm(10^8)
Error: cannot allocate vector of size 762.9 Mb
> sum(v<=0.5)
Error: object 'v' not found
R(3446,0xa046b540) malloc: *** mmap(size=800002048) failed (error code=12)
*** error: can't allocate region
*** set a breakpoint in malloc_error_break to debug
R(3446,0xa046b540) malloc: *** mmap(size=800002048) failed (error code=12)
*** error: can't allocate region
*** set a breakpoint in malloc_error_break to debug

Is there any way to do this given that I can't reduce the size of vector v?
Thanks",r,,,,,,open,0,88,8,"comparing a huge vector to a fixed number I want assess how many components of  v=rnorm(10^8) are less than or equal to 0.5. So I wrote this 
v=rnorm(10^8)

sum(v<=0.5)

Unfortunately, I'm getting this message

> v=rnorm(10^8)
Error: cannot allocate vector of size 762.9 Mb
> sum(v<=0.5)
Error: object 'v' not found
R(3446,0xa046b540) malloc: *** mmap(size=800002048) failed (error code=12)
*** error: can't allocate region
*** set a breakpoint in malloc_error_break to debug
R(3446,0xa046b540) malloc: *** mmap(size=800002048) failed (error code=12)
*** error: can't allocate region
*** set a breakpoint in malloc_error_break to debug

Is there any way to do this given that I can't reduce the size of vector v?
Thanks",1
10432202,05/03/2012 13:16:20,1046428,11/14/2011 21:37:56,42,0,Output function results to a vector,"I have created a function to call a function on each row in a dataset. I would like to have the output as a vector. As you can see below the function outputs the results to the screen, but I cannot figure out how to redirect the output to a vector that I can use outside the function. 
    
    n_markers <- nrow(data)
    p_values <-rep(0, n_markers)
    
    test_markers <- function()
       {
       for (i in 1:n_markers)
          {
          hets  <- data[i, 2]
          hom_1 <- data[i, 3]
          hom_2 <- data[i, 4]   
          p_values[i] <- SNPHWE(hets, hom_1, hom_2)
          }
          return(p_values)
       }
   
    test_markers()",r,function,vector,output,,,open,0,203,6,"Output function results to a vector I have created a function to call a function on each row in a dataset. I would like to have the output as a vector. As you can see below the function outputs the results to the screen, but I cannot figure out how to redirect the output to a vector that I can use outside the function. 
    
    n_markers <- nrow(data)
    p_values <-rep(0, n_markers)
    
    test_markers <- function()
       {
       for (i in 1:n_markers)
          {
          hets  <- data[i, 2]
          hom_1 <- data[i, 3]
          hom_2 <- data[i, 4]   
          p_values[i] <- SNPHWE(hets, hom_1, hom_2)
          }
          return(p_values)
       }
   
    test_markers()",4
8533459,12/16/2011 11:20:49,1063530,11/24/2011 08:41:05,48,3,Error when installing ggplot2 r-package developer version from github,"I encounter an error when installing an r-package (`ggplot2`) from **github**. unfortunately i wasn't able to find any documentation (which might well be my own incompetence)
   
    require(devtools)
    install_github(""ggplot2"")
       Installing ggplot2 from hadley
       Installing ggplot2
      * checking for file 'C:\Users\Sebastian\AppData\Local\Temp\RtmpcjooIC\hadley-     ggplot2-      bb3ee41/DESCRIPTION' ... OK
      * preparing 'ggplot2':
      * checking DESCRIPTION meta-information ... OK
      * checking for LF line-endings in source and make files
      * checking for empty or unneeded directories
      * looking to see if a 'data/datalist' file should be added
      * building 'ggplot2_0.9.0.tar.gz'
       ERROR
       packaging into .tar.gz failed
      Error: Command failed (1)
      Additional: warning:
      Execution of Command '""C:/PROGRA~1/R/R-214~1.0/bin/x64/R"" CMD build ""C:\Users\Sebastian\AppData\Local\Temp\RtmpcjooIC\hadley-ggplot2-bb3ee41"" --no-manual --no-vignettes' resultet in status 1 

The last two lines I translated on my own - so I hope this is correct. Do you have any hint for me? (Additional info: I would like to install the developer version in order to mitigate a bug in the `scale_date()` function which is closely related to [this][1] post) Thanks a lot in advance!


  [1]: http://stackoverflow.com/q/8522686/1063530",r,github,ggplot2,,,12/17/2011 18:11:47,off topic,1,257,9,"Error when installing ggplot2 r-package developer version from github I encounter an error when installing an r-package (`ggplot2`) from **github**. unfortunately i wasn't able to find any documentation (which might well be my own incompetence)
   
    require(devtools)
    install_github(""ggplot2"")
       Installing ggplot2 from hadley
       Installing ggplot2
      * checking for file 'C:\Users\Sebastian\AppData\Local\Temp\RtmpcjooIC\hadley-     ggplot2-      bb3ee41/DESCRIPTION' ... OK
      * preparing 'ggplot2':
      * checking DESCRIPTION meta-information ... OK
      * checking for LF line-endings in source and make files
      * checking for empty or unneeded directories
      * looking to see if a 'data/datalist' file should be added
      * building 'ggplot2_0.9.0.tar.gz'
       ERROR
       packaging into .tar.gz failed
      Error: Command failed (1)
      Additional: warning:
      Execution of Command '""C:/PROGRA~1/R/R-214~1.0/bin/x64/R"" CMD build ""C:\Users\Sebastian\AppData\Local\Temp\RtmpcjooIC\hadley-ggplot2-bb3ee41"" --no-manual --no-vignettes' resultet in status 1 

The last two lines I translated on my own - so I hope this is correct. Do you have any hint for me? (Additional info: I would like to install the developer version in order to mitigate a bug in the `scale_date()` function which is closely related to [this][1] post) Thanks a lot in advance!


  [1]: http://stackoverflow.com/q/8522686/1063530",3
2710609,04/26/2010 01:04:57,325630,04/26/2010 01:04:57,1,0,Power Analysis in [R] for Two-Way Anova,"I am trying to calculate the necessary sample size for a 2x2 factorial design. I have two questions.

1) I am using the package pwr and the one way anova function to calculate the necessary sample size using the following code  
    
    pwr.anova.test(k = , n = , f = , sig.level = , power = )

However, I would like to look at two way anova, since this is more efficient at estimating group means than one way anova.  There is no two-way anova function that I could find.  Is there a package or routine in [R] to do this?

2) Moreover, am I safe in assuming that since I am using a one-way anova power calculations, that the sample size will be more conservative (i.e. larger)?",r,power,analysis,anova,,04/27/2010 01:46:35,off topic,1,135,7,"Power Analysis in [R] for Two-Way Anova I am trying to calculate the necessary sample size for a 2x2 factorial design. I have two questions.

1) I am using the package pwr and the one way anova function to calculate the necessary sample size using the following code  
    
    pwr.anova.test(k = , n = , f = , sig.level = , power = )

However, I would like to look at two way anova, since this is more efficient at estimating group means than one way anova.  There is no two-way anova function that I could find.  Is there a package or routine in [R] to do this?

2) Moreover, am I safe in assuming that since I am using a one-way anova power calculations, that the sample size will be more conservative (i.e. larger)?",4
9778232,03/19/2012 21:29:00,1000343,10/18/2011 03:41:52,2352,95,ggplot unexplained outcome,"I started making a reproducible example to ask another question and can't even get past that.  Anyway I am attempting to plot categorical data into a faceted bar plot.  So I made my own data set using CO3 (code at bottom).  Just plotting x by itself seems normal:
![enter image description here][1]

but then gets funky when I try to facet.  Showing everything is all equal. 
![enter image description here][2]

That doesn't make sense as it would indicate every sub group had equal proportions of the out come that isn't evidenced by an `ftable` of the data:


                       Type Quebec Mississippi
    outcome Treatment                         
    none    nonchilled           7           6
            chilled              4           7
    some    nonchilled           6           4
            chilled              5           5
    lots    nonchilled           5           4
            chilled              6           3
    tons    nonchilled           3           7
            chilled              6           6

**What am I doing wrong?**

    library(ggplot2)
    set.seed(10)
    CO3 <- data.frame(CO2[, 2:3], outcome=factor(sample(c('none', 'some', 'lots', 'tons'), 
               nrow(CO2), rep=T), levels=c('none', 'some', 'lots', 'tons')))
    CO3
    x <- ggplot(CO3, aes(x=outcome)) + geom_bar(aes(x=outcome))
    x
    x  + facet_grid(Treatment~., margins=TRUE)
    
    with(CO3, ftable(outcome, Treatment, Type))


  [1]: http://i.stack.imgur.com/HCG35.png
  [2]: http://i.stack.imgur.com/lK7Qi.png
",r,ggplot2,,,,,open,0,507,3,"ggplot unexplained outcome I started making a reproducible example to ask another question and can't even get past that.  Anyway I am attempting to plot categorical data into a faceted bar plot.  So I made my own data set using CO3 (code at bottom).  Just plotting x by itself seems normal:
![enter image description here][1]

but then gets funky when I try to facet.  Showing everything is all equal. 
![enter image description here][2]

That doesn't make sense as it would indicate every sub group had equal proportions of the out come that isn't evidenced by an `ftable` of the data:


                       Type Quebec Mississippi
    outcome Treatment                         
    none    nonchilled           7           6
            chilled              4           7
    some    nonchilled           6           4
            chilled              5           5
    lots    nonchilled           5           4
            chilled              6           3
    tons    nonchilled           3           7
            chilled              6           6

**What am I doing wrong?**

    library(ggplot2)
    set.seed(10)
    CO3 <- data.frame(CO2[, 2:3], outcome=factor(sample(c('none', 'some', 'lots', 'tons'), 
               nrow(CO2), rep=T), levels=c('none', 'some', 'lots', 'tons')))
    CO3
    x <- ggplot(CO3, aes(x=outcome)) + geom_bar(aes(x=outcome))
    x
    x  + facet_grid(Treatment~., margins=TRUE)
    
    with(CO3, ftable(outcome, Treatment, Type))


  [1]: http://i.stack.imgur.com/HCG35.png
  [2]: http://i.stack.imgur.com/lK7Qi.png
",2
7094674,08/17/2011 14:38:08,887651,08/10/2011 10:03:05,13,0,TTR library for R-cran has old data,"I'm checking the market capital value of the nasdaq stocks and i see strange result with TTR library `stockSymbols()`
It seems have old result, do they update the database each X Weeks ?",r,,,,,08/17/2011 15:47:43,off topic,1,32,7,"TTR library for R-cran has old data I'm checking the market capital value of the nasdaq stocks and i see strange result with TTR library `stockSymbols()`
It seems have old result, do they update the database each X Weeks ?",1
9590134,03/06/2012 19:02:20,313163,04/09/2010 20:31:46,1715,45,Why is knitr unable to find framed.sty or kpsewhich?,"    knit('test2.rnw')
    
    
    processing file: test2.rnw
      |>>>>>>>>>>>>>                                                    |  20%
      |>>>>>>>>>>>>>>>>>>>>>>>>>>                                       |  40%
    label: setup (with options) 
    List of 2
     $ include: logi FALSE
     $ cache  : logi FALSE
    
    sh: kpsewhich: command not found
      |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                          |  60%
      |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>             |  80%
      |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>| 100%
      ordinary text without R code
    
    
    output file: /Users/xxx/Desktop/R_meetup/documentation/test2.tex
    
    Warning message:
    In test_latex_pkg(""framed"", system.file(""misc"", ""framed.sty"", package = ""knitr"")) :
      unable to find LaTeX package 'framed'; will use a copy from knitr
     >

I'm on OSX Lion 10.7.3

Path for kpsewhich:

       $ which kpsewhich
        /usr/texbin/kpsewhich


    $ which pdflatex
    /usr/texbin/pdflatex

Both paths are in my `.bash_profile` in my `home` directory. I do have the framed package in my Tex install. 

    PATH=$PATH:/usr/texbin/pdflatex
    PATH=$PATH:/usr/texbin/kpsewhich
    export PATH

",r,latex,knitr,,,,open,0,364,9,"Why is knitr unable to find framed.sty or kpsewhich?     knit('test2.rnw')
    
    
    processing file: test2.rnw
      |>>>>>>>>>>>>>                                                    |  20%
      |>>>>>>>>>>>>>>>>>>>>>>>>>>                                       |  40%
    label: setup (with options) 
    List of 2
     $ include: logi FALSE
     $ cache  : logi FALSE
    
    sh: kpsewhich: command not found
      |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                          |  60%
      |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>             |  80%
      |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>| 100%
      ordinary text without R code
    
    
    output file: /Users/xxx/Desktop/R_meetup/documentation/test2.tex
    
    Warning message:
    In test_latex_pkg(""framed"", system.file(""misc"", ""framed.sty"", package = ""knitr"")) :
      unable to find LaTeX package 'framed'; will use a copy from knitr
     >

I'm on OSX Lion 10.7.3

Path for kpsewhich:

       $ which kpsewhich
        /usr/texbin/kpsewhich


    $ which pdflatex
    /usr/texbin/pdflatex

Both paths are in my `.bash_profile` in my `home` directory. I do have the framed package in my Tex install. 

    PATH=$PATH:/usr/texbin/pdflatex
    PATH=$PATH:/usr/texbin/kpsewhich
    export PATH

",3
10254059,04/20/2012 21:52:35,1322919,04/10/2012 00:03:34,23,0,"R help: determine interval, binning a number","I'm trying to ""bin"" some randomly generated numbers between an interval defined between to adjacent values within a vector of values I previously have.  So essentially, I have the following:

vectorA containing 101 values ranging from 101 to 0.
I generate a random number called x.  Now I'd like to see which interval, between two numbers adjacent to each other in vectorA does it belong to?  Finally, once it has found the interval, I return those two values.  

I have an if statement going; if(x < vectorA[k] | x > vectorA[k+1]), under a for loop so the if statement can go through all the increments of vectorA.

I want to stay away from R's breaks method because I need to grab the actual bin interval values and use them to compute something.

Any suggestions would be helpful.

Thanks",r,if-statement,for-loop,between,binning,,open,0,137,7,"R help: determine interval, binning a number I'm trying to ""bin"" some randomly generated numbers between an interval defined between to adjacent values within a vector of values I previously have.  So essentially, I have the following:

vectorA containing 101 values ranging from 101 to 0.
I generate a random number called x.  Now I'd like to see which interval, between two numbers adjacent to each other in vectorA does it belong to?  Finally, once it has found the interval, I return those two values.  

I have an if statement going; if(x < vectorA[k] | x > vectorA[k+1]), under a for loop so the if statement can go through all the increments of vectorA.

I want to stay away from R's breaks method because I need to grab the actual bin interval values and use them to compute something.

Any suggestions would be helpful.

Thanks",5
4445293,12/14/2010 23:20:10,199217,10/29/2009 20:28:01,554,8,R CMD build is stuck on a good function - because of escaped chars '\\#'?,"I have a test function 'foo.R' in my package 'PKG'; foo.R consists of:

    foo <- function (filename, n)
      text <- scan(file = filename, what = ""character"")
      if (n==1) text <- gsub(""\\#GGG"", '\\#', text)
      if (n>1)  text <- gsub(""\\#GGG"", '', text)
      writeLines(text, con = 'newfn.R')
    }

The intent of foo is to either uncomment lines by replacing ""#GGG"" with """" when n>1 or to leave the line commented if n == 1

The minimal code required to produce this error is:

    foo <- function (string)
        gsub(""\\#GGG"", '', string)
    }

However, when I run `R CMD build PKG` I get the following error:

    Error in parse(outFile) : 
      /tmp/RtmpLbFQF0/R.INSTALL2edd9a07/PKG/R/foo.R:3:1: unexpected '}'
    2:   gsub(""\\#GGG"", '\\#', string)
    3: }
       ^

I presume that this is because the R CMD build is reading the '\\#' as a comment and therefore thinks that the braces need to be closed, but how can I get around this?

Thanks!",r,text,gsub,,,12/16/2010 22:24:36,not a real question,1,208,15,"R CMD build is stuck on a good function - because of escaped chars '\\#'? I have a test function 'foo.R' in my package 'PKG'; foo.R consists of:

    foo <- function (filename, n)
      text <- scan(file = filename, what = ""character"")
      if (n==1) text <- gsub(""\\#GGG"", '\\#', text)
      if (n>1)  text <- gsub(""\\#GGG"", '', text)
      writeLines(text, con = 'newfn.R')
    }

The intent of foo is to either uncomment lines by replacing ""#GGG"" with """" when n>1 or to leave the line commented if n == 1

The minimal code required to produce this error is:

    foo <- function (string)
        gsub(""\\#GGG"", '', string)
    }

However, when I run `R CMD build PKG` I get the following error:

    Error in parse(outFile) : 
      /tmp/RtmpLbFQF0/R.INSTALL2edd9a07/PKG/R/foo.R:3:1: unexpected '}'
    2:   gsub(""\\#GGG"", '\\#', string)
    3: }
       ^

I presume that this is because the R CMD build is reading the '\\#' as a comment and therefore thinks that the braces need to be closed, but how can I get around this?

Thanks!",3
10256503,04/21/2012 05:30:43,1270695,03/15/2012 04:34:02,391,16,"Function for median similar to ""which.max"" and ""which.min"" / Extracting median rows from a data.frame","I occasionally need to extract specific rows from a data.frame based on values from one of the variables. <code>R</code> has built-in functions for maximum (<code>which.max()</code>) and minimum (<code>which.min()</code>) that allow me to easily extract those rows.

Is there an equivalent for median? Or is my best bet to just write my own function?

Here's an example data.frame and how I would use <code>which.max()</code> and <code>which.min()</code>:


    set.seed(1) # so you can reproduce this example
    dat = data.frame(V1 = 1:10, V2 = rnorm(10), V3 = rnorm(10), 
                     V4 = sample(1:20, 10, replace=T))
    
    # To return the first row, which contains the max value in V4
    dat[which.max(dat$V4), ]
    # To return the seventh row, which contains the min value in V4
    dat[which.min(dat$V4), ]

For this particular example, since there are an even number of observations, I would need to have two rows returned, in this case, rows 2 and 10.",r,data.frame,subset,,,,open,0,186,15,"Function for median similar to ""which.max"" and ""which.min"" / Extracting median rows from a data.frame I occasionally need to extract specific rows from a data.frame based on values from one of the variables. <code>R</code> has built-in functions for maximum (<code>which.max()</code>) and minimum (<code>which.min()</code>) that allow me to easily extract those rows.

Is there an equivalent for median? Or is my best bet to just write my own function?

Here's an example data.frame and how I would use <code>which.max()</code> and <code>which.min()</code>:


    set.seed(1) # so you can reproduce this example
    dat = data.frame(V1 = 1:10, V2 = rnorm(10), V3 = rnorm(10), 
                     V4 = sample(1:20, 10, replace=T))
    
    # To return the first row, which contains the max value in V4
    dat[which.max(dat$V4), ]
    # To return the seventh row, which contains the min value in V4
    dat[which.min(dat$V4), ]

For this particular example, since there are an even number of observations, I would need to have two rows returned, in this case, rows 2 and 10.",3
5877671,05/04/2011 01:58:44,697363,04/07/2011 18:20:01,16,2,ODS functionality in R,"Is there any functionality in R which is just like that of ods functionality in SAS.

Rgds,
A",r,,,,,05/04/2011 13:05:47,not a real question,1,16,4,"ODS functionality in R Is there any functionality in R which is just like that of ods functionality in SAS.

Rgds,
A",1
7561963,09/26/2011 22:11:30,309284,04/05/2010 14:55:02,49,5,make .combine function scaleable,"I am trying to use foreach and am having problems making the .combine function scalable.  For example, here is a simple combine function
     
    MyComb <- function(part1,part2){
              xs <- c(part1$x,part2$x)
              ys <- c(part1$y,part2$y)
              return(list(xs,ys))
              }

When I use this function to combine a foreach statement with an iterator other than 2 it returns it incorrectly.  For example this works:

       x = foreach(i=1:2,.combine=MyComb) %dopar% list(""x""=i*2,""y""=i*3)

But not this:

     x = foreach(i=1:3,.combine=MyComb) %dopar% list(""x""=i*2,""y""=i*3)

Is there a way to generalize the combine function to make it scalable to n iterations? ",r,foreach,domc,,,,open,0,157,4,"make .combine function scaleable I am trying to use foreach and am having problems making the .combine function scalable.  For example, here is a simple combine function
     
    MyComb <- function(part1,part2){
              xs <- c(part1$x,part2$x)
              ys <- c(part1$y,part2$y)
              return(list(xs,ys))
              }

When I use this function to combine a foreach statement with an iterator other than 2 it returns it incorrectly.  For example this works:

       x = foreach(i=1:2,.combine=MyComb) %dopar% list(""x""=i*2,""y""=i*3)

But not this:

     x = foreach(i=1:3,.combine=MyComb) %dopar% list(""x""=i*2,""y""=i*3)

Is there a way to generalize the combine function to make it scalable to n iterations? ",3
9923028,03/29/2012 09:54:59,1300432,03/29/2012 09:48:37,1,0,Future prediction on past data analysis,"being a programmer, when i look at the movie moneyball its shows the strength of stats practical usage. Taking the inspiration im reading now Head First's Data Analysis in a hope that lately i may, by analyzing some past data e.g; by looking at past 10years exam papers of a particular subject can predict the likely questions to be asked in the upcoming exams. Plz validate my path and im afraid it may take a long time for me, any suggestions are always welcomed.",r,statistics,,,,03/29/2012 12:28:43,not a real question,1,84,6,"Future prediction on past data analysis being a programmer, when i look at the movie moneyball its shows the strength of stats practical usage. Taking the inspiration im reading now Head First's Data Analysis in a hope that lately i may, by analyzing some past data e.g; by looking at past 10years exam papers of a particular subject can predict the likely questions to be asked in the upcoming exams. Plz validate my path and im afraid it may take a long time for me, any suggestions are always welcomed.",2
8302119,11/28/2011 20:48:56,1030771,11/05/2011 04:58:12,65,0,"Significance testing in R, determining if the proportion in one column is significantly different from the other column within the single variable","I'm sure this is an easy command in R, but for some reason, I'm having trouble finding a solution.

I'm trying to run a bunch of crosstabs (using the table() command) in R, and each tab has two columns (treatment and no treatment).  I would like to know if the difference between the columns are significantly different for each other for all rows (the rows are a handful of answer choices from a survey).  I'm not interested in overall significance, only within the crosstab comparing treatment vs. no treatment.

This type of analysis is very easy in SPSS (link below to illustrate what I'm talking about), but I can't seem to get it working in R.  Do you know I can do this?

http://help.vovici.net/robohelp/robohelp/server/general/projects_fhpro/survey_workbench_MX/Significance_testing.htm

Thanks!",r,crosstab,significance,,,,open,0,124,22,"Significance testing in R, determining if the proportion in one column is significantly different from the other column within the single variable I'm sure this is an easy command in R, but for some reason, I'm having trouble finding a solution.

I'm trying to run a bunch of crosstabs (using the table() command) in R, and each tab has two columns (treatment and no treatment).  I would like to know if the difference between the columns are significantly different for each other for all rows (the rows are a handful of answer choices from a survey).  I'm not interested in overall significance, only within the crosstab comparing treatment vs. no treatment.

This type of analysis is very easy in SPSS (link below to illustrate what I'm talking about), but I can't seem to get it working in R.  Do you know I can do this?

http://help.vovici.net/robohelp/robohelp/server/general/projects_fhpro/survey_workbench_MX/Significance_testing.htm

Thanks!",3
11079587,06/18/2012 08:54:08,1048408,11/15/2011 20:20:35,312,1,Error checking an inputed value,"I've got a function that uses readline to ask people to enter in data. But I'm at a loss as to the best method to insure that the data entered meet my criteria. I'm figuring ""if"" statements may be the best way to go to check for errors, but I'm not sure how to incorporate them. My attempt at using them is obviously flawed (see below).

As a simple example, 2 of the most likely problems I'm going to run into would be I'd like to insure that at least some value is entered in for x (and if a value is entered for x it is a number) and that V1 and V2 contain the same number of values.

    fun<-function(){
	T<-readline(""What is x"" )
	
	if(T=="""" | typeof(x)!=numeric)
	{print(""Input non-aceptable"")
	T<-readline(""What is x "")}
	
	else
	
	V<-readline(""Enter 4 values"" )
	V2<-readline(""Enter 4 more values "")
	
	if(length(V1)!=length(V2))
	{print(""V1 & V2 do not contain equal # of values"")
        V<-readline(""Enter 4 values "")
	    V<-readline(""Enter 4 more values "")}
	
	 else

    T<-as.numeric(T)
    V<-as.numeric(V)
    V2<-as.numeric(V2)
    return(list(x,V1,V2)
}

As you can see, my hope is to try and spot potential errors before they cause an actual error to happen, and then to give the person an opportunity to re-enter the data. If ""if"" statements are the way to go, can I get some help on using the correctly? 

Thanks!",r,,,,,,open,0,234,5,"Error checking an inputed value I've got a function that uses readline to ask people to enter in data. But I'm at a loss as to the best method to insure that the data entered meet my criteria. I'm figuring ""if"" statements may be the best way to go to check for errors, but I'm not sure how to incorporate them. My attempt at using them is obviously flawed (see below).

As a simple example, 2 of the most likely problems I'm going to run into would be I'd like to insure that at least some value is entered in for x (and if a value is entered for x it is a number) and that V1 and V2 contain the same number of values.

    fun<-function(){
	T<-readline(""What is x"" )
	
	if(T=="""" | typeof(x)!=numeric)
	{print(""Input non-aceptable"")
	T<-readline(""What is x "")}
	
	else
	
	V<-readline(""Enter 4 values"" )
	V2<-readline(""Enter 4 more values "")
	
	if(length(V1)!=length(V2))
	{print(""V1 & V2 do not contain equal # of values"")
        V<-readline(""Enter 4 values "")
	    V<-readline(""Enter 4 more values "")}
	
	 else

    T<-as.numeric(T)
    V<-as.numeric(V)
    V2<-as.numeric(V2)
    return(list(x,V1,V2)
}

As you can see, my hope is to try and spot potential errors before they cause an actual error to happen, and then to give the person an opportunity to re-enter the data. If ""if"" statements are the way to go, can I get some help on using the correctly? 

Thanks!",1
7843741,10/21/2011 00:13:56,391399,07/14/2010 09:37:37,450,11,R: abbrivations and functions in preperation of programming contest,"
I am participating in a big programming competition tomorrow where I use R.

Time is the main factor (only 2 hours for 7 coding problems). 
The problems are very mathematics related.

1)
I would like to write ""f"" instead of ""function"" when I define a function.
This can be done and I had the code to do so, but I lost it and cannot find it.

2)
Where do I find sin() functions for degrees input, not radian?

3)
(optional) Is there any algorithm specific task view or libraries.

4) Any tip for a programming contest?

I prepared the following cheat sheet for the contest:
http://pastebin.com/h5xDLhvg

I looked at bnlearn and nnet, maybe a graph library with graph algorithms can also be handy.

 ",r,,,,,10/21/2011 09:10:08,not constructive,1,112,9,"R: abbrivations and functions in preperation of programming contest 
I am participating in a big programming competition tomorrow where I use R.

Time is the main factor (only 2 hours for 7 coding problems). 
The problems are very mathematics related.

1)
I would like to write ""f"" instead of ""function"" when I define a function.
This can be done and I had the code to do so, but I lost it and cannot find it.

2)
Where do I find sin() functions for degrees input, not radian?

3)
(optional) Is there any algorithm specific task view or libraries.

4) Any tip for a programming contest?

I prepared the following cheat sheet for the contest:
http://pastebin.com/h5xDLhvg

I looked at bnlearn and nnet, maybe a graph library with graph algorithms can also be handy.

 ",1
5605090,04/09/2011 12:42:42,699946,04/09/2011 12:42:42,1,0,neuralnet package fitted values issue,"The problem I've encountered after trying to train neural networks isn't a new one : The fitted values I'm getting are all the same. Here's some oversimplified code as an example:  
`a<-c( 123, 223, 234, 226, 60)  
b<-c(60, 90, 53, 54, 91)  
d<-c(40,100,207,290,241)  
q<-cbind(a,b,d)  
nn<-neuralnet(a~b+d,data=q,hidden=2,threshold=0.01,err.fc=""sse"")  
nn$net.result`

Previous answers I have stumbled upon suggest using nnet instead. I am getting the same results though, unless I set the decay argument to a value not equal to 0. Instead of blindly using the decay option, just because it seems to ""work"" though, I would appreciate understanding what goes wrong with my neuralnet model to begin with.

Thank you in advance,  
Nishi",r,,,,,,open,0,114,5,"neuralnet package fitted values issue The problem I've encountered after trying to train neural networks isn't a new one : The fitted values I'm getting are all the same. Here's some oversimplified code as an example:  
`a<-c( 123, 223, 234, 226, 60)  
b<-c(60, 90, 53, 54, 91)  
d<-c(40,100,207,290,241)  
q<-cbind(a,b,d)  
nn<-neuralnet(a~b+d,data=q,hidden=2,threshold=0.01,err.fc=""sse"")  
nn$net.result`

Previous answers I have stumbled upon suggest using nnet instead. I am getting the same results though, unless I set the decay argument to a value not equal to 0. Instead of blindly using the decay option, just because it seems to ""work"" though, I would appreciate understanding what goes wrong with my neuralnet model to begin with.

Thank you in advance,  
Nishi",1
8109610,11/13/2011 04:29:11,629000,02/22/2011 19:53:21,277,0,Methods for making R plots look more professional?,"I've been poking around with R graphical parameters trying to make my plots look a little more professional (e.g., `las=1`, `bty=""n""` usually help). But not quite there. Started playing with `tikzDevice`. A huge improvement! Amazing how much better things look when the font sizes and styles in the figure match those of the surrounding document. Still, not quite there. What I'm ultimately looking for are those professional gradient shading, rounded corners, and shadow effects found in MS Excel plots. I know they're probably considered chart junk, but I like them. They're just nice looking.

Q: How can I get these effects into my R plots? Do people usually just export to Inkscape and doodle over there? It would be nice if there were a literate programming approach. Is there an R package that handles these effects outright? ",r,graphics,,,,11/15/2011 09:32:33,not a real question,1,137,8,"Methods for making R plots look more professional? I've been poking around with R graphical parameters trying to make my plots look a little more professional (e.g., `las=1`, `bty=""n""` usually help). But not quite there. Started playing with `tikzDevice`. A huge improvement! Amazing how much better things look when the font sizes and styles in the figure match those of the surrounding document. Still, not quite there. What I'm ultimately looking for are those professional gradient shading, rounded corners, and shadow effects found in MS Excel plots. I know they're probably considered chart junk, but I like them. They're just nice looking.

Q: How can I get these effects into my R plots? Do people usually just export to Inkscape and doodle over there? It would be nice if there were a literate programming approach. Is there an R package that handles these effects outright? ",2
4222800,11/19/2010 06:54:47,373908,06/23/2010 05:52:19,294,3,the line that cause odfWeave-generated odt file to be considered as corrupted,"I have tried `odfWeave` a bit, nice app. But at the very beginning, even I tried exactly as the example provided in the manual, I can't generate any file.

I have searched a bit inside the odt file and found a statement inside the `content.xml` cause the problem, it is:

`text:bullet-char=""<U+25CF>""`

what is the statement actually? how can I fix that in R? Thanks!",r,odf,,,,,open,0,61,12,"the line that cause odfWeave-generated odt file to be considered as corrupted I have tried `odfWeave` a bit, nice app. But at the very beginning, even I tried exactly as the example provided in the manual, I can't generate any file.

I have searched a bit inside the odt file and found a statement inside the `content.xml` cause the problem, it is:

`text:bullet-char=""<U+25CF>""`

what is the statement actually? how can I fix that in R? Thanks!",2
6909831,08/02/2011 09:22:41,373908,06/23/2010 05:52:19,722,6,how to convert date in STATA?,"I have a date with a format like below, and I use R to convert it from string to date

    date <- ""20 Nov 2010 21:44:00:000""
    strptime(date,""%d %b %Y %H:%M:%S"")

I want to do it in STATA, but how? Thanks.",r,stata,,,,,open,0,45,6,"how to convert date in STATA? I have a date with a format like below, and I use R to convert it from string to date

    date <- ""20 Nov 2010 21:44:00:000""
    strptime(date,""%d %b %Y %H:%M:%S"")

I want to do it in STATA, but how? Thanks.",2
3529996,08/20/2010 10:25:50,74658,03/06/2009 11:15:21,925,32,Adding an exponential geom_smooth in ggplot2 / R,"I am trying to produce some example graphics using ggplot2, and one of the examples I picked was the [birthday problem][1], here using code 'borrowed' from a Revolution [computing presentation][2] at Oscon.

    birthday<-function(n){
    	ntests<-1000
    	pop<-1:365
    	anydup<-function(i){
    		any(duplicated(sample(pop,n,replace=TRUE)))
    		}
    	sum(sapply(seq(ntests), anydup))/ntests
    	}

    x<-data.frame(x=rep(1:100, each=5))	
    x<-ddply(x, .(x), function(df) {return(data.frame(x=df$x, prob=birthday(df$x)))})
    birthdayplot<-ggplot(x, aes(x, prob))+
    		geom_point()+geom_smooth()+
    		theme_bw()+
    		opts(title = ""Probability that at least two people share a birthday in a random group"")+
    		labs(x=""Size of Group"", y=""Probability"")

Here my graph is what I would describe as exponential, but the geom_smooth doesn't fit the data particularly well.  I've tried the loess method but this didn't change things much.  Can anyone suggest how to add a better smooth ?

Thanks

Paul.


  [1]: http://en.wikipedia.org/wiki/Birthday_problem
  [2]: http://www.oscon.com/oscon2009/public/schedule/detail/8972",r,ggplot2,,,,,open,0,164,8,"Adding an exponential geom_smooth in ggplot2 / R I am trying to produce some example graphics using ggplot2, and one of the examples I picked was the [birthday problem][1], here using code 'borrowed' from a Revolution [computing presentation][2] at Oscon.

    birthday<-function(n){
    	ntests<-1000
    	pop<-1:365
    	anydup<-function(i){
    		any(duplicated(sample(pop,n,replace=TRUE)))
    		}
    	sum(sapply(seq(ntests), anydup))/ntests
    	}

    x<-data.frame(x=rep(1:100, each=5))	
    x<-ddply(x, .(x), function(df) {return(data.frame(x=df$x, prob=birthday(df$x)))})
    birthdayplot<-ggplot(x, aes(x, prob))+
    		geom_point()+geom_smooth()+
    		theme_bw()+
    		opts(title = ""Probability that at least two people share a birthday in a random group"")+
    		labs(x=""Size of Group"", y=""Probability"")

Here my graph is what I would describe as exponential, but the geom_smooth doesn't fit the data particularly well.  I've tried the loess method but this didn't change things much.  Can anyone suggest how to add a better smooth ?

Thanks

Paul.


  [1]: http://en.wikipedia.org/wiki/Birthday_problem
  [2]: http://www.oscon.com/oscon2009/public/schedule/detail/8972",2
10268762,04/22/2012 14:24:44,114989,05/31/2009 03:06:30,219,0,Using R to plot messages per user given data frame listing all messages,"Given a data frame representing messages like this:


    df <- structure(list(message.id = c(123L, 456L), user.id = c(999L, 888L), 
          message.date = structure(c(1310950467, 1311119810), class = c(""POSIXct"", 
          ""POSIXt""), tzone = """")), .Names = c(""messageid"", ""user.id"", 
          ""message.date""), row.names = c(NA, -2L), class = ""data.frame"")

    head(df)
    message.id   user.id    message.date         
    123         999       2011-07-17 17:54:27
    456         888       2011-07-19 16:56:50

How would you plot the total number of messages per user assuming that some users would have a lot of messages and others very few (e.g. pareto distribution).

Thanks.",r,,,,,,open,0,167,13,"Using R to plot messages per user given data frame listing all messages Given a data frame representing messages like this:


    df <- structure(list(message.id = c(123L, 456L), user.id = c(999L, 888L), 
          message.date = structure(c(1310950467, 1311119810), class = c(""POSIXct"", 
          ""POSIXt""), tzone = """")), .Names = c(""messageid"", ""user.id"", 
          ""message.date""), row.names = c(NA, -2L), class = ""data.frame"")

    head(df)
    message.id   user.id    message.date         
    123         999       2011-07-17 17:54:27
    456         888       2011-07-19 16:56:50

How would you plot the total number of messages per user assuming that some users would have a lot of messages and others very few (e.g. pareto distribution).

Thanks.",1
3931836,10/14/2010 09:33:51,378085,06/28/2010 13:59:10,25,0,"RODBC import to keep ""NA""","When I import country data using RODBC I often use ISO2 codes. The unfortunate part is Namibia with ISO2 ""NA"" always gets set to missing. There are various ways around this of course, but I wonder if anyone has found a way to just import correctly. I've tried

    df <- sqlFetch(ch,""data_from_database"", na.strings="""")
    df <- sqlFetch(ch,""data_from_database"", as.is)

to no avail. Any tips? ",r,,,,,,open,0,67,5,"RODBC import to keep ""NA"" When I import country data using RODBC I often use ISO2 codes. The unfortunate part is Namibia with ISO2 ""NA"" always gets set to missing. There are various ways around this of course, but I wonder if anyone has found a way to just import correctly. I've tried

    df <- sqlFetch(ch,""data_from_database"", na.strings="""")
    df <- sqlFetch(ch,""data_from_database"", as.is)

to no avail. Any tips? ",1
6770521,07/21/2011 01:52:15,818555,06/27/2011 22:58:32,66,0,How to use ChebyShev's Inequality in R,"I have a statistical question in R and I was hoping to use Chebyshev inequality theorem, but I don't know how to implement it. 

Example:
Imagine a dataset with a nonnormal distribution, I need to be able to use Chebyshev's inequality theorem to assign NA values to any data point that falls within a certain lower bound of that distribution. For example, say the lower 5% of that distribution. This distribution is one-tailed with an absolute zero. 

I am unfamiliar with how to go about this, as well as with what sort of example might help. 

If it is helpful to know, this problem is stemming from a large amount of different datasets with all different types of distribution - all nonnormal. I need to be able to select a certain lower percentage of that distribution and assign NA values to them to discount them from the rest of the analysis. Will appreciate any help!

Thanks!",r,statistics,distribution,theorem,,07/21/2011 12:07:24,off topic,1,154,7,"How to use ChebyShev's Inequality in R I have a statistical question in R and I was hoping to use Chebyshev inequality theorem, but I don't know how to implement it. 

Example:
Imagine a dataset with a nonnormal distribution, I need to be able to use Chebyshev's inequality theorem to assign NA values to any data point that falls within a certain lower bound of that distribution. For example, say the lower 5% of that distribution. This distribution is one-tailed with an absolute zero. 

I am unfamiliar with how to go about this, as well as with what sort of example might help. 

If it is helpful to know, this problem is stemming from a large amount of different datasets with all different types of distribution - all nonnormal. I need to be able to select a certain lower percentage of that distribution and assign NA values to them to discount them from the rest of the analysis. Will appreciate any help!

Thanks!",4
11436222,07/11/2012 15:35:07,859868,07/24/2011 01:55:04,86,0,How to remove missing data and do a pair-wise analysis for a dataset?,"Input:

t1 <- data.frame(CName=c(""334"",""5as4"",""ggg"",""bbb""),D1=c(1,"" "",3,1),D2=c(3,4,5,5),D3=c(4,5,2,"" ""), D4=c(6,6,2,1))

Produce:

      CName D1 D2 D3 D4
    1   334  1  3  4  6
    2  5as4     4  5  6
    3   ggg  3  5  2  2
    4   bbb  1  5     1


Desire Output:

(1) Pearson coefficient for each rows, i.e. 334 vs 5as4, and then 334 vs ggg, and then 334 vs bbb, etc....

(2) Very important, I have to make sure the pearson coefficient was calculated AFTER skipping those missing values.  For example, for 334 vs 5as4, I only want to consider D2, D3 and D4.  For example, for 334 vs ggg, all D1, D2, D3 and D4 are considered.  please kindly noted that the neglect part is different for each pairs.

(3) Then, I want to get a table with ""334 vs 5as4"" etc in the first column, and pearson coef in the 2nd column.

What can I do?  Thanks.



",r,missing,,,,07/11/2012 21:02:47,too localized,1,187,13,"How to remove missing data and do a pair-wise analysis for a dataset? Input:

t1 <- data.frame(CName=c(""334"",""5as4"",""ggg"",""bbb""),D1=c(1,"" "",3,1),D2=c(3,4,5,5),D3=c(4,5,2,"" ""), D4=c(6,6,2,1))

Produce:

      CName D1 D2 D3 D4
    1   334  1  3  4  6
    2  5as4     4  5  6
    3   ggg  3  5  2  2
    4   bbb  1  5     1


Desire Output:

(1) Pearson coefficient for each rows, i.e. 334 vs 5as4, and then 334 vs ggg, and then 334 vs bbb, etc....

(2) Very important, I have to make sure the pearson coefficient was calculated AFTER skipping those missing values.  For example, for 334 vs 5as4, I only want to consider D2, D3 and D4.  For example, for 334 vs ggg, all D1, D2, D3 and D4 are considered.  please kindly noted that the neglect part is different for each pairs.

(3) Then, I want to get a table with ""334 vs 5as4"" etc in the first column, and pearson coef in the 2nd column.

What can I do?  Thanks.



",2
9872813,03/26/2012 13:15:30,1198478,02/08/2012 23:43:17,6,0,Nested logit in R,"I want to make a nested logistic regression in R with the package mlogit

I test how producer's decision to enter organisations (14 organisations) or not is affected by different factors....

Producer can be in specific organisation and other according to year. A precedent help advise me to use year as separate variable in columns. This is OK. But for my different organisations it will be the same? So my producer appear in my database several times (6 times) in row it is not redundant? 

Your help is very much appreciated. Many thanks.
",r,,,,,03/26/2012 14:18:00,off topic,1,91,4,"Nested logit in R I want to make a nested logistic regression in R with the package mlogit

I test how producer's decision to enter organisations (14 organisations) or not is affected by different factors....

Producer can be in specific organisation and other according to year. A precedent help advise me to use year as separate variable in columns. This is OK. But for my different organisations it will be the same? So my producer appear in my database several times (6 times) in row it is not redundant? 

Your help is very much appreciated. Many thanks.
",1
1708934,11/10/2009 15:46:14,163053,08/25/2009 20:22:41,3666,118,What useful R package doesn't currently exist?,"I have been working on a few R packages for some general tools that aren't currently available in R: blogging, report delivery, logging, and scheduling.  This led me to wonder: what are the most important things that people *wish* existed in R that currently aren't available?  

My hope is that we can use this to pinpoint some gaps, and possible collaboratively work on them.",r,,,,,11/18/2011 06:25:31,not constructive,1,66,7,"What useful R package doesn't currently exist? I have been working on a few R packages for some general tools that aren't currently available in R: blogging, report delivery, logging, and scheduling.  This led me to wonder: what are the most important things that people *wish* existed in R that currently aren't available?  

My hope is that we can use this to pinpoint some gaps, and possible collaboratively work on them.",1
8865633,01/14/2012 21:51:10,382271,07/02/2010 17:28:58,99,3,R data frame: How to control the conversion of matrix containing scientific notation strings to numeric,"I have a string matrix where the fields were derived from numbers in scientific notation. For example (note the quotes around the numbers):

    ""6e-23"", ""1e-14"", ""2e-6""

I want to convert the character matrix to a data frame and use these fields as numeric. During the conversion R will convert these strings to factors by default, maybe because of the 'e' character in the middle of the number. If the stringAsFactors() option is set to FALSE, the columns will be left as character, so still not numeric.

How can I force the data frame to interpret these strings as numbers? data.matrix() does actually convert string in scientific notation to numeric, but I want to know if there is a way to control the data frame conversion directly.",r,data.frame,,,,,open,0,127,16,"R data frame: How to control the conversion of matrix containing scientific notation strings to numeric I have a string matrix where the fields were derived from numbers in scientific notation. For example (note the quotes around the numbers):

    ""6e-23"", ""1e-14"", ""2e-6""

I want to convert the character matrix to a data frame and use these fields as numeric. During the conversion R will convert these strings to factors by default, maybe because of the 'e' character in the middle of the number. If the stringAsFactors() option is set to FALSE, the columns will be left as character, so still not numeric.

How can I force the data frame to interpret these strings as numbers? data.matrix() does actually convert string in scientific notation to numeric, but I want to know if there is a way to control the data frame conversion directly.",2
11276383,06/30/2012 18:00:41,1493368,06/30/2012 17:53:59,1,0,gvisMotionChart in R,"I have a trivial problem of running gvisMotionChart in R. Two days ago I was able to run the program and it worked perfect. Now, after two days, I am trying to run the same program and it says 
Error: could not find function ""gvisMotionChart""

I have updated the R but the problem is still there. Please help me.
",r,google-visualization,,,,07/01/2012 12:54:04,too localized,1,58,3,"gvisMotionChart in R I have a trivial problem of running gvisMotionChart in R. Two days ago I was able to run the program and it worked perfect. Now, after two days, I am trying to run the same program and it says 
Error: could not find function ""gvisMotionChart""

I have updated the R but the problem is still there. Please help me.
",2
4452039,12/15/2010 16:04:24,472592,10/11/2010 18:40:35,21,0,R's equivalent to ind2sub/sub2ind in matlab,"Matlab has two useful functions for converting matrix subscripts to linear indices and vice versa. (ind2sub and sub2ind)

Is there an equivalent way in R?

",r,matlab,,,,,open,0,24,6,"R's equivalent to ind2sub/sub2ind in matlab Matlab has two useful functions for converting matrix subscripts to linear indices and vice versa. (ind2sub and sub2ind)

Is there an equivalent way in R?

",2
4681034,01/13/2011 14:19:15,99542,05/01/2009 19:00:34,473,29,how reliable is the r programming language?,'R' has a lot of moving parts. The scariest is that it includes fortran in it's build (I think; and I certainly have not tracked Fortran's progress over the years). Secondly my application is almost entirely implemented in erlang so I'm hoping for similar reliability or uptime.,r,erlang,,,,01/13/2011 14:31:13,not a real question,1,47,7,how reliable is the r programming language? 'R' has a lot of moving parts. The scariest is that it includes fortran in it's build (I think; and I certainly have not tracked Fortran's progress over the years). Secondly my application is almost entirely implemented in erlang so I'm hoping for similar reliability or uptime.,2
3420950,08/06/2010 04:12:21,318976,04/17/2010 00:08:09,212,8,R checking pairs of rows in a dataframe,"I have a data frame holding information on options like this

    > chData
    myIdx strike_price       date     exdate cp_flag strike_price    return
    1 8355342       605000 1996-04-02 1996-05-18       P       605000  0.002340
    2 8355433       605000 1996-04-02 1996-05-18       C       605000  0.002340
    3 8356541       605000 1996-04-09 1996-05-18       P       605000 -0.003182
    4 8356629       605000 1996-04-09 1996-05-18       C       605000 -0.003182
    5 8358033       605000 1996-04-16 1996-05-18       P       605000  0.003907
    6 8358119       605000 1996-04-16 1996-05-18       C       605000  0.003907
    7 8359391       605000 1996-04-23 1996-05-18       P       605000  0.005695

where cp_flag means that a certain option is either a call or a put. What is a way to make sure that for each date, there is a both a call and a put, and drop the rows for which this does not exist? I can do it with a for loop, but is there a more clever way?
",r,,,,,,open,0,305,8,"R checking pairs of rows in a dataframe I have a data frame holding information on options like this

    > chData
    myIdx strike_price       date     exdate cp_flag strike_price    return
    1 8355342       605000 1996-04-02 1996-05-18       P       605000  0.002340
    2 8355433       605000 1996-04-02 1996-05-18       C       605000  0.002340
    3 8356541       605000 1996-04-09 1996-05-18       P       605000 -0.003182
    4 8356629       605000 1996-04-09 1996-05-18       C       605000 -0.003182
    5 8358033       605000 1996-04-16 1996-05-18       P       605000  0.003907
    6 8358119       605000 1996-04-16 1996-05-18       C       605000  0.003907
    7 8359391       605000 1996-04-23 1996-05-18       P       605000  0.005695

where cp_flag means that a certain option is either a call or a put. What is a way to make sure that for each date, there is a both a call and a put, and drop the rows for which this does not exist? I can do it with a for loop, but is there a more clever way?
",1
1647236,10/29/2009 23:50:05,178787,09/24/2009 23:51:45,1,0,Matching strings across columns in R,"I've got a data frame with 2 character columns. I'd like to find the rows which one column contains the other, however grepl is being strange. Any ideas?


>`>` ( df <- data.frame(letter=c('a','b'),food = c('apple','pear','bun','beets')) )

>  letter  food

>1      a apple

>2      b  pear

>3      a   bun

>4      b beets 

>`>` grepl(df$letter,df$food)

>[1]  TRUE  TRUE FALSE FALSE

but i want T F F T

Thanks.",r,,,,,,open,0,84,6,"Matching strings across columns in R I've got a data frame with 2 character columns. I'd like to find the rows which one column contains the other, however grepl is being strange. Any ideas?


>`>` ( df <- data.frame(letter=c('a','b'),food = c('apple','pear','bun','beets')) )

>  letter  food

>1      a apple

>2      b  pear

>3      a   bun

>4      b beets 

>`>` grepl(df$letter,df$food)

>[1]  TRUE  TRUE FALSE FALSE

but i want T F F T

Thanks.",1
5538738,04/04/2011 12:54:30,1112637,04/01/2011 09:08:53,3,0,Calculate error?,"i created this program to estimate error:
Is everything all riht with this?
pune is an exel with 22 datas.

> pune<-read.csv(""C:/Users/ervis/Desktop/Te dhenat e konsum energji/pune.csv"", header=T,dec="","", sep="";"")
> pune<-data.matrix(pune,rownames.force=NA)
> min(pune)
[1] 12271
> max(pune)
[1] 15447.95
> mean(pune)
[1] 14325.82
> m1<-seq(from=14274.19,to=14458.17,length.out=10000)
> MSE1<-numeric(length=10000)
> for(i in seq_along(MSE1)) {
+ MSE1[i]<-1/length(pune)]sum((pune-m1[i])^2)
Errore: unexpected ']' in:
""for(i in seq_along(MSE1)) {
MSE1[i]<-1/length(pune)]""
> for(i in seq_along(MSE1)) {
+ MSE1[i]<-1/length(pune)*sum((pune-m1[i])^2)
+ }
> min(MSE1)
[1] 535611.5
> which.min(MSE1)
[1] 2807
> m[which.min(MSE1)]
[1] 13860.54
> MAPE1<-numeric(length=10000)
> for(i in seq_along(MAPE1)) {
+ MAPE1[i]<-1/length(pune)*sum(abs((pune-m1[i])/pune))
+ }
> min(MAPE1)
[1] 0.03847801
> which.min(MAPE1)
[1] 10000
> m[which.min(MAPE1)]
[1] 13911.62
> MAPE1[which.min(MSE1)]<-1/length(pune)*sum(abs((pune-m1[which.min(MSE1)])/pune))
> MAPE1[which.min(MSE1)]
[1] 0.03928335
> MSE1[which.min(MAPE1)]<-1/length(pune)*sum((pune-m1[which.min(MAPE1)])^2)
> MSE1[which.min(MAPE1)]
[1] 553127
",r,,,,,04/06/2011 00:27:29,not a real question,1,76,2,"Calculate error? i created this program to estimate error:
Is everything all riht with this?
pune is an exel with 22 datas.

> pune<-read.csv(""C:/Users/ervis/Desktop/Te dhenat e konsum energji/pune.csv"", header=T,dec="","", sep="";"")
> pune<-data.matrix(pune,rownames.force=NA)
> min(pune)
[1] 12271
> max(pune)
[1] 15447.95
> mean(pune)
[1] 14325.82
> m1<-seq(from=14274.19,to=14458.17,length.out=10000)
> MSE1<-numeric(length=10000)
> for(i in seq_along(MSE1)) {
+ MSE1[i]<-1/length(pune)]sum((pune-m1[i])^2)
Errore: unexpected ']' in:
""for(i in seq_along(MSE1)) {
MSE1[i]<-1/length(pune)]""
> for(i in seq_along(MSE1)) {
+ MSE1[i]<-1/length(pune)*sum((pune-m1[i])^2)
+ }
> min(MSE1)
[1] 535611.5
> which.min(MSE1)
[1] 2807
> m[which.min(MSE1)]
[1] 13860.54
> MAPE1<-numeric(length=10000)
> for(i in seq_along(MAPE1)) {
+ MAPE1[i]<-1/length(pune)*sum(abs((pune-m1[i])/pune))
+ }
> min(MAPE1)
[1] 0.03847801
> which.min(MAPE1)
[1] 10000
> m[which.min(MAPE1)]
[1] 13911.62
> MAPE1[which.min(MSE1)]<-1/length(pune)*sum(abs((pune-m1[which.min(MSE1)])/pune))
> MAPE1[which.min(MSE1)]
[1] 0.03928335
> MSE1[which.min(MAPE1)]<-1/length(pune)*sum((pune-m1[which.min(MAPE1)])^2)
> MSE1[which.min(MAPE1)]
[1] 553127
",1
4643350,01/10/2011 03:00:39,569313,01/10/2011 02:55:08,101,0,color2D.matplot Legend in R,"Does anyone know how to change the placement of the legend in color2D.matplot (plotrix)? I have a12 x 12 correlation matrix. I noticed I had to reverse the row names and change the margin to get the long names to fit. But now I am stumped on how to move the legend (without increasing the margin even more and making the graph look odd with so much white space at the bottom. Thanks!

    
    cors<-cor(train)cellcol<-color.scale(cbind(cors,c(-1,rep(1,11))),c(0,1),0,c(1,0))[,1:12]
    par(mar = c(10,8,4,2) + 0.1)
    color2D.matplot(cors,cellcolors=cellcol,show.legend=TRUE,show.values=2,
      axes=FALSE, xlab="""",ylab="""")
    axis(1,at=0.5:11.5,las=2,labels=colnames(cors))
    axis(2,at=0.5:11.5,las=2,labels=rev(rownames(cors)))

![alt text][1]


  [1]: http://i.stack.imgur.com/QBnA1.jpg",r,,,,,,open,0,112,4,"color2D.matplot Legend in R Does anyone know how to change the placement of the legend in color2D.matplot (plotrix)? I have a12 x 12 correlation matrix. I noticed I had to reverse the row names and change the margin to get the long names to fit. But now I am stumped on how to move the legend (without increasing the margin even more and making the graph look odd with so much white space at the bottom. Thanks!

    
    cors<-cor(train)cellcol<-color.scale(cbind(cors,c(-1,rep(1,11))),c(0,1),0,c(1,0))[,1:12]
    par(mar = c(10,8,4,2) + 0.1)
    color2D.matplot(cors,cellcolors=cellcol,show.legend=TRUE,show.values=2,
      axes=FALSE, xlab="""",ylab="""")
    axis(1,at=0.5:11.5,las=2,labels=colnames(cors))
    axis(2,at=0.5:11.5,las=2,labels=rev(rownames(cors)))

![alt text][1]


  [1]: http://i.stack.imgur.com/QBnA1.jpg",1
10978895,06/11/2012 11:05:04,1436673,06/05/2012 06:35:54,3,0,how to compare two dataframes in R,I have two dataframes each having two columns(say x and y).I need to compare the two dataframes and see whether any of the values in x or y or both x and y are similar in the two dataframes.Any one answer this.,r,,,,,06/11/2012 23:49:31,not a real question,1,42,7,how to compare two dataframes in R I have two dataframes each having two columns(say x and y).I need to compare the two dataframes and see whether any of the values in x or y or both x and y are similar in the two dataframes.Any one answer this.,1
11342332,07/05/2012 10:22:07,1283999,03/21/2012 16:52:00,1,0,to generate heatmap using R,"please help me in generating heatmap of the following data using R

Pos	4	5	6	7	8	39	40	43	44	45	46	47	48	49	50	51	52	53
4		0.20	-0.04	0.20	0.24	0.00	0.09	0.20	0.23	-0.08	-0.03	0.14	0.15	0.29	-0.02	0.00	0.24	0.02
5			0.21	0.20	0.20	0.08	0.14	0.11	0.06	0.09	0.22	0.23	0.33	0.24	0.14	0.00	0.43	0.10
6				0.07	-0.17	0.11	0.28	0.10	-0.02	0.05	0.17	0.11	0.23	0.05	-0.06	0.00	0.01	0.19
7					0.35	0.04	0.19	0.02	0.06	0.13	0.22	-0.14	-0.04	0.15	-0.08	0.00	0.22	0.19
8						0.01	0.06	-0.11	0.04	-0.05	0.09	0.02	0.02	0.14	-0.07	0.00	0.21	0.08
39							0.15	0.01	0.09	0.06	0.27	-0.04	0.27	0.11	0.27	0.00	0.16	0.20
40								0.18	0.11	0.09	0.15	0.18	0.03	0.08	0.09	0.00	0.12	0.17
43									0.35	0.17	0.09	0.18	0.22	0.17	0.35	0.00	0.19	0.15
44										0.17	0.04	0.09	0.32	0.07	0.01	0.00	0.13	0.17
45											0.17	0.22	0.05	0.04	0.27	0.00	0.18	0.12
46												-0.03	0.24	0.02	0.05	0.00	0.22	0.25
47													0.02	-0.02	0.18	0.00	0.11	0.02
48														0.12	0.34	0.00	0.31	0.15
49															0.05	0.00	0.34	0.26
50																0.00	0.25	0.02
51																	0.00	0.00
52																		0.23
53																		
",r,,,,,07/05/2012 13:23:02,not a real question,1,12,5,"to generate heatmap using R please help me in generating heatmap of the following data using R

Pos	4	5	6	7	8	39	40	43	44	45	46	47	48	49	50	51	52	53
4		0.20	-0.04	0.20	0.24	0.00	0.09	0.20	0.23	-0.08	-0.03	0.14	0.15	0.29	-0.02	0.00	0.24	0.02
5			0.21	0.20	0.20	0.08	0.14	0.11	0.06	0.09	0.22	0.23	0.33	0.24	0.14	0.00	0.43	0.10
6				0.07	-0.17	0.11	0.28	0.10	-0.02	0.05	0.17	0.11	0.23	0.05	-0.06	0.00	0.01	0.19
7					0.35	0.04	0.19	0.02	0.06	0.13	0.22	-0.14	-0.04	0.15	-0.08	0.00	0.22	0.19
8						0.01	0.06	-0.11	0.04	-0.05	0.09	0.02	0.02	0.14	-0.07	0.00	0.21	0.08
39							0.15	0.01	0.09	0.06	0.27	-0.04	0.27	0.11	0.27	0.00	0.16	0.20
40								0.18	0.11	0.09	0.15	0.18	0.03	0.08	0.09	0.00	0.12	0.17
43									0.35	0.17	0.09	0.18	0.22	0.17	0.35	0.00	0.19	0.15
44										0.17	0.04	0.09	0.32	0.07	0.01	0.00	0.13	0.17
45											0.17	0.22	0.05	0.04	0.27	0.00	0.18	0.12
46												-0.03	0.24	0.02	0.05	0.00	0.22	0.25
47													0.02	-0.02	0.18	0.00	0.11	0.02
48														0.12	0.34	0.00	0.31	0.15
49															0.05	0.00	0.34	0.26
50																0.00	0.25	0.02
51																	0.00	0.00
52																		0.23
53																		
",1
8865400,01/14/2012 21:13:43,318752,04/16/2010 17:42:06,419,6,Security issues with CRAN packages,"I have a cronjob on my Ubuntu servers that downloads and installs every source package from CRAN. However on the same server I started to notice some irregular activity. It might be totally unrelated, but it got me thinking about if there could be a possibility that some CRAN packages contain malicious code.

The process of creating and publishing a cran package is extremely easy. Maybe a little too easy. You upload your package to the FTP, Kurt will do a check, and publish it. With the volume of R packages that is being uploaded every day, it is reasonable to assume that there is no extensive auditing of the package going on. Also there is no signing of a package using a private key, like most distro packages. Even the email address in the DESCRIPTION is rarely verified. 

Now it would not be very hard to include some code that installs a rootkit, either at compile time or at run time. Compile time is probably more vulnerable, because I install my packages using sudo, which I probably should stop doing. But also at runtime a lot can be done. The linux kernel has had several security vulnerabilities lately, and I have confirmed myself that it can be extremely easy to obtain root via a privilege escalation exploit, on a completely up to date system. As R usually has internet access, the malicious code does not even have to be included in the package, it can simply be downloaded from somewhere using wget or download.file().

That said, are R users considering this at all? Or is the philosophy mostly that you should only download packages from people you trust? Still without signing the packages that is not very reliable. What could be a safer approach to installing cran packages? I have considered something like a separate machine for building packages and then copying the binaries, and always running R in a sandbox. That is a little cumbersome though.",r,cran,,,,01/15/2012 07:15:57,off topic,1,326,5,"Security issues with CRAN packages I have a cronjob on my Ubuntu servers that downloads and installs every source package from CRAN. However on the same server I started to notice some irregular activity. It might be totally unrelated, but it got me thinking about if there could be a possibility that some CRAN packages contain malicious code.

The process of creating and publishing a cran package is extremely easy. Maybe a little too easy. You upload your package to the FTP, Kurt will do a check, and publish it. With the volume of R packages that is being uploaded every day, it is reasonable to assume that there is no extensive auditing of the package going on. Also there is no signing of a package using a private key, like most distro packages. Even the email address in the DESCRIPTION is rarely verified. 

Now it would not be very hard to include some code that installs a rootkit, either at compile time or at run time. Compile time is probably more vulnerable, because I install my packages using sudo, which I probably should stop doing. But also at runtime a lot can be done. The linux kernel has had several security vulnerabilities lately, and I have confirmed myself that it can be extremely easy to obtain root via a privilege escalation exploit, on a completely up to date system. As R usually has internet access, the malicious code does not even have to be included in the package, it can simply be downloaded from somewhere using wget or download.file().

That said, are R users considering this at all? Or is the philosophy mostly that you should only download packages from people you trust? Still without signing the packages that is not very reliable. What could be a safer approach to installing cran packages? I have considered something like a separate machine for building packages and then copying the binaries, and always running R in a sandbox. That is a little cumbersome though.",2
6991434,08/09/2011 04:41:12,874102,07/28/2011 06:50:27,92,0,Assigning results of a for loop to an empty matrix,"I have another question for the brilliant minds out there (this site is so addictive).

I am running some simulations on a matrix and I have nested for loops for this purpose.  The first creates a vector that increases by one each time a loop cycles.  The nested loop is running simulations by randomizing the vector, attaching it to the matrix, and calculating some simple properties on the new matrix.  (For an example, I used properties that will not vary in the simulations, but in practice I require the simulations to get a good idea of the impact of the randomized vector.)  The nested loop runs 100 simulations, and ultimately I want only the column means of those simulations.

Here's some example code:


    property<-function(mat){                       #where mat is a matrix
      a=sum(mat) 
      b=sum(colMeans(mat))
      c=mean(mat)
      d=sum(rowMeans(mat))
      e=nrow(mat)*ncol(mat)
      answer=list(a,b,c,d,e)
      return(answer)
      }

    x=matrix(c(1,0,1,0, 0,1,1,0, 0,0,0,1, 1,0,0,0, 1,0,0,1), byrow=T, nrow=5, ncol=4)

    obj=matrix(nrow=100,ncol=5,byrow=T)            #create an empty matrix to dump results into

    for(i in 1:ncol(x)){                           #nested for loops
      a=rep(1,times=i)                             #repeat 1 for 1:# columns in x
      b=rep(0,times=(ncol(x)-length(a)))           #have the rest of the vector be 0
      I.vec=append(a,b)                            #append these two for the I vector
        for (j in 1:100){
          I.vec2=sample(I.vec,replace=FALSE)       #randomize I vector
          temp=rbind(x,I.vec2)
          prop<-property(temp)
          obj[[j]]<-prop
          }
      write.table(colMeans(obj), 'myfile.csv', quote = FALSE, sep = ',', row.names = FALSE)
      }

The problem I am encountering is how to fill in the empty object matrix with the results of the nested loop.  obj ends up as a vector of mostly NAs, so it is clear that I am not assigning the results properly.  I want each cycle to add a row of prop to obj, but if I try

    obj[j,]<-prop

R tells me that there is an incorrect number of subscripts on the matrix.

Thank you so much for your help!",r,object,loops,,,,open,0,553,10,"Assigning results of a for loop to an empty matrix I have another question for the brilliant minds out there (this site is so addictive).

I am running some simulations on a matrix and I have nested for loops for this purpose.  The first creates a vector that increases by one each time a loop cycles.  The nested loop is running simulations by randomizing the vector, attaching it to the matrix, and calculating some simple properties on the new matrix.  (For an example, I used properties that will not vary in the simulations, but in practice I require the simulations to get a good idea of the impact of the randomized vector.)  The nested loop runs 100 simulations, and ultimately I want only the column means of those simulations.

Here's some example code:


    property<-function(mat){                       #where mat is a matrix
      a=sum(mat) 
      b=sum(colMeans(mat))
      c=mean(mat)
      d=sum(rowMeans(mat))
      e=nrow(mat)*ncol(mat)
      answer=list(a,b,c,d,e)
      return(answer)
      }

    x=matrix(c(1,0,1,0, 0,1,1,0, 0,0,0,1, 1,0,0,0, 1,0,0,1), byrow=T, nrow=5, ncol=4)

    obj=matrix(nrow=100,ncol=5,byrow=T)            #create an empty matrix to dump results into

    for(i in 1:ncol(x)){                           #nested for loops
      a=rep(1,times=i)                             #repeat 1 for 1:# columns in x
      b=rep(0,times=(ncol(x)-length(a)))           #have the rest of the vector be 0
      I.vec=append(a,b)                            #append these two for the I vector
        for (j in 1:100){
          I.vec2=sample(I.vec,replace=FALSE)       #randomize I vector
          temp=rbind(x,I.vec2)
          prop<-property(temp)
          obj[[j]]<-prop
          }
      write.table(colMeans(obj), 'myfile.csv', quote = FALSE, sep = ',', row.names = FALSE)
      }

The problem I am encountering is how to fill in the empty object matrix with the results of the nested loop.  obj ends up as a vector of mostly NAs, so it is clear that I am not assigning the results properly.  I want each cycle to add a row of prop to obj, but if I try

    obj[j,]<-prop

R tells me that there is an incorrect number of subscripts on the matrix.

Thank you so much for your help!",3
11601583,07/22/2012 15:35:07,1420099,05/27/2012 11:59:43,21,0,how to make some changes to histogram in lattice?,"im pretty new to this lattice stuff so i guess my questions will be rather simple

for this example

    a<-30
    b<-40
    c<-50
    group <- c(rep(""a"",a),rep(""b"",b),rep(""c"",c))
    data1 <- sample(40, a, replace=T)
    data2 <- sample(40, b, replace=T)
    data3 <- sample(40, c, replace=T)
    data <- c(data1,data2,data3)
    data <- cbind(data,group)
    data <- as.data.frame(data)
    
    histogram(~as.numeric(data$data) | data$group, layout=c(3,1))


i would like to change the following:

- the color of the topic bar where ""a"", ""b"", ""c"" is written in (i dont like this pinkish color very much...)

- the color of the bars - i know i can change them by setting col=""..."" but i would like to change the color for every single histogramm separately i.e. the first is yellow, 2nd green etc...

- last but not least i would like to put a lable in each histogram in which the samplesize associated to the histogram is written in.


thx alot and have a nice sunday
kind regards druss",r,statistics,plot,histogram,lattice,,open,0,185,9,"how to make some changes to histogram in lattice? im pretty new to this lattice stuff so i guess my questions will be rather simple

for this example

    a<-30
    b<-40
    c<-50
    group <- c(rep(""a"",a),rep(""b"",b),rep(""c"",c))
    data1 <- sample(40, a, replace=T)
    data2 <- sample(40, b, replace=T)
    data3 <- sample(40, c, replace=T)
    data <- c(data1,data2,data3)
    data <- cbind(data,group)
    data <- as.data.frame(data)
    
    histogram(~as.numeric(data$data) | data$group, layout=c(3,1))


i would like to change the following:

- the color of the topic bar where ""a"", ""b"", ""c"" is written in (i dont like this pinkish color very much...)

- the color of the bars - i know i can change them by setting col=""..."" but i would like to change the color for every single histogramm separately i.e. the first is yellow, 2nd green etc...

- last but not least i would like to put a lable in each histogram in which the samplesize associated to the histogram is written in.


thx alot and have a nice sunday
kind regards druss",5
9941363,03/30/2012 10:36:03,1222065,02/20/2012 21:22:48,1,0,Plots in lordif,"I'm using the package lordif and I was wondering why I can't get to all the maps
produced by the function [plot][1].


When I do the following

    library(lordif)
    data(""Anxiety"")
    Age<-Anxiety$age
    Resp<-Anxiety[paste(""R"",1:29,sep="""")]
    ageDIF<-lordif(Resp,Age,criterion=""Chisqr"",alpha=0.01,minCell=5)
    plot(ageDIF,labels=c(""Younger (<65)"",""Older (65+)""))

When I run the above I can see lots of maps as if I was flicking through a book but the only one that remains static is the very last one (showing difference between theta and theta-purified).

I've tried to use par(mfrow) but nothing changed. I've also tried to save the plot both manually and through R code but I still cannot access the whole set of plots.


1) How do I browse through the whole ""plot book"" and possibly save each image?

2) Is it possible to create two images one with plot 1-3 and another 3 to 7? 

Many thanks

  [1]: http://127.0.0.1:17489/library/lordif/html/plot.lordif.html",r,plot,,,,,open,0,152,3,"Plots in lordif I'm using the package lordif and I was wondering why I can't get to all the maps
produced by the function [plot][1].


When I do the following

    library(lordif)
    data(""Anxiety"")
    Age<-Anxiety$age
    Resp<-Anxiety[paste(""R"",1:29,sep="""")]
    ageDIF<-lordif(Resp,Age,criterion=""Chisqr"",alpha=0.01,minCell=5)
    plot(ageDIF,labels=c(""Younger (<65)"",""Older (65+)""))

When I run the above I can see lots of maps as if I was flicking through a book but the only one that remains static is the very last one (showing difference between theta and theta-purified).

I've tried to use par(mfrow) but nothing changed. I've also tried to save the plot both manually and through R code but I still cannot access the whole set of plots.


1) How do I browse through the whole ""plot book"" and possibly save each image?

2) Is it possible to create two images one with plot 1-3 and another 3 to 7? 

Many thanks

  [1]: http://127.0.0.1:17489/library/lordif/html/plot.lordif.html",2
7730668,10/11/2011 18:26:22,990052,10/11/2011 17:47:02,1,0,"When I use eval with aggregate, I lose the variable names","I have a bit of code which aggregates data:

    pivot.present.RT <- with(
      subset(correct.data, relevantTarget == 1),
      aggregate(
        data.frame(RT = RT),
        list(
          identifier = identifier,
          set.size = relevantSS,
          stimulus = stimulus
          ),
        mean
        )
      )

I would like to make this more flexible by specifying different column names to take the place of ""relevantSS"". I thought I could do this with eval:

    set.size.options <- c(""relevantSS"",""irrelevantSS"")
    pivot.present.RT <- with(
      subset(correct.data, relevantTarget == 1),
      aggregate(
        data.frame(RT = RT),
        list(
          identifier = identifier,
          eval(parse(text = paste(""set.size = "", set.size.options[relevant.index]))),
          stimulus = stimulus
          ),
        mean
        )
      )


However, when I run the second bit of code, while it does correctly aggregate the data, I lose the variable name ""set.size"". If I call str, I get output like this:

    'data.frame':	48 obs. of  4 variables:
     $ identifier: Factor w/ 9 levels ""aks"",""ejr"",""ejr3"",..: 1 2 4 5 6 7 8 9 1 2 ...
     $ Group.2   : int  4 4 4 4 4 4 4 4 8 8 ...
     $ stimulus  : Factor w/ 2 levels ""moving"",""stationary"": 1 1 1 1 1 1 1 1 1 1 ...
     $ RT        : num  1161 1026 1257 1264 1324 ...

If I run the original code, it correctly identifies the second variable as ""set.size"".

Any idea what I'm missing here?",r,,,,,,open,0,405,11,"When I use eval with aggregate, I lose the variable names I have a bit of code which aggregates data:

    pivot.present.RT <- with(
      subset(correct.data, relevantTarget == 1),
      aggregate(
        data.frame(RT = RT),
        list(
          identifier = identifier,
          set.size = relevantSS,
          stimulus = stimulus
          ),
        mean
        )
      )

I would like to make this more flexible by specifying different column names to take the place of ""relevantSS"". I thought I could do this with eval:

    set.size.options <- c(""relevantSS"",""irrelevantSS"")
    pivot.present.RT <- with(
      subset(correct.data, relevantTarget == 1),
      aggregate(
        data.frame(RT = RT),
        list(
          identifier = identifier,
          eval(parse(text = paste(""set.size = "", set.size.options[relevant.index]))),
          stimulus = stimulus
          ),
        mean
        )
      )


However, when I run the second bit of code, while it does correctly aggregate the data, I lose the variable name ""set.size"". If I call str, I get output like this:

    'data.frame':	48 obs. of  4 variables:
     $ identifier: Factor w/ 9 levels ""aks"",""ejr"",""ejr3"",..: 1 2 4 5 6 7 8 9 1 2 ...
     $ Group.2   : int  4 4 4 4 4 4 4 4 8 8 ...
     $ stimulus  : Factor w/ 2 levels ""moving"",""stationary"": 1 1 1 1 1 1 1 1 1 1 ...
     $ RT        : num  1161 1026 1257 1264 1324 ...

If I run the original code, it correctly identifies the second variable as ""set.size"".

Any idea what I'm missing here?",1
6838851,07/27/2011 02:29:33,633426,02/25/2011 01:51:48,19,0,How to copy plots in R when working in Ubuntu?,"I am working on the most recent of R on Ubuntu. After I created some
plots, the cursor became a '+' and won't let me access the 
copy and paste options. Moreover unlike the R in Windows, it does
not have a toolbar that have the 'Copy' option. What can I do to 
copy my plots?

Thank you.",r,copy,plot,,,,open,0,55,10,"How to copy plots in R when working in Ubuntu? I am working on the most recent of R on Ubuntu. After I created some
plots, the cursor became a '+' and won't let me access the 
copy and paste options. Moreover unlike the R in Windows, it does
not have a toolbar that have the 'Copy' option. What can I do to 
copy my plots?

Thank you.",3
11546912,07/18/2012 17:22:31,1491459,06/29/2012 14:51:26,6,0,pathClass package use,"I wanted to try out a model with SVM RFE by using pathclass package, but I am not really sure whether one can model any kind of data or just biological data with this package?

Thanks in advance!
",r,svm,,,,07/19/2012 14:19:21,not a real question,1,37,3,"pathClass package use I wanted to try out a model with SVM RFE by using pathclass package, but I am not really sure whether one can model any kind of data or just biological data with this package?

Thanks in advance!
",2
3638196,09/03/2010 17:41:35,253579,01/18/2010 22:45:39,25,1,labels including an expression in cut function used in a boxplot in R,"I'm using the cut function to convert a numeric variable into a factor with two levels and using this in a boxplot like this:

    boxplot(Sp$Var1 ~ cut(Spt$Var5, breaks = c(0,50,100), labels =c(""below 50%"", ""above 50%"")), ...)

I want to include sample size as ""n=..."" below each of the labels used in the cut function. I can get the sample size using length with a subset, as this,

    length(subset(Sp$Var1, SpDet$Var5<50)

And use cat and paste to get the sample size below the label

    cat(paste(""above 50%"", ""\n"", ""n ="", length(subset(Sp$Var1, Sp$Var5<50)), sep=""""))

My problem is that I've not been able to insert this into the labels argument of the cut function. Simply, inserting the above into the labels vector prints the boxplot ok, but prints the labels in the R console. I think I may need to use the expression function, but I haven't got this to work either. Any help or alternative methods appreciated.
",r,labels,cut,boxplot,,,open,0,159,13,"labels including an expression in cut function used in a boxplot in R I'm using the cut function to convert a numeric variable into a factor with two levels and using this in a boxplot like this:

    boxplot(Sp$Var1 ~ cut(Spt$Var5, breaks = c(0,50,100), labels =c(""below 50%"", ""above 50%"")), ...)

I want to include sample size as ""n=..."" below each of the labels used in the cut function. I can get the sample size using length with a subset, as this,

    length(subset(Sp$Var1, SpDet$Var5<50)

And use cat and paste to get the sample size below the label

    cat(paste(""above 50%"", ""\n"", ""n ="", length(subset(Sp$Var1, Sp$Var5<50)), sep=""""))

My problem is that I've not been able to insert this into the labels argument of the cut function. Simply, inserting the above into the labels vector prints the boxplot ok, but prints the labels in the R console. I think I may need to use the expression function, but I haven't got this to work either. Any help or alternative methods appreciated.
",4
10628547,05/17/2012 00:41:50,1165618,01/23/2012 19:09:09,615,11,Use superscripts in R axis labels,"Using base graphics in R, how can I add superscripts to axis labels, as one might want to when plotting latitude and longitude axes on a map.

Consider this example:

    plot(-100:-50, 50:100, type=""n"", xlab="""", ylab="""", axes=FALSE)
    axis(1, seq(-100, -50, 10), labels=paste(abs(seq(-100, -50, 10)), ""o"", ""W"", sep=""""))
    axis(2, seq(50, 100, 10), labels=paste(seq(50,100,10), ""o"", ""N"", sep=""""))
    box()

Produces a nice frame around a map.  It would be even nicer to make the degree symbol superscript.  

This can usually be done in other plotting functions such as `mtext()` and `text()` using `expression(paste(...))` or `substitute()` but how to do it in this case?


",r,,,,,,open,0,111,6,"Use superscripts in R axis labels Using base graphics in R, how can I add superscripts to axis labels, as one might want to when plotting latitude and longitude axes on a map.

Consider this example:

    plot(-100:-50, 50:100, type=""n"", xlab="""", ylab="""", axes=FALSE)
    axis(1, seq(-100, -50, 10), labels=paste(abs(seq(-100, -50, 10)), ""o"", ""W"", sep=""""))
    axis(2, seq(50, 100, 10), labels=paste(seq(50,100,10), ""o"", ""N"", sep=""""))
    box()

Produces a nice frame around a map.  It would be even nicer to make the degree symbol superscript.  

This can usually be done in other plotting functions such as `mtext()` and `text()` using `expression(paste(...))` or `substitute()` but how to do it in this case?


",1
11108206,06/19/2012 19:42:16,627259,02/21/2011 20:25:41,229,2,Override y-scale and x-scale using xlim/ylim or xrange/yrange in quantmod::chart_Series() - impossible?,"I am trying to plot some support/resistance lines on top of quantmod::chart_Series(). The issue is that the interesting support/resistance lines are outside (below or above) series data range up to current time (I would also like to extend a chart a bit to the right beyond last timestamp of data).

Looking at the source code of quantmod::chart_Series() I can see no way of specifying ylim/xlim or, what was possible in ""the old days"" with quantmod::chartSeries using yrange to override y-scale. Comment here https://r-forge.r-project.org/scm/viewvc.php?view=rev&root=quantmod&revision=520 is also comfirming my hunch...

Is my diagnosis correct or is there maybe a way that enables y-scale overriding in quantmod::chart_Series? Any idea how to do what I want highly appreciated.

Thanks.

Best,
Samo",r,finance,quantmod,,,,open,0,112,12,"Override y-scale and x-scale using xlim/ylim or xrange/yrange in quantmod::chart_Series() - impossible? I am trying to plot some support/resistance lines on top of quantmod::chart_Series(). The issue is that the interesting support/resistance lines are outside (below or above) series data range up to current time (I would also like to extend a chart a bit to the right beyond last timestamp of data).

Looking at the source code of quantmod::chart_Series() I can see no way of specifying ylim/xlim or, what was possible in ""the old days"" with quantmod::chartSeries using yrange to override y-scale. Comment here https://r-forge.r-project.org/scm/viewvc.php?view=rev&root=quantmod&revision=520 is also comfirming my hunch...

Is my diagnosis correct or is there maybe a way that enables y-scale overriding in quantmod::chart_Series? Any idea how to do what I want highly appreciated.

Thanks.

Best,
Samo",3
6645450,07/11/2011 03:18:25,597925,02/01/2011 05:43:10,366,9,Can you change the default values inside R (base) functions?,"Couldn't see a solution online but I thought this might be quite common.

With write.csv I basically always have the argument row.name set to F. Is it possible to run a line once and update the default value of the argument for the rest of the session?

I tried:

    paste <- paste(sep="""")

Which ran and returned no error but seemed to do nothing (and didn't destroy the paste function). This is another one, I always set sep to """" with paste, like I always have 'exclude=NULL' when I am using table so I can see the N/A values.

Thanks",r,,,,,,open,0,98,10,"Can you change the default values inside R (base) functions? Couldn't see a solution online but I thought this might be quite common.

With write.csv I basically always have the argument row.name set to F. Is it possible to run a line once and update the default value of the argument for the rest of the session?

I tried:

    paste <- paste(sep="""")

Which ran and returned no error but seemed to do nothing (and didn't destroy the paste function). This is another one, I always set sep to """" with paste, like I always have 'exclude=NULL' when I am using table so I can see the N/A values.

Thanks",1
10773847,05/27/2012 12:03:46,1420099,05/27/2012 11:59:43,1,0,R: How can i change the point color in densityplot (lattice),"by points i mean the points on the bottom of the densityplot. furthermore i would like to set some value of ""jitter"" such that all the points are not just on a straight line and more ""scattered"" around.

Thx for your help
kind regards
druss",r,,,,,05/28/2012 16:24:38,not a real question,1,42,11,"R: How can i change the point color in densityplot (lattice) by points i mean the points on the bottom of the densityplot. furthermore i would like to set some value of ""jitter"" such that all the points are not just on a straight line and more ""scattered"" around.

Thx for your help
kind regards
druss",1
10480292,05/07/2012 10:12:31,184046,10/04/2009 20:35:01,11206,49,Why am I getting dark text like this?,"I am using the following to plot my data:

    plot =  ggplot(transData, aes(x=A, y=ecd)) + 
      scale_fill_brewer(palette=""Set1"") + 
      geom_line(size=1.5, aes(color=Type, group=Type, linetype=Type)) + 
      scale_y_continuous(""P[X<x]"", lim=c(0,1)) +
      theme_bw() +

      geom_hline(aes(yintercept=0.5), linetype=4) +
      geom_text(aes(1200, .55, label=""Median""),size=7) +
      geom_hline(aes(yintercept=0.95), linetype=4) +
      geom_text(aes(1200, 0.90, label=""95th Percentile""),size=7) +
      
      geom_vline(aes(xintercept=30), linetype=4) +
      geom_text(aes(40, .85, label=""1 month""),size=7) +
      geom_vline(aes(xintercept=90), linetype=4) +
      geom_text(aes(190, .15, label=""6 months""),size=7) +
      geom_vline(aes(xintercept=365), linetype=4) +
      geom_text(aes(380, .40, label=""1 year""),size=7) +
      geom_vline(aes(xintercept=365*2), linetype=4) +
      geom_text(aes(380*2, .15, label=""2 years""),size=7) +
      geom_vline(aes(xintercept=365*3), linetype=4) +
      geom_text(aes(380*3, .15, label=""3 years""),size=7) +
      #scale_x_log10(""Annualized Downtime (s)"") + 
      #scale_y_continuous(""P[X<x]"") +
      #geom_point(aes(shape=Type, fill=Type), size=4) +
      
      opts(plot.title = theme_text(size=36, colour=""black"", face=""bold""),
           axis.text.y=theme_text(size=16, colour=""black""), 
           axis.text.x=theme_text(size=16, colour=""black""),
           axis.title.y=theme_text(angle=90, size=14, vjust=0.2, hjust=0.5, face=""bold""), 
           axis.title.x=theme_text(size=14, vjust=0.2, hjust=0.5, face=""bold""),
           legend.position = ""top"", 
           legend.title = theme_blank(),
           strip.text.y=theme_text(size=12, colour=""black"", face=""bold""), 
           strip.text.x=theme_text(size=12, colour=""black"", face=""bold""),
           legend.text = theme_text(size=12, face=""bold""),
           legend.key.size = unit(1.5, ""lines""),
           legend.key.width = unit(4, ""line""),
           legend.direction = ""horizontal"")

Does anyone know why I am getting the text from `geom_text` so dark? When I insert this into a pdf file, it gets really ugly. Any suggestions on how to solve this?

![enter image description here][1]


  [1]: http://i.stack.imgur.com/ujxCR.png",r,graph,ggplot2,,,,open,0,428,8,"Why am I getting dark text like this? I am using the following to plot my data:

    plot =  ggplot(transData, aes(x=A, y=ecd)) + 
      scale_fill_brewer(palette=""Set1"") + 
      geom_line(size=1.5, aes(color=Type, group=Type, linetype=Type)) + 
      scale_y_continuous(""P[X<x]"", lim=c(0,1)) +
      theme_bw() +

      geom_hline(aes(yintercept=0.5), linetype=4) +
      geom_text(aes(1200, .55, label=""Median""),size=7) +
      geom_hline(aes(yintercept=0.95), linetype=4) +
      geom_text(aes(1200, 0.90, label=""95th Percentile""),size=7) +
      
      geom_vline(aes(xintercept=30), linetype=4) +
      geom_text(aes(40, .85, label=""1 month""),size=7) +
      geom_vline(aes(xintercept=90), linetype=4) +
      geom_text(aes(190, .15, label=""6 months""),size=7) +
      geom_vline(aes(xintercept=365), linetype=4) +
      geom_text(aes(380, .40, label=""1 year""),size=7) +
      geom_vline(aes(xintercept=365*2), linetype=4) +
      geom_text(aes(380*2, .15, label=""2 years""),size=7) +
      geom_vline(aes(xintercept=365*3), linetype=4) +
      geom_text(aes(380*3, .15, label=""3 years""),size=7) +
      #scale_x_log10(""Annualized Downtime (s)"") + 
      #scale_y_continuous(""P[X<x]"") +
      #geom_point(aes(shape=Type, fill=Type), size=4) +
      
      opts(plot.title = theme_text(size=36, colour=""black"", face=""bold""),
           axis.text.y=theme_text(size=16, colour=""black""), 
           axis.text.x=theme_text(size=16, colour=""black""),
           axis.title.y=theme_text(angle=90, size=14, vjust=0.2, hjust=0.5, face=""bold""), 
           axis.title.x=theme_text(size=14, vjust=0.2, hjust=0.5, face=""bold""),
           legend.position = ""top"", 
           legend.title = theme_blank(),
           strip.text.y=theme_text(size=12, colour=""black"", face=""bold""), 
           strip.text.x=theme_text(size=12, colour=""black"", face=""bold""),
           legend.text = theme_text(size=12, face=""bold""),
           legend.key.size = unit(1.5, ""lines""),
           legend.key.width = unit(4, ""line""),
           legend.direction = ""horizontal"")

Does anyone know why I am getting the text from `geom_text` so dark? When I insert this into a pdf file, it gets really ugly. Any suggestions on how to solve this?

![enter image description here][1]


  [1]: http://i.stack.imgur.com/ujxCR.png",3
7136990,08/21/2011 07:28:34,390388,07/13/2010 11:18:58,186,5,change font in a legend,"I have a graph use the base graphics package. For the labels on specific points I use 

       text(i, MSSAcar$summary[i,7]+.7, qld$LGA[i],
       col='red',  cex=.7, family='serif')

I have also used this in the plot for main titles and axis labels. They all come out as expected.

When I add a legend I cannot seem to be able to set the font family.

Can anyone help please.

Thanks.
",r,plot,legend,,,,open,0,74,5,"change font in a legend I have a graph use the base graphics package. For the labels on specific points I use 

       text(i, MSSAcar$summary[i,7]+.7, qld$LGA[i],
       col='red',  cex=.7, family='serif')

I have also used this in the plot for main titles and axis labels. They all come out as expected.

When I add a legend I cannot seem to be able to set the font family.

Can anyone help please.

Thanks.
",3
11542584,07/18/2012 13:30:01,168775,09/04/2009 20:42:51,5958,308,Refer to time series object by column name,"I have a data frame object `test` that has column names:

    > test
           a     b     c     d     e
    1  -0.67 -0.02 -0.10 -0.22 -0.32
    2   0.46 -1.51 -0.79  0.26  1.19
    3   0.22 -0.18 -1.40  0.41 -0.32
    4  -2.21  0.79  0.36  1.00 -0.51
    5  -0.69  0.39 -0.76 -0.73 -0.43

In this format, I can easily access the columns using the `test$b` notation. I can convert this to a time series object without difficulty:

    test.ts <- ts(test, frequency=<value>, start=<value>

However, once it's a `ts` object, is there any easy way to access the columns (or rows) by name instead of by column number? The `test.ts` object still has the column name information, shown by using `colnames`:

    > colnames(test.ts)
    [1] ""a"" ""b"" ""c"" ""d"" ""e""

However, `test.ts$b` doesn't work. Note that by ""easily"" I mean without writing something ugly like `test.ts[,which(colnames(test.ts)==""b""]`, because that's not easy, that's ugly. Yes, I could write my own function to do that, but I was wondering whether there's a built-in way to do this. Thanks!
",r,time-series,,,,,open,0,231,8,"Refer to time series object by column name I have a data frame object `test` that has column names:

    > test
           a     b     c     d     e
    1  -0.67 -0.02 -0.10 -0.22 -0.32
    2   0.46 -1.51 -0.79  0.26  1.19
    3   0.22 -0.18 -1.40  0.41 -0.32
    4  -2.21  0.79  0.36  1.00 -0.51
    5  -0.69  0.39 -0.76 -0.73 -0.43

In this format, I can easily access the columns using the `test$b` notation. I can convert this to a time series object without difficulty:

    test.ts <- ts(test, frequency=<value>, start=<value>

However, once it's a `ts` object, is there any easy way to access the columns (or rows) by name instead of by column number? The `test.ts` object still has the column name information, shown by using `colnames`:

    > colnames(test.ts)
    [1] ""a"" ""b"" ""c"" ""d"" ""e""

However, `test.ts$b` doesn't work. Note that by ""easily"" I mean without writing something ugly like `test.ts[,which(colnames(test.ts)==""b""]`, because that's not easy, that's ugly. Yes, I could write my own function to do that, but I was wondering whether there's a built-in way to do this. Thanks!
",2
6625691,07/08/2011 14:11:59,66526,02/14/2009 23:06:35,107,2,Is it possible to switch the side of y-axis breaks and labels on a faceted plot?,"By default, on a faceted ggplot (facet_grid), the y-axis facet labels are on the right and the y-axis breaks and labels are on the left.

Is it possible to switch them? ",r,ggplot2,,,,,open,0,31,16,"Is it possible to switch the side of y-axis breaks and labels on a faceted plot? By default, on a faceted ggplot (facet_grid), the y-axis facet labels are on the right and the y-axis breaks and labels are on the left.

Is it possible to switch them? ",2
11719153,07/30/2012 09:58:41,221342,11/30/2009 13:23:02,903,29,Computing Quadratic Variation in R,"In order to compute the quadratic variation of a timeseries in R, I would like to to sum for every point the square of log returns of the current point and the last x points.

I know that you can build the square of log returns of py by typing 

    diff(log(py))^2

However how can I build a timeseries which is summing at every point the last 5 points in order to build the quadratic variation ?",r,volatility,,,,,open,0,78,5,"Computing Quadratic Variation in R In order to compute the quadratic variation of a timeseries in R, I would like to to sum for every point the square of log returns of the current point and the last x points.

I know that you can build the square of log returns of py by typing 

    diff(log(py))^2

However how can I build a timeseries which is summing at every point the last 5 points in order to build the quadratic variation ?",2
3491175,08/16/2010 07:00:32,377031,06/26/2010 16:51:13,386,1,adjusting x-axis in R histograms,"This histogram is really ugly:

    hist(rbinom(10000, 20000, 0.0001),freq=F,right=F)

I don't want spaces between my bars. I tried different `breaks=` methods but they all produce similar results. Any ideas?

I also want each bin value (or mean values )to be printed under the **center** of it's bar.",r,plot,histogram,,,,open,0,47,5,"adjusting x-axis in R histograms This histogram is really ugly:

    hist(rbinom(10000, 20000, 0.0001),freq=F,right=F)

I don't want spaces between my bars. I tried different `breaks=` methods but they all produce similar results. Any ideas?

I also want each bin value (or mean values )to be printed under the **center** of it's bar.",3
8612540,12/23/2011 05:25:50,737297,05/04/2011 04:04:32,402,4,Which R functions are useful for analysis of an investment strategy's profitability?,"I have multiple variations of an automated strategy for trading certain investment vehicles. For each of these variations I have cross-validated backtests on historic data. I  need to pick the best-performing test. There is significant variation between the tests in terms of trades per day, net position size, etc. This makes it difficult to compare one to another.

The nature of the test relies on the predictions of a multidimensional nearest-neighbor search.

Having been recently acquainted with R, I am looking for packages/functions/tests to help me analyze various elements of my strategies' performance. Particularly, I am interested in two things:
1. Packages/functions/metrics that gauge the efficacy of my predictor.
2. Packages/functions/metrics that gauge the relative ""profitability"" of one variation to another.

If you know something that I should take a look at, please do not hesitate to post it!
",r,financial,forecasting,statistical-analysis,,12/23/2011 09:21:08,off topic,1,135,12,"Which R functions are useful for analysis of an investment strategy's profitability? I have multiple variations of an automated strategy for trading certain investment vehicles. For each of these variations I have cross-validated backtests on historic data. I  need to pick the best-performing test. There is significant variation between the tests in terms of trades per day, net position size, etc. This makes it difficult to compare one to another.

The nature of the test relies on the predictions of a multidimensional nearest-neighbor search.

Having been recently acquainted with R, I am looking for packages/functions/tests to help me analyze various elements of my strategies' performance. Particularly, I am interested in two things:
1. Packages/functions/metrics that gauge the efficacy of my predictor.
2. Packages/functions/metrics that gauge the relative ""profitability"" of one variation to another.

If you know something that I should take a look at, please do not hesitate to post it!
",4
7536639,09/24/2011 02:39:04,235298,12/19/2009 22:49:01,34,1,ggplot heatmap legend doesn't represent negative numbers,"I'm trying to make a heat map for a data set that includes negative numbers:

Ne BR Error  
10000 0.00001 1.62  
10000 0.000001 -1.03  
10000 0.0000001 -0.124  
100000 0.00001 36.73  
100000 0.000001 5.86  
100000 0.0000001 -0.79  
1000000 0.00001 -8.335  
1000000 0.000001 39.465  
1000000 0.0000001  2.59  

I've used this code:  


library(ggplot2)  
data=read.csv('full_path')  
(p<- ggplot(data[1:9,], aes(Base_Ne, Birth_Rate)) +geom_tile(aes(fill=Error), colour=""white"") + scale_fill_gradient(low=""white"", high=""black"") + scale_x_log('Ne') + scale_y_log('Birth Rate') + opts(axis.ticks = theme_blank()))  

This produces a fine heatmap, but the legend doesn't account for the negative numbers.  The colors appear to correctly reflect the negative values, but the legend stops at zero, which is light grey.  How can I get a legend out of this that covers the full range of the data in the 'Error' column of my dataframe?

![heatmap](~/Desktop/Rplot.pdf)",r,legend,heatmap,ggplot,,,open,0,142,7,"ggplot heatmap legend doesn't represent negative numbers I'm trying to make a heat map for a data set that includes negative numbers:

Ne BR Error  
10000 0.00001 1.62  
10000 0.000001 -1.03  
10000 0.0000001 -0.124  
100000 0.00001 36.73  
100000 0.000001 5.86  
100000 0.0000001 -0.79  
1000000 0.00001 -8.335  
1000000 0.000001 39.465  
1000000 0.0000001  2.59  

I've used this code:  


library(ggplot2)  
data=read.csv('full_path')  
(p<- ggplot(data[1:9,], aes(Base_Ne, Birth_Rate)) +geom_tile(aes(fill=Error), colour=""white"") + scale_fill_gradient(low=""white"", high=""black"") + scale_x_log('Ne') + scale_y_log('Birth Rate') + opts(axis.ticks = theme_blank()))  

This produces a fine heatmap, but the legend doesn't account for the negative numbers.  The colors appear to correctly reflect the negative values, but the legend stops at zero, which is light grey.  How can I get a legend out of this that covers the full range of the data in the 'Error' column of my dataframe?

![heatmap](~/Desktop/Rplot.pdf)",4
9291323,02/15/2012 10:05:31,1172558,01/27/2012 00:52:40,17,1,Error in glmer: NA/NaN/Inf in foreign function call (arg 1),"I'm trying to fit the model

    glmer(trans.dummy ~ pop + year + (year | munname), data=pool, family=binomial(link = ""logit""), REML=T, verbose=T)

but I keep receiving the following error:

    Error in glm.fit(fr$X, fr$Y, weights = wts, offset = offset, family = family,  : 
    NA/NaN/Inf in foreign function call (arg 1)

I omitted NAs, changed the model specification, transformed pop to log(pop) but nothing solved the problem. I think the problem is on variable 'pop' given that is the only one that causes that problem. When I run

    mod6 <- glmer(trans.dummy ~ constituency.coa + I(governat.part) + I(district2) + gdp.cap + year + ifdm + year + (year | munname) + (year | uf), data=pool, family=binomial(link = ""logit""), REML=T, verbose=T)

I don't have any problem.

Any idea about what is happening?",r,error-message,lme4,,,,open,0,138,10,"Error in glmer: NA/NaN/Inf in foreign function call (arg 1) I'm trying to fit the model

    glmer(trans.dummy ~ pop + year + (year | munname), data=pool, family=binomial(link = ""logit""), REML=T, verbose=T)

but I keep receiving the following error:

    Error in glm.fit(fr$X, fr$Y, weights = wts, offset = offset, family = family,  : 
    NA/NaN/Inf in foreign function call (arg 1)

I omitted NAs, changed the model specification, transformed pop to log(pop) but nothing solved the problem. I think the problem is on variable 'pop' given that is the only one that causes that problem. When I run

    mod6 <- glmer(trans.dummy ~ constituency.coa + I(governat.part) + I(district2) + gdp.cap + year + ifdm + year + (year | munname) + (year | uf), data=pool, family=binomial(link = ""logit""), REML=T, verbose=T)

I don't have any problem.

Any idea about what is happening?",3
6392266,06/17/2011 21:58:15,242673,01/03/2010 12:44:48,313,15,rotating the grid to plot horizontal errors bars with Hmisc::xYplot in R,"I'm using xYplot to plot my regressions results with error bars. However, xYplot only plots horizontal error bars, and I need vertical error bars. Looking around for a solution, I found [this thread][1] where someone asked roughly the same question. After some messages, the user who asked the question says that ""I just  discovered that using the xYplot (Hmisc) and rotating the grid
viewport (and the labels etc) given me exactly what I need"".

So I looked around on how to rotate the grid and found that using grid library and pushviewport etc. you can rotate the grid. However, my code isn't working. Here is what I tryed so far:

    estimate=structure(list(coefi = c(-5.08608456607495, -4.17906898422091, 
    -2.85696514398422, -3.06968196245069, -2.73660002514793, -1.0017403629931, 
    -1.66291850690335, 0.431265159072978, -0.472895611533531, 0.845421348865878, 
    -0.437434919008876, 0.269041451577909, -0.233066564595661, 0.0137190330621302, 
    -2.94808159763314, 1.9166875739645), lower = c(-8.1895, -6.8485, 
    -5.214125, -5.532875, -5.106625, -3.271625, -3.97375, -0.09773, 
    -1.340625, 0.415125, -0.86615, 0.02665125, -0.5861, -2.079, -5.626625, 
    0.8115125), upper = c(-2.11475, -1.611125, -0.5602375, -0.7309625, 
    -0.3721375, 1.259875, 0.7167875, 0.9672875, 0.39035, 1.30025, 
    -0.05634125, 0.5115, 0.07237875, 2.14275, -0.3653, 4.202625), 
    x = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 
    16)), .Names = c(""coefi"", ""lower"", ""upper"", ""x""), row.names = c(""alpha.1."", 
    ""alpha.2."", ""alpha.3."", ""alpha.4."", ""alpha.5."", ""alpha.6."", ""alpha.7."", 
    ""b.income"", ""b.democracy"", ""b.ginete"", ""b.educ"", ""b.patent"", 
    ""b.fdi"", ""b.0"", ""mu.alpha"", ""sigma.alpha""), class = ""data.frame"")

    legenda=c(as.character(seq(1970,2000,5)),""PIB_pc"", ""democ"",  ""legis"", ""educ"", ""patent"", ""FDI"",  ""b.0"", ""mu.ano"", ""var.ano"" )

    grid.newpage()
    pushViewport(viewport(angle = 90, name = ""VP""))
    upViewport() 
    xYplot(Cbind(coefi,lower, upper) ~x, data=estimate, , varwidth = TRUE, ylab=""Betas"",
    xlab=""Inclinação das Covariáveis com respectivos 95% intervalos de credibilidade \n N=409"",
    ylim=with(estimate,c(min(lower)-.5, max(upper)+.5)),  scales=list(cex=1.2, x = list(at=seq(1,16,     by=1), labels = legenda)) ,abline=c(list(h=0), lty=""dotted"", col= ""grey69""), xlab.top=""Adesão ao Tratado de Cooperação de Patentes, 1970-2000"", draw.in = ""VP"")

I'd apreciate any help.

  [1]: http://r.789695.n4.nabble.com/R-Hmisc-or-Lattice-plot-with-error-bars-Depth-independent-variable-on-Y-axis-td811365.html",r,lattice,,,,,open,0,365,12,"rotating the grid to plot horizontal errors bars with Hmisc::xYplot in R I'm using xYplot to plot my regressions results with error bars. However, xYplot only plots horizontal error bars, and I need vertical error bars. Looking around for a solution, I found [this thread][1] where someone asked roughly the same question. After some messages, the user who asked the question says that ""I just  discovered that using the xYplot (Hmisc) and rotating the grid
viewport (and the labels etc) given me exactly what I need"".

So I looked around on how to rotate the grid and found that using grid library and pushviewport etc. you can rotate the grid. However, my code isn't working. Here is what I tryed so far:

    estimate=structure(list(coefi = c(-5.08608456607495, -4.17906898422091, 
    -2.85696514398422, -3.06968196245069, -2.73660002514793, -1.0017403629931, 
    -1.66291850690335, 0.431265159072978, -0.472895611533531, 0.845421348865878, 
    -0.437434919008876, 0.269041451577909, -0.233066564595661, 0.0137190330621302, 
    -2.94808159763314, 1.9166875739645), lower = c(-8.1895, -6.8485, 
    -5.214125, -5.532875, -5.106625, -3.271625, -3.97375, -0.09773, 
    -1.340625, 0.415125, -0.86615, 0.02665125, -0.5861, -2.079, -5.626625, 
    0.8115125), upper = c(-2.11475, -1.611125, -0.5602375, -0.7309625, 
    -0.3721375, 1.259875, 0.7167875, 0.9672875, 0.39035, 1.30025, 
    -0.05634125, 0.5115, 0.07237875, 2.14275, -0.3653, 4.202625), 
    x = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 
    16)), .Names = c(""coefi"", ""lower"", ""upper"", ""x""), row.names = c(""alpha.1."", 
    ""alpha.2."", ""alpha.3."", ""alpha.4."", ""alpha.5."", ""alpha.6."", ""alpha.7."", 
    ""b.income"", ""b.democracy"", ""b.ginete"", ""b.educ"", ""b.patent"", 
    ""b.fdi"", ""b.0"", ""mu.alpha"", ""sigma.alpha""), class = ""data.frame"")

    legenda=c(as.character(seq(1970,2000,5)),""PIB_pc"", ""democ"",  ""legis"", ""educ"", ""patent"", ""FDI"",  ""b.0"", ""mu.ano"", ""var.ano"" )

    grid.newpage()
    pushViewport(viewport(angle = 90, name = ""VP""))
    upViewport() 
    xYplot(Cbind(coefi,lower, upper) ~x, data=estimate, , varwidth = TRUE, ylab=""Betas"",
    xlab=""Inclinação das Covariáveis com respectivos 95% intervalos de credibilidade \n N=409"",
    ylim=with(estimate,c(min(lower)-.5, max(upper)+.5)),  scales=list(cex=1.2, x = list(at=seq(1,16,     by=1), labels = legenda)) ,abline=c(list(h=0), lty=""dotted"", col= ""grey69""), xlab.top=""Adesão ao Tratado de Cooperação de Patentes, 1970-2000"", draw.in = ""VP"")

I'd apreciate any help.

  [1]: http://r.789695.n4.nabble.com/R-Hmisc-or-Lattice-plot-with-error-bars-Depth-independent-variable-on-Y-axis-td811365.html",2
9584576,03/06/2012 13:24:34,1252286,03/06/2012 13:21:24,1,0,can i divert text output ( cat() &/or print() ) to a separate window,"platform:  R 2.14.2   -    Windows XP 

When I use cat()  or print()  in a script, the output text is of course 
interlaced with the lines of the script.

I can  sink()  the output to a textfile and then play the file back at the 
end of the script, but that's not really what I'm after.

It would be nice if the lines of text output could appear in a separate 
window,  in the same sort of way that you'd use  windows() for graphical 
output.

Apologies if I'm missing something obvious.

        Bob Kinley",r,,,,,,open,0,108,14,"can i divert text output ( cat() &/or print() ) to a separate window platform:  R 2.14.2   -    Windows XP 

When I use cat()  or print()  in a script, the output text is of course 
interlaced with the lines of the script.

I can  sink()  the output to a textfile and then play the file back at the 
end of the script, but that's not really what I'm after.

It would be nice if the lines of text output could appear in a separate 
window,  in the same sort of way that you'd use  windows() for graphical 
output.

Apologies if I'm missing something obvious.

        Bob Kinley",1
10485999,05/07/2012 16:59:09,1372997,05/03/2012 15:56:40,1,0,Unexpected input while building package in R,"I am a newbie in R, and I am tryiing to build an R package but I keep getting an unexpected input error when I try using the build, check or install commands. I used the following command to generate the skeleton:

package.skeleton(""test"")

After this I went to the command prompt and to the directory with the test folder and ran the command:

R CMD build test

I got the following error message:
Error: 37:1: unexpected input
37: 
     ^

I even tried the check and INSTALL commands on the package and got the same error. I tried googling the error code and the error message but it seems like I should not be getting such an error while building the package itself. Everywhere I checked it seemed like simply typing that command in command prompt should work. I am wondering if anyone knows what I am doing wrong? Thanks for the help. 

I am using R 2.15.0 on Windows 7. ",r,packaging,,,,05/09/2012 06:36:59,too localized,1,159,7,"Unexpected input while building package in R I am a newbie in R, and I am tryiing to build an R package but I keep getting an unexpected input error when I try using the build, check or install commands. I used the following command to generate the skeleton:

package.skeleton(""test"")

After this I went to the command prompt and to the directory with the test folder and ran the command:

R CMD build test

I got the following error message:
Error: 37:1: unexpected input
37: 
     ^

I even tried the check and INSTALL commands on the package and got the same error. I tried googling the error code and the error message but it seems like I should not be getting such an error while building the package itself. Everywhere I checked it seemed like simply typing that command in command prompt should work. I am wondering if anyone knows what I am doing wrong? Thanks for the help. 

I am using R 2.15.0 on Windows 7. ",2
7942706,10/30/2011 01:08:57,827370,07/04/2011 00:05:30,13,0,how to use estimated parameters to calculate probabilities using a density function?,"I used a qqplot to find my data is from the t distribution.  Then i used the fitdistr function to estimate the parameters of the model for the t distribution and i got a df=4, scale=181.28, and location=10.68.  I then want to calculate the probability that X <= -1 using the parameters of the model and my data.  How should I approach this.",r,,,,,10/30/2011 10:42:12,off topic,1,66,12,"how to use estimated parameters to calculate probabilities using a density function? I used a qqplot to find my data is from the t distribution.  Then i used the fitdistr function to estimate the parameters of the model for the t distribution and i got a df=4, scale=181.28, and location=10.68.  I then want to calculate the probability that X <= -1 using the parameters of the model and my data.  How should I approach this.",1
11665265,07/26/2012 08:26:12,1310039,04/03/2012 09:30:34,10,0,How to get diagnostic information when attempting a PCM model fit using eRm package in R,"I am new to Rasch modelling, attempting to fit a partial credit model using the PCM() function in the eRm package in R. My data set comprises 88 items in 43 individuals with possible scores 0,1,2,3. I have a significant missing data rate (about 20%). The scores are from an instrument whose Rasch psychometrics (e.g unidimensionality) have been established in other clinical populations. 

Attempting either a PCM or RSM model fit simply hangs - spinning ""in progress"" wheel but no error messages and I'm trying to generate diagnostics. Setting options(warn=1) doesn't appear to help. Alternatively I note that PCM and RSM use the nlm function: is it possible to pass a print.level flag value?

Thanks",r,,,,,07/26/2012 12:50:59,not a real question,1,114,16,"How to get diagnostic information when attempting a PCM model fit using eRm package in R I am new to Rasch modelling, attempting to fit a partial credit model using the PCM() function in the eRm package in R. My data set comprises 88 items in 43 individuals with possible scores 0,1,2,3. I have a significant missing data rate (about 20%). The scores are from an instrument whose Rasch psychometrics (e.g unidimensionality) have been established in other clinical populations. 

Attempting either a PCM or RSM model fit simply hangs - spinning ""in progress"" wheel but no error messages and I'm trying to generate diagnostics. Setting options(warn=1) doesn't appear to help. Alternatively I note that PCM and RSM use the nlm function: is it possible to pass a print.level flag value?

Thanks",1
8182612,11/18/2011 12:44:16,592419,01/27/2011 15:14:21,88,2,Change x-axis label distance in qplot,"I have a qplot and when I graph it, it splits up the x-axis automatically. I want to change that to split it up by defined sequential breaks, so I tried this:

    breaks <- seq(a,b,7)
    qplot(data=data, x=xvar, y=yvar, colour=yvar, group=grouping, geom=c(""point"", ""line"")) + scale_x_discrete(breaks = breaks, labels=paste(""Break"", breaks))

However, this didnt work. In fact, nothing shows up on the x-axis when I do that. Thoughts?",r,ggplot2,,,,,open,0,70,6,"Change x-axis label distance in qplot I have a qplot and when I graph it, it splits up the x-axis automatically. I want to change that to split it up by defined sequential breaks, so I tried this:

    breaks <- seq(a,b,7)
    qplot(data=data, x=xvar, y=yvar, colour=yvar, group=grouping, geom=c(""point"", ""line"")) + scale_x_discrete(breaks = breaks, labels=paste(""Break"", breaks))

However, this didnt work. In fact, nothing shows up on the x-axis when I do that. Thoughts?",2
5746705,04/21/2011 16:01:41,688852,04/02/2011 11:12:13,6,0,How to rollback to R 2.12.1 version on Ubuntu?,I was using R 2.12.1 on my desktop and recently the R got updated to the new version R 2.13.0 which is not I want. Is there anyway to use the R 2.12.1 again and how can I use that? ,r,ubuntu,,,,04/25/2011 19:08:41,off topic,1,41,9,How to rollback to R 2.12.1 version on Ubuntu? I was using R 2.12.1 on my desktop and recently the R got updated to the new version R 2.13.0 which is not I want. Is there anyway to use the R 2.12.1 again and how can I use that? ,2
9188505,02/08/2012 05:59:56,707145,04/14/2011 02:08:22,1145,30,Column alignment in xtable output,"I'm using `xtable` to manage `R` output in `Sweave`. See code below:

    CC <- data.frame(
        y = c(449, 413, 326, 409, 358, 291, 341, 278, 312)/12,
        P = ordered(gl(3, 3)), N = ordered(gl(3, 1, 9))
    )
    CC.aov <- aov(y ~ N * P, data = CC , weights = rep(12, 9))
    Summary <- summary(CC.aov, split = list(N = list(L = 1, Q = 2),
                                            P = list(L = 1, Q = 2)))
    Summary
    
                Df Sum Sq Mean Sq
    N            2 1016.7   508.3
      N: L       1 1012.5  1012.5
      N: Q       1    4.2     4.2
    P            2  917.4   458.7
      P: L       1  917.3   917.3
      P: Q       1    0.0     0.0
    N:P          4  399.3    99.8
      N:P: L.L   1  184.1   184.1
      N:P: Q.L   1  152.1   152.1
      N:P: L.Q   1   49.0    49.0
      N:P: Q.Q   1   14.1    14.1

I like to indent the first column as shown in the output above. But when I use `xtable(Summary)` the first column is aligned left. I know how to align left, right or center but could not figure out how to get the output as indented in the first. Any help in this regard will be highly appreciated. Thanks",r,latex,sweave,xtable,,,open,0,433,5,"Column alignment in xtable output I'm using `xtable` to manage `R` output in `Sweave`. See code below:

    CC <- data.frame(
        y = c(449, 413, 326, 409, 358, 291, 341, 278, 312)/12,
        P = ordered(gl(3, 3)), N = ordered(gl(3, 1, 9))
    )
    CC.aov <- aov(y ~ N * P, data = CC , weights = rep(12, 9))
    Summary <- summary(CC.aov, split = list(N = list(L = 1, Q = 2),
                                            P = list(L = 1, Q = 2)))
    Summary
    
                Df Sum Sq Mean Sq
    N            2 1016.7   508.3
      N: L       1 1012.5  1012.5
      N: Q       1    4.2     4.2
    P            2  917.4   458.7
      P: L       1  917.3   917.3
      P: Q       1    0.0     0.0
    N:P          4  399.3    99.8
      N:P: L.L   1  184.1   184.1
      N:P: Q.L   1  152.1   152.1
      N:P: L.Q   1   49.0    49.0
      N:P: Q.Q   1   14.1    14.1

I like to indent the first column as shown in the output above. But when I use `xtable(Summary)` the first column is aligned left. I know how to align left, right or center but could not figure out how to get the output as indented in the first. Any help in this regard will be highly appreciated. Thanks",4
8536529,12/16/2011 15:43:42,914308,08/26/2011 14:38:22,667,9,R resetting a cumsum to zero at the start of each year,"I have a `dataframe` with a bunch of donations data.  I take the data and arrange it in time order from oldest to most recent gifts.  Next I add a column containing a cumulative sum of the gifts over time.  The data has multiple years of data and I was looking for a good way to reset the `cumsum` to 0 at the start of each year (the year starts and ends July 1st for fiscal purposes).

This is how it currently is:

    id        date          giftamt      cumsum()
    005       01-05-2001     20.00        20.00
    007       06-05-2001     25.00        45.00
    009       12-05-2001     20.00        65.00
    012       02-05-2002     30.00        95.00
    015       08-05-2002     50.00       145.00
    025       12-05-2002     25.00       170.00
    ...          ...          ...         ...

this is how I would like it to look:

    id        date          giftamt      cumsum()
    005       01-05-2001     20.00        20.00
    007       06-05-2001     25.00        45.00
    009       12-05-2001     20.00        20.00
    012       02-05-2002     30.00        50.00
    015       08-05-2002     50.00        50.00
    025       12-05-2002     25.00        75.00
    ...          ...          ...          ...

Any suggestions?",r,,,,,,open,0,503,12,"R resetting a cumsum to zero at the start of each year I have a `dataframe` with a bunch of donations data.  I take the data and arrange it in time order from oldest to most recent gifts.  Next I add a column containing a cumulative sum of the gifts over time.  The data has multiple years of data and I was looking for a good way to reset the `cumsum` to 0 at the start of each year (the year starts and ends July 1st for fiscal purposes).

This is how it currently is:

    id        date          giftamt      cumsum()
    005       01-05-2001     20.00        20.00
    007       06-05-2001     25.00        45.00
    009       12-05-2001     20.00        65.00
    012       02-05-2002     30.00        95.00
    015       08-05-2002     50.00       145.00
    025       12-05-2002     25.00       170.00
    ...          ...          ...         ...

this is how I would like it to look:

    id        date          giftamt      cumsum()
    005       01-05-2001     20.00        20.00
    007       06-05-2001     25.00        45.00
    009       12-05-2001     20.00        20.00
    012       02-05-2002     30.00        50.00
    015       08-05-2002     50.00        50.00
    025       12-05-2002     25.00        75.00
    ...          ...          ...          ...

Any suggestions?",1
5200593,03/04/2011 23:58:28,65148,02/11/2009 15:56:00,841,27,Change stack order of True and False in R/ggplot2,"When plotting logical values with qplot in ggplot2 the False counts are always on the bottom, but more often than not I want True on the bottom so that it is easier to read.  Here is an example


    y<-as.logical(rbinom(100,1,0.7))
    x<-factor(rep(letters[1:2], each=50))
    qplot(x,fill=y, geom='bar')

How can I get the counts for TRUE on the bottom of the stack?
",r,ggplot2,,,,,open,0,66,9,"Change stack order of True and False in R/ggplot2 When plotting logical values with qplot in ggplot2 the False counts are always on the bottom, but more often than not I want True on the bottom so that it is easier to read.  Here is an example


    y<-as.logical(rbinom(100,1,0.7))
    x<-factor(rep(letters[1:2], each=50))
    qplot(x,fill=y, geom='bar')

How can I get the counts for TRUE on the bottom of the stack?
",2
10503784,05/08/2012 17:50:42,61855,02/03/2009 09:34:36,4597,123,caret: Error when using LOGV,"I'm trying to use the R caret module for model generation and I want to use LGOCV (Leave Group Out Cross Validation). As I need to generate multiple models and compare them, I want first to partition the data into the groups for cross validation and then use the same partitions for each model generation.

I wrote the following method (simplified example which also throws the error)

    library(cart)
    data(trees)
    
    testModels <- function() {
        fold=createFolds(trees$Volume, 3)
        cat(""Fold is\n"")
        cat(str(fold))
    
        formula=Volume~Girth+Height
    
        myControl <- trainControl(method='LGOCV', index=fold)
    
        model_LM <<- train(formula, data=trees,  method='lm', trControl=myControl)
        model_CART <<- train(formula, data=trees,  method='rpart', trControl=myControl)
    }

If you run this method multiple times, you get the following error sometimes. In some cases, the function just runs through, in other cases you get this error:

> Warning message: In nominalTrainWorkflow(dat = trainData, info =
> trainInfo, method = method,  :   There were missing values in
> resampled performance measures.

What does this error mean and how do I make it go away? I searched on the internet, not a single hit for this error-message. I traced the error down to the `rpart` model generation. It somehow outputs this error message, all other mode-generation-methods work fine!

Everything works fine if I let `caret` generate the partitions, so the error is somehow related to fold/partition generation.


    > sessionInfo()
    R version 2.15.0 (2012-03-30)
    Platform: x86_64-pc-linux-gnu (64-bit)
    
    locale:
     [1] LC_CTYPE=en_GB.UTF-8       LC_NUMERIC=C              
     [3] LC_TIME=en_GB.UTF-8        LC_COLLATE=en_GB.UTF-8    
     [5] LC_MONETARY=en_GB.UTF-8    LC_MESSAGES=en_GB.UTF-8   
     [7] LC_PAPER=C                 LC_NAME=C                 
     [9] LC_ADDRESS=C               LC_TELEPHONE=C            
    [11] LC_MEASUREMENT=en_GB.UTF-8 LC_IDENTIFICATION=C       
    
    attached base packages:
    [1] parallel  stats     graphics  grDevices utils     datasets  methods  
    [8] base     
    
    other attached packages:
     [1] earth_3.2-3           plotrix_3.4           plotmo_1.3-1         
     [4] leaps_2.9             doMC_1.2.5            multicore_0.1-7      
     [7] iterators_1.0.6       forecast_3.20         RcppArmadillo_0.3.0.2
    [10] Rcpp_0.9.10           fracdiff_1.4-1        tseries_0.10-28      
    [13] zoo_1.7-7             quadprog_1.5-4        caret_5.15-023       
    [16] foreach_1.4.0         cluster_1.14.2        reshape_0.8.4        
    [19] plyr_1.7.1            lattice_0.20-6        mda_0.4-2            
    [22] class_7.3-3           rpart_3.1-52          data.table_1.8.0     
    
    loaded via a namespace (and not attached):
    [1] codetools_0.2-8 compiler_2.15.0 grid_2.15.0    

",r,,,,,,open,0,794,5,"caret: Error when using LOGV I'm trying to use the R caret module for model generation and I want to use LGOCV (Leave Group Out Cross Validation). As I need to generate multiple models and compare them, I want first to partition the data into the groups for cross validation and then use the same partitions for each model generation.

I wrote the following method (simplified example which also throws the error)

    library(cart)
    data(trees)
    
    testModels <- function() {
        fold=createFolds(trees$Volume, 3)
        cat(""Fold is\n"")
        cat(str(fold))
    
        formula=Volume~Girth+Height
    
        myControl <- trainControl(method='LGOCV', index=fold)
    
        model_LM <<- train(formula, data=trees,  method='lm', trControl=myControl)
        model_CART <<- train(formula, data=trees,  method='rpart', trControl=myControl)
    }

If you run this method multiple times, you get the following error sometimes. In some cases, the function just runs through, in other cases you get this error:

> Warning message: In nominalTrainWorkflow(dat = trainData, info =
> trainInfo, method = method,  :   There were missing values in
> resampled performance measures.

What does this error mean and how do I make it go away? I searched on the internet, not a single hit for this error-message. I traced the error down to the `rpart` model generation. It somehow outputs this error message, all other mode-generation-methods work fine!

Everything works fine if I let `caret` generate the partitions, so the error is somehow related to fold/partition generation.


    > sessionInfo()
    R version 2.15.0 (2012-03-30)
    Platform: x86_64-pc-linux-gnu (64-bit)
    
    locale:
     [1] LC_CTYPE=en_GB.UTF-8       LC_NUMERIC=C              
     [3] LC_TIME=en_GB.UTF-8        LC_COLLATE=en_GB.UTF-8    
     [5] LC_MONETARY=en_GB.UTF-8    LC_MESSAGES=en_GB.UTF-8   
     [7] LC_PAPER=C                 LC_NAME=C                 
     [9] LC_ADDRESS=C               LC_TELEPHONE=C            
    [11] LC_MEASUREMENT=en_GB.UTF-8 LC_IDENTIFICATION=C       
    
    attached base packages:
    [1] parallel  stats     graphics  grDevices utils     datasets  methods  
    [8] base     
    
    other attached packages:
     [1] earth_3.2-3           plotrix_3.4           plotmo_1.3-1         
     [4] leaps_2.9             doMC_1.2.5            multicore_0.1-7      
     [7] iterators_1.0.6       forecast_3.20         RcppArmadillo_0.3.0.2
    [10] Rcpp_0.9.10           fracdiff_1.4-1        tseries_0.10-28      
    [13] zoo_1.7-7             quadprog_1.5-4        caret_5.15-023       
    [16] foreach_1.4.0         cluster_1.14.2        reshape_0.8.4        
    [19] plyr_1.7.1            lattice_0.20-6        mda_0.4-2            
    [22] class_7.3-3           rpart_3.1-52          data.table_1.8.0     
    
    loaded via a namespace (and not attached):
    [1] codetools_0.2-8 compiler_2.15.0 grid_2.15.0    

",1
10257264,04/21/2012 07:53:00,1347961,04/21/2012 06:46:23,1,0,(R) RSVGTipsDevice : How to change location of tooltip box?,"In RSVGTipsDevice, default positioning of tooltip box occludes other regions of the image (or plot). Is there a way to place tooltip box at the bottom/top of the plot, so that it doesn't block other data-points ?

Screenshot of defalut tooltip layout : https://docs.google.com/open?id=0B3tgXHsXj9tBeEFXLUg3X3J3eW8",r,svg,tooltip,,,,open,0,43,10,"(R) RSVGTipsDevice : How to change location of tooltip box? In RSVGTipsDevice, default positioning of tooltip box occludes other regions of the image (or plot). Is there a way to place tooltip box at the bottom/top of the plot, so that it doesn't block other data-points ?

Screenshot of defalut tooltip layout : https://docs.google.com/open?id=0B3tgXHsXj9tBeEFXLUg3X3J3eW8",3
6931008,08/03/2011 18:01:01,597551,01/31/2011 21:59:15,10,0,"How to group one column into intervals, and aggregate the corresponding values from another column","In a datafreame i have 2 variables, one for number of Free Samples sent, and the other for Number of Purchases resulted.  I would like to group free sample variables into intervals of say 0, 1 to 5, 5 to 10, more than 10.  Then cumulate the observations from the number of purchases column withing each of the intervals to present as a table.

Any help would be greatly appreciated",r,grouping,aggregate,intervals,,01/21/2012 13:51:00,too localized,1,71,15,"How to group one column into intervals, and aggregate the corresponding values from another column In a datafreame i have 2 variables, one for number of Free Samples sent, and the other for Number of Purchases resulted.  I would like to group free sample variables into intervals of say 0, 1 to 5, 5 to 10, more than 10.  Then cumulate the observations from the number of purchases column withing each of the intervals to present as a table.

Any help would be greatly appreciated",4
1295955,08/18/2009 19:26:27,143305,07/23/2009 00:34:05,2361,83,What is the most useful R trick?,"In order to share some more tips and tricks for [**R**][1], what is you single-most useful feature or trick?  Clever vectorization?  Data input/output?  Visualization and graphics?  Statistical analysis?  Special functions?  The interactive environment itself? 

One item per post, and we will see if we get a winner by means of votes.


  [1]: http://www.r-project.org",r,tips-and-tricks,,,,11/18/2011 04:48:02,not constructive,1,60,7,"What is the most useful R trick? In order to share some more tips and tricks for [**R**][1], what is you single-most useful feature or trick?  Clever vectorization?  Data input/output?  Visualization and graphics?  Statistical analysis?  Special functions?  The interactive environment itself? 

One item per post, and we will see if we get a winner by means of votes.


  [1]: http://www.r-project.org",2
1173463,07/23/2009 18:01:31,142477,07/22/2009 02:24:55,11,1,Recommendations for Windows text editor for R,"Any recommendations for a good Windows text editor for R?  

I've been using Tinn-R, and it's been working reasonably well, but am curious to know what else people use.  ",r,,,,,06/10/2012 10:52:16,not constructive,1,32,7,"Recommendations for Windows text editor for R Any recommendations for a good Windows text editor for R?  

I've been using Tinn-R, and it's been working reasonably well, but am curious to know what else people use.  ",1
10847883,06/01/2012 09:38:16,1430274,06/01/2012 08:49:12,1,0,R readLines HTML error,"I made a simple webscraping function in order to extract information from thousands of ads on a website (the links of the ad are in the list of character ""links"")

My extraction function began with : 

>  infos <- function(x) {
>      library(XML)   
>      script <- readLines(x)

+ the rest of the script is dedicated to extract the informations I need. 

I first extracted the ads links, so, at the end of it, I apply the infos function to these links, using : 

      info <- unlist(lapply(links, infos)) 


My problem is that, when I did this, I got this error almost every time : 

 

    Error in file(con, ""r"") : cannot open the connection In addition: 
    Warning message: In file(con, ""r"") : cannot open: HTTP status was '404 Not Found'

and I couldn't get any informations at the end (the ""info"" list didn't form itself). 


As I was trying to solve it, I realised the error came from the ""readLines"", everything else worked fine. 
So, as a solution, I tried : 

     script <- try(readLines(x), silent = T) 
     if (inherits(script, ""try.error"")) script <- NA

This time, the function worked, and I was able to get ""info"", with lapply...
BUT : I only obtain a certain number of informations, for example, on 100 links, I extracted datas from only 95 links, and got nothing for the 5 other.
At the end (after modifying what I obtain with lapply into a matrix), I have a
I wondered whether this bug came from the links or from the readLines, so I used a simple script in order to see which links hadn't worked properly :

     for(i in seq(along = links)) {
      script <- try(readLines(links[i]), silent=TRUE)
      if (inherits(script, ""try-error"")) cat(links[i], ""bad\n"")
      else cat(links[i], ""ok\n"")
    }

I ran this several time, and it appeared the ""broken"" links were never the same.
On an hundred links, for 20 tries I always got between 10 and 0 error, randomly taken in the 100 links...


So, I suppose the error definitely comes from the readLines...
My problem is, **how to solve it ?** 

I tried : 

      scripta <- try(readLines(x), silent = T) 
      if (inherits(script, ""try.error"")) {
        Sys.Sleep(10)
        script <- try(readLines(x), silent=T)} 
      else { script <- scripta }

With this script, the time the function take to finish is proportional to the number of error I got at the end, meaning the Sys.sleep apparently works... However I get as many error as before (a random number between 1 and 10, often 5).


I really don't know what to do and haven't found anything similar...





To recap : 

- I randomly get errors with readLines, and don't know where it comes from.
- I'd like to solve it to be able to extract all the informations I want, and not a random part of them. 



Thanks.

",r,,,,,,open,0,539,4,"R readLines HTML error I made a simple webscraping function in order to extract information from thousands of ads on a website (the links of the ad are in the list of character ""links"")

My extraction function began with : 

>  infos <- function(x) {
>      library(XML)   
>      script <- readLines(x)

+ the rest of the script is dedicated to extract the informations I need. 

I first extracted the ads links, so, at the end of it, I apply the infos function to these links, using : 

      info <- unlist(lapply(links, infos)) 


My problem is that, when I did this, I got this error almost every time : 

 

    Error in file(con, ""r"") : cannot open the connection In addition: 
    Warning message: In file(con, ""r"") : cannot open: HTTP status was '404 Not Found'

and I couldn't get any informations at the end (the ""info"" list didn't form itself). 


As I was trying to solve it, I realised the error came from the ""readLines"", everything else worked fine. 
So, as a solution, I tried : 

     script <- try(readLines(x), silent = T) 
     if (inherits(script, ""try.error"")) script <- NA

This time, the function worked, and I was able to get ""info"", with lapply...
BUT : I only obtain a certain number of informations, for example, on 100 links, I extracted datas from only 95 links, and got nothing for the 5 other.
At the end (after modifying what I obtain with lapply into a matrix), I have a
I wondered whether this bug came from the links or from the readLines, so I used a simple script in order to see which links hadn't worked properly :

     for(i in seq(along = links)) {
      script <- try(readLines(links[i]), silent=TRUE)
      if (inherits(script, ""try-error"")) cat(links[i], ""bad\n"")
      else cat(links[i], ""ok\n"")
    }

I ran this several time, and it appeared the ""broken"" links were never the same.
On an hundred links, for 20 tries I always got between 10 and 0 error, randomly taken in the 100 links...


So, I suppose the error definitely comes from the readLines...
My problem is, **how to solve it ?** 

I tried : 

      scripta <- try(readLines(x), silent = T) 
      if (inherits(script, ""try.error"")) {
        Sys.Sleep(10)
        script <- try(readLines(x), silent=T)} 
      else { script <- scripta }

With this script, the time the function take to finish is proportional to the number of error I got at the end, meaning the Sys.sleep apparently works... However I get as many error as before (a random number between 1 and 10, often 5).


I really don't know what to do and haven't found anything similar...





To recap : 

- I randomly get errors with readLines, and don't know where it comes from.
- I'd like to solve it to be able to extract all the informations I want, and not a random part of them. 



Thanks.

",1
11485344,07/14/2012 16:15:48,836015,07/08/2011 19:57:46,50,0,Better to learn Latent Dirichlet Allocation on single corpus or multiple corpora when word usage may differ?,"I am using Jonathan Chang's `lda` package for R comparing news stories from both state-friendly and independent media outlets in a non-democracy (for a similar approach, see Hall, Jurafsky, and Manning's piece on studying the history of ideas in linguistics). 

I am interested in the extent to which they cover the same topics and how they cover them. I can't figure out, however, if I should estimate one model across all corpora or if I should estimate different models for the different kinds of sources. On the one hand, I expect them to cover mostly the same *topics* (in the colloquial sense), but I also expect their word choices to be different, which would generate different ""topics"" (in the LDA/word-grouping sense).

Does anyone have any experience with this or advice? Thanks.
",r,nlp,lda,,,07/16/2012 01:02:44,off topic,1,130,17,"Better to learn Latent Dirichlet Allocation on single corpus or multiple corpora when word usage may differ? I am using Jonathan Chang's `lda` package for R comparing news stories from both state-friendly and independent media outlets in a non-democracy (for a similar approach, see Hall, Jurafsky, and Manning's piece on studying the history of ideas in linguistics). 

I am interested in the extent to which they cover the same topics and how they cover them. I can't figure out, however, if I should estimate one model across all corpora or if I should estimate different models for the different kinds of sources. On the one hand, I expect them to cover mostly the same *topics* (in the colloquial sense), but I also expect their word choices to be different, which would generate different ""topics"" (in the LDA/word-grouping sense).

Does anyone have any experience with this or advice? Thanks.
",3
1395174,09/08/2009 17:18:03,23929,09/30/2008 20:35:34,287,6,How to call R from within a web server (like Apache)?,"That is, is there an embedded R interpreter available?",r,apache,embedded,,,,open,0,9,11,"How to call R from within a web server (like Apache)? That is, is there an embedded R interpreter available?",3
4368548,12/06/2010 16:22:38,170352,09/08/2009 18:23:21,1623,66,Choosing the Row that has the most complete information,"Let's say I have rows like this: 

        First, Last, Address, Address 2, Email, Custom1, Custom2, Custom3
    1    A, B, C, D, E@E.com,1,2,3
    2    A,  , C, D, E@E.com,1,2,
    3     ,  ,  ,  , E@E.com,1, ,  

What I would like to to do is create a function that pulls that row which is most complete and I'm wondering if there are any packages or pre-existing methods (recommendations, even) for doing this. In the example above, I would like to to have a function that chooses row 1. 

I can't use complete.cases() or na.omit() because in many circumstances the cases are not complete and contain at least one NA. I've tried combining unique() with a number of specific pulls... but I'm not having much luck automating this manipulation task. ",r,,,,,,open,0,160,9,"Choosing the Row that has the most complete information Let's say I have rows like this: 

        First, Last, Address, Address 2, Email, Custom1, Custom2, Custom3
    1    A, B, C, D, E@E.com,1,2,3
    2    A,  , C, D, E@E.com,1,2,
    3     ,  ,  ,  , E@E.com,1, ,  

What I would like to to do is create a function that pulls that row which is most complete and I'm wondering if there are any packages or pre-existing methods (recommendations, even) for doing this. In the example above, I would like to to have a function that chooses row 1. 

I can't use complete.cases() or na.omit() because in many circumstances the cases are not complete and contain at least one NA. I've tried combining unique() with a number of specific pulls... but I'm not having much luck automating this manipulation task. ",1
11040670,06/14/2012 20:12:19,984532,10/07/2011 18:26:21,122,3,"In R, for which database engines there are more advanced (native) connection libraries?","I deal with many different database engines.
I am used to use the RODBC package but it has limits (e.g., only 256 chars retrieved for strings).

This questions asks for overview of all libraries within R which deal with connecting to databases and what do they add to the bare RODBC package or RJDBC.

I am only aware for those 2: (database: R library)

    MySQL:RMySQL
    postrgreSQL:RPostgreSQL
    MS SQL: none!


Are there others? (specifically I am most interested in connecting to MS SQL server ""natively"")
If there is no native library for a given database, then state so.


",r,odbc,database-connection,driver,rodbc,06/15/2012 09:10:23,not a real question,1,101,13,"In R, for which database engines there are more advanced (native) connection libraries? I deal with many different database engines.
I am used to use the RODBC package but it has limits (e.g., only 256 chars retrieved for strings).

This questions asks for overview of all libraries within R which deal with connecting to databases and what do they add to the bare RODBC package or RJDBC.

I am only aware for those 2: (database: R library)

    MySQL:RMySQL
    postrgreSQL:RPostgreSQL
    MS SQL: none!


Are there others? (specifically I am most interested in connecting to MS SQL server ""natively"")
If there is no native library for a given database, then state so.


",5
6437080,06/22/2011 08:36:27,809975,06/22/2011 08:36:27,1,0,bar chart of constant height for factors in time series,"I am a beginner to try R for making graphs. Please help me. I have data of multiple columns (time series). Each column holds factors (please see the one column example data below). I would like to make a constant height (say 1 unit) bar chart of the time series and would like to represent “A” and “B” in different colors with the DATE on the x axis. Any tip?
Thanking you in advance!
DATE                      GROUP
2011.06.18 00:00:00         R
2011.06.18 06:00:00         L
2011.06.18 12:00:00         R
2011.06.18 18:00:00         R
2011.06.19 00:00:00         L
2011.06.19 06:00:00         L
2011.06.19 12:00:00         R
2011.06.19 18:00:00         L
2011.06.20 00:00:00         L
2011.06.20 06:00:00         L
2011.06.20 12:00:00         R
2011.06.20 18:00:00         L
2011.06.21 00:00:00         R
2011.06.21 06:00:00         L
",r,,,,,,open,0,235,10,"bar chart of constant height for factors in time series I am a beginner to try R for making graphs. Please help me. I have data of multiple columns (time series). Each column holds factors (please see the one column example data below). I would like to make a constant height (say 1 unit) bar chart of the time series and would like to represent “A” and “B” in different colors with the DATE on the x axis. Any tip?
Thanking you in advance!
DATE                      GROUP
2011.06.18 00:00:00         R
2011.06.18 06:00:00         L
2011.06.18 12:00:00         R
2011.06.18 18:00:00         R
2011.06.19 00:00:00         L
2011.06.19 06:00:00         L
2011.06.19 12:00:00         R
2011.06.19 18:00:00         L
2011.06.20 00:00:00         L
2011.06.20 06:00:00         L
2011.06.20 12:00:00         R
2011.06.20 18:00:00         L
2011.06.21 00:00:00         R
2011.06.21 06:00:00         L
",1
4034840,10/27/2010 15:19:42,489021,10/27/2010 15:19:42,1,0,MLE for Naive Bayes in R,"i am using naivebayes function of e1071 library of R like below:

    model <- naiveBayes(Species ~ ., data = iris)
    pred <- predict(model, iris[,])

my question is: how can i get maximum likelihood estimate for conditional probability distibution of this model?",r,statistics,machine-learning,,,10/28/2010 20:43:25,off topic,1,46,6,"MLE for Naive Bayes in R i am using naivebayes function of e1071 library of R like below:

    model <- naiveBayes(Species ~ ., data = iris)
    pred <- predict(model, iris[,])

my question is: how can i get maximum likelihood estimate for conditional probability distibution of this model?",3
7036539,08/12/2011 06:48:25,891259,08/12/2011 06:48:25,1,0,Calculate statistics (e.g. average) across cells of identical data-frames,"I am having a list of identically sorted dataframes. More specific these are the imputed dataframes which I get after doing Multiple imputations with the AmeliaII package. Now I want to create a new dataframe that is identical in structure, but contains the mean values of the cells calculated across the dataframes. 

The way I achieve this at the moment is the following:

    ## do the Amelia run ------------------------------------------------------------

    a.out <- amelia(merged, m=5, ts=""Year"", cs =""GEO"",polytime=1)

    ## Calculate the output statistics ----------------------------------------------
    left.side <- a.out$imputations[[1]][,1:2]
    a.out.ncol <- ncol(a.out$imputations[[1]])

    a <- a.out$imputations[[1]][,3:a.out.ncol]
    b <- a.out$imputations[[2]][,3:a.out.ncol]
    c <- a.out$imputations[[3]][,3:a.out.ncol]
    d <- a.out$imputations[[4]][,3:a.out.ncol]
    e <- a.out$imputations[[5]][,3:a.out.ncol]

    # Calculate the Mean of the matrices
    mean.right <- apply(abind(a,b,c,d,e,f,g,h,i,j,along=3),c(1,2),mean) 

    # recombine factors with values
    mean <- cbind(left.side,mean.right) 

 
I suppose that there is a much better way of doing this by using apply, plyr or the like, but as a R Newbie I am really a bit lost here. Do you have any suggestions how to go about this? 

Thanks in advance for your help.

Cheers 
Albrecht",r,data.frame,,,,,open,0,213,9,"Calculate statistics (e.g. average) across cells of identical data-frames I am having a list of identically sorted dataframes. More specific these are the imputed dataframes which I get after doing Multiple imputations with the AmeliaII package. Now I want to create a new dataframe that is identical in structure, but contains the mean values of the cells calculated across the dataframes. 

The way I achieve this at the moment is the following:

    ## do the Amelia run ------------------------------------------------------------

    a.out <- amelia(merged, m=5, ts=""Year"", cs =""GEO"",polytime=1)

    ## Calculate the output statistics ----------------------------------------------
    left.side <- a.out$imputations[[1]][,1:2]
    a.out.ncol <- ncol(a.out$imputations[[1]])

    a <- a.out$imputations[[1]][,3:a.out.ncol]
    b <- a.out$imputations[[2]][,3:a.out.ncol]
    c <- a.out$imputations[[3]][,3:a.out.ncol]
    d <- a.out$imputations[[4]][,3:a.out.ncol]
    e <- a.out$imputations[[5]][,3:a.out.ncol]

    # Calculate the Mean of the matrices
    mean.right <- apply(abind(a,b,c,d,e,f,g,h,i,j,along=3),c(1,2),mean) 

    # recombine factors with values
    mean <- cbind(left.side,mean.right) 

 
I suppose that there is a much better way of doing this by using apply, plyr or the like, but as a R Newbie I am really a bit lost here. Do you have any suggestions how to go about this? 

Thanks in advance for your help.

Cheers 
Albrecht",2
3355107,07/28/2010 16:29:05,404650,07/28/2010 14:45:34,1,0,Possibly inconsistent behavior in qplot() ??,"I'm trying to use qplot() to plot a simple time series as one might do using plot().  The x variable is as.POSIXlt and the y is just some continuous measurement.  Here is the code with some brief comments.  Any help on why these data.frames behave differently would be very much appreciated.  As you can see below, I can work around the problem, but I'm curious as to why is doesn't work as I would expect.  

A few details:  
platform: OS X 10.6.4  
R version: R 2.11.0

Disclaimer:  I realize that I could dig into the source code and figure this out myself.  I've never used SO and thought that it might be a nice topic for this forum. 

Disclaimer (2):  I'm new to ggplot2

    library(ggplot2)
    ws.dat <- read.csv(""~/path/to/filename.csv"",header=F)
    names(ws.dat) <- c(""s"",""t"",""w"")
    ws.dat$new.t <- as.POSIXlt(ws.dat$t)
    ws.dat[1:5,]
      ##       s                   t    w               new.t
      ## 1 29522 2005-07-02 00:00:00 5.00 2005-07-02 00:00:00
      ## 2 29522 2005-07-02 00:10:00 5.29 2005-07-02 00:10:00
      ## 3 29522 2005-07-02 00:20:00 5.48 2005-07-02 00:20:00
      ## 4 29522 2005-07-02 00:30:00 5.54 2005-07-02 00:30:00
      ## 5 29522 2005-07-02 00:40:00 5.49 2005-07-02 00:40:00


    ## the following works
    plot(as.POSIXlt(ws.dat$t), ws.dat$w)

    ## doesn't work
    qplot(as.POSIXlt(t), w, data = ws.dat)
      ## Error in if (length(range) == 1 || diff(range) == 0) { : 
      ## missing value where TRUE/FALSE needed

    ## doesn't work
    ws.dat$new.t <- as.POSIXlt(ws.dat$t)
    qplot(new.t, w, data = ws.dat)
      ## Same error as above

    ## Note - I could find a more elegant way of doing this; I'm just trying
    ##   to reproduce as fast as possible.
    new.df <- data.frame(ws.dat$new.t, ws.dat$w)
    new.df[1:5,]
      ##          ws.dat.new.t ws.dat.w
      ## 1 2005-07-02 00:00:00     5.00
      ## 2 2005-07-02 00:10:00     5.29
      ## 3 2005-07-02 00:20:00     5.48
      ## 4 2005-07-02 00:30:00     5.54
      ## 5 2005-07-02 00:40:00     5.49

    ## 'works as *I* would expect'; this is != 'works *as* expected' 
    qplot(ws.dat.new.t, ws.dat.w, data = new.df)




",r,statistics,,,,,open,0,514,6,"Possibly inconsistent behavior in qplot() ?? I'm trying to use qplot() to plot a simple time series as one might do using plot().  The x variable is as.POSIXlt and the y is just some continuous measurement.  Here is the code with some brief comments.  Any help on why these data.frames behave differently would be very much appreciated.  As you can see below, I can work around the problem, but I'm curious as to why is doesn't work as I would expect.  

A few details:  
platform: OS X 10.6.4  
R version: R 2.11.0

Disclaimer:  I realize that I could dig into the source code and figure this out myself.  I've never used SO and thought that it might be a nice topic for this forum. 

Disclaimer (2):  I'm new to ggplot2

    library(ggplot2)
    ws.dat <- read.csv(""~/path/to/filename.csv"",header=F)
    names(ws.dat) <- c(""s"",""t"",""w"")
    ws.dat$new.t <- as.POSIXlt(ws.dat$t)
    ws.dat[1:5,]
      ##       s                   t    w               new.t
      ## 1 29522 2005-07-02 00:00:00 5.00 2005-07-02 00:00:00
      ## 2 29522 2005-07-02 00:10:00 5.29 2005-07-02 00:10:00
      ## 3 29522 2005-07-02 00:20:00 5.48 2005-07-02 00:20:00
      ## 4 29522 2005-07-02 00:30:00 5.54 2005-07-02 00:30:00
      ## 5 29522 2005-07-02 00:40:00 5.49 2005-07-02 00:40:00


    ## the following works
    plot(as.POSIXlt(ws.dat$t), ws.dat$w)

    ## doesn't work
    qplot(as.POSIXlt(t), w, data = ws.dat)
      ## Error in if (length(range) == 1 || diff(range) == 0) { : 
      ## missing value where TRUE/FALSE needed

    ## doesn't work
    ws.dat$new.t <- as.POSIXlt(ws.dat$t)
    qplot(new.t, w, data = ws.dat)
      ## Same error as above

    ## Note - I could find a more elegant way of doing this; I'm just trying
    ##   to reproduce as fast as possible.
    new.df <- data.frame(ws.dat$new.t, ws.dat$w)
    new.df[1:5,]
      ##          ws.dat.new.t ws.dat.w
      ## 1 2005-07-02 00:00:00     5.00
      ## 2 2005-07-02 00:10:00     5.29
      ## 3 2005-07-02 00:20:00     5.48
      ## 4 2005-07-02 00:30:00     5.54
      ## 5 2005-07-02 00:40:00     5.49

    ## 'works as *I* would expect'; this is != 'works *as* expected' 
    qplot(ws.dat.new.t, ws.dat.w, data = new.df)




",2
7903972,10/26/2011 14:16:02,134830,07/08/2009 09:59:12,12591,460,Can you specify different geoms for different facets in a ggplot?,"How do you specify different geoms for different facets in a ggplot?

(Asked on behalf of @pacomet, who [wanted to know][1].)


  [1]: http://stackoverflow.com/questions/7901925/r-setting-space-between-graphs-on-a-multiplot/7902621#7902621",r,ggplot2,,,,,open,0,23,11,"Can you specify different geoms for different facets in a ggplot? How do you specify different geoms for different facets in a ggplot?

(Asked on behalf of @pacomet, who [wanted to know][1].)


  [1]: http://stackoverflow.com/questions/7901925/r-setting-space-between-graphs-on-a-multiplot/7902621#7902621",2
7001799,08/09/2011 19:27:26,636656,02/27/2011 17:23:21,3248,105,ggplot2: Curly braces on an axis?,"In [answering a recent visualization][1] question I really needed braces to show a span on an axis, and I can't figure out how to do it in ggplot2.  Here's the plot:

![example plot][2]

Instead of a tick mark, I'd really like the y axis label ""Second letter of two-letter names"" to have a brace extending from 1 to 10 (the vertical span of the red and blue second letters).  But I'm not sure how to make that happen.  The x axis could benefit from similar treatment.

Code is available in the linked CrossValidated question (and unnecessarily complicated for this example, so I won't show it).  Instead, here's a minimal example:

    library(ggplot2)
    x <- c(runif(10),runif(10)+2)
    y <- c(runif(10),runif(10)+2)
    qplot(x=x,y=y) +
      scale_x_continuous("""",breaks=c(.5,2.5),labels=c(""Low types"",""High types"") )

![minimal example][3]

In this case, a brace from (0,1) for low types and from (2,3) for the high types would be ideal instead of tick marks.

How would I accomplish this?  

I'd rather not use `geom_rect` because:

 - The tick marks will remain
 - I'd prefer braces
 - It will be inside the plot instead of outside it

  [1]: http://stats.stackexchange.com/questions/13999/visualizing-2-letter-combinations/14028#14028
  [2]: http://i.stack.imgur.com/QIDj1.png
  [3]: http://i.stack.imgur.com/8H4zO.png",r,ggplot2,,,,,open,0,206,6,"ggplot2: Curly braces on an axis? In [answering a recent visualization][1] question I really needed braces to show a span on an axis, and I can't figure out how to do it in ggplot2.  Here's the plot:

![example plot][2]

Instead of a tick mark, I'd really like the y axis label ""Second letter of two-letter names"" to have a brace extending from 1 to 10 (the vertical span of the red and blue second letters).  But I'm not sure how to make that happen.  The x axis could benefit from similar treatment.

Code is available in the linked CrossValidated question (and unnecessarily complicated for this example, so I won't show it).  Instead, here's a minimal example:

    library(ggplot2)
    x <- c(runif(10),runif(10)+2)
    y <- c(runif(10),runif(10)+2)
    qplot(x=x,y=y) +
      scale_x_continuous("""",breaks=c(.5,2.5),labels=c(""Low types"",""High types"") )

![minimal example][3]

In this case, a brace from (0,1) for low types and from (2,3) for the high types would be ideal instead of tick marks.

How would I accomplish this?  

I'd rather not use `geom_rect` because:

 - The tick marks will remain
 - I'd prefer braces
 - It will be inside the plot instead of outside it

  [1]: http://stats.stackexchange.com/questions/13999/visualizing-2-letter-combinations/14028#14028
  [2]: http://i.stack.imgur.com/QIDj1.png
  [3]: http://i.stack.imgur.com/8H4zO.png",2
10943181,06/08/2012 04:45:08,1116065,12/26/2011 08:33:41,6,0,Where are the vertex names in an iGraph graph,"My general problem is that I loose the vertex names / labels (not sure about the right word here) when generating a graph using iGraph. 

I have an edge list IC_edge_sub of a bipartite network, that looks like the following:

  new_individualID new_companyID
1             <NA>     10024354c
3        10069415i      2020225c
4        10069415i     16020347c
5        10069272i      2020225c
6        10069272i     16020347c
7        10069274i      2020225c

*I then create a graph element:*
IC_projected_graphs <- bipartite.projection(IC_twomode, types = is.bipartite(IC_twomode)$type)

*Then collapse it to identify only connections between companyIDs*
IC_projected_graphs <- bipartite.projection(IC_twomode, types = is.bipartite(IC_twomode)$type)

*And then get the adjacency matrix:*
CC_matrix_IC_based <- get.adjacency(CC_graph_IC_based); CC_matrix_IC_based

In iGraph node numbering starts at zero and thus also the matrix naming starts at zero. However, I would instead now need the ""new_companyID"" as specified in the 2nd column of the edgelist in the eventual CC_matrix_IC_based matrix. 

Can you help me how to use the information form the original edgelist to put in rownames and colnames in the eventual adjacency matrix?

I googled it and searched stack flow, but could not really find a working answer. Thanks a lot for your help",r,igraph,bipartite,,,,open,0,241,9,"Where are the vertex names in an iGraph graph My general problem is that I loose the vertex names / labels (not sure about the right word here) when generating a graph using iGraph. 

I have an edge list IC_edge_sub of a bipartite network, that looks like the following:

  new_individualID new_companyID
1             <NA>     10024354c
3        10069415i      2020225c
4        10069415i     16020347c
5        10069272i      2020225c
6        10069272i     16020347c
7        10069274i      2020225c

*I then create a graph element:*
IC_projected_graphs <- bipartite.projection(IC_twomode, types = is.bipartite(IC_twomode)$type)

*Then collapse it to identify only connections between companyIDs*
IC_projected_graphs <- bipartite.projection(IC_twomode, types = is.bipartite(IC_twomode)$type)

*And then get the adjacency matrix:*
CC_matrix_IC_based <- get.adjacency(CC_graph_IC_based); CC_matrix_IC_based

In iGraph node numbering starts at zero and thus also the matrix naming starts at zero. However, I would instead now need the ""new_companyID"" as specified in the 2nd column of the edgelist in the eventual CC_matrix_IC_based matrix. 

Can you help me how to use the information form the original edgelist to put in rownames and colnames in the eventual adjacency matrix?

I googled it and searched stack flow, but could not really find a working answer. Thanks a lot for your help",3
7478923,09/20/2011 00:43:49,840546,07/12/2011 11:00:59,5,0,Feasibility of using Mapreduce and Hadoop for my problem,"# Background #
I know what Hadoop and Mapreduce conceptually. I am planning on using them for my project (for research purpose not for production). The project involves lots of data analysis and also use of many statistical function. I thought map-reduce using R should be the best option.I am the only person involved in the project.I don't know R too.Also 

# Problem #
I wanted to know.
1)Difficulty or time consuming will it be to set up hadoop ?
2)Will it require extensive learning of R ?
3)Will writing Map Reduce for some of my analysis which I easily implemented in matlab be difficult ?
4)I need your advice and some pointers for starting this and wanted to know if its worth the try.",r,hadoop,mapreduce,data-analysis,,09/20/2011 01:10:38,not constructive,1,119,9,"Feasibility of using Mapreduce and Hadoop for my problem # Background #
I know what Hadoop and Mapreduce conceptually. I am planning on using them for my project (for research purpose not for production). The project involves lots of data analysis and also use of many statistical function. I thought map-reduce using R should be the best option.I am the only person involved in the project.I don't know R too.Also 

# Problem #
I wanted to know.
1)Difficulty or time consuming will it be to set up hadoop ?
2)Will it require extensive learning of R ?
3)Will writing Map Reduce for some of my analysis which I easily implemented in matlab be difficult ?
4)I need your advice and some pointers for starting this and wanted to know if its worth the try.",4
8967446,01/23/2012 05:11:10,1164339,01/23/2012 05:05:41,1,0,Generating sequence in R,"I need to generate the numbers from 2 to 100 excluding the numbers (11,21,31,41 etc).

I know to do 2 to 100 is simply 2:100 but I am uncertain how I would go about excluding certain values.

Thanks for any help.
",r,numbers,sequence,sequences,exclusion,01/23/2012 16:46:15,too localized,1,39,4,"Generating sequence in R I need to generate the numbers from 2 to 100 excluding the numbers (11,21,31,41 etc).

I know to do 2 to 100 is simply 2:100 but I am uncertain how I would go about excluding certain values.

Thanks for any help.
",5
7014068,08/10/2011 16:07:44,164148,08/27/2009 11:33:59,1120,28,Sorting CSV files in R with certain matching Column?,"data.csv

    ID value
    1  7
    2  77
    5  777
    6

data2.csv

    ID value
    1  6
    3  66
    5  666
    6  6666

data3.csv

    ID value
    1  9
    3
    5  999
    6  9999


**Expected result**

    ID V1   V2   V3
    5  777  666  999


how?",r,,,,,08/12/2011 01:41:29,not a real question,1,106,9,"Sorting CSV files in R with certain matching Column? data.csv

    ID value
    1  7
    2  77
    5  777
    6

data2.csv

    ID value
    1  6
    3  66
    5  666
    6  6666

data3.csv

    ID value
    1  9
    3
    5  999
    6  9999


**Expected result**

    ID V1   V2   V3
    5  777  666  999


how?",1
11729897,07/30/2012 21:27:59,1483598,06/26/2012 18:00:17,1,0,"*** caught segfault *** address (nil), cause 'memory not mapped' Traceback:","I've an R script which works with smaller sample files, but if I give a large file it ends up giving this error. The other functions work fine, it is only the plots that are using QRQC that are causing the error.

Can someone help me fix this?

*** caught segfault ***
address (nil), cause 'memory not mapped'

Traceback:
 1: .Call(""summarize_file"", filename, as.integer(max.length), as.integer(qtype),     as.logical(hash), as.numeric(hash.prop), as.logical(kmer),     as.integer(k), as.logical(verbose))
 2: readSeqFile(fn, quality = qual, hash = FALSE, max.length = maxlength)
 3: doplot_qrqc(y[i, 1], encoding, 200, 400)

",r,,,,,07/31/2012 08:03:32,not a real question,1,90,11,"*** caught segfault *** address (nil), cause 'memory not mapped' Traceback: I've an R script which works with smaller sample files, but if I give a large file it ends up giving this error. The other functions work fine, it is only the plots that are using QRQC that are causing the error.

Can someone help me fix this?

*** caught segfault ***
address (nil), cause 'memory not mapped'

Traceback:
 1: .Call(""summarize_file"", filename, as.integer(max.length), as.integer(qtype),     as.logical(hash), as.numeric(hash.prop), as.logical(kmer),     as.integer(k), as.logical(verbose))
 2: readSeqFile(fn, quality = qual, hash = FALSE, max.length = maxlength)
 3: doplot_qrqc(y[i, 1], encoding, 200, 400)

",1
9885067,03/27/2012 07:28:06,1212164,02/15/2012 18:53:20,8,0,Extract sub- and superdiagonal of a matrix in R,"As the title implies, how does one extract the sub- and superdiagonal of a matrix?",r,matrix,extract,diagonal,,,open,0,15,9,"Extract sub- and superdiagonal of a matrix in R As the title implies, how does one extract the sub- and superdiagonal of a matrix?",4
3988076,10/21/2010 13:44:05,155406,08/12/2009 20:54:53,396,9,Export igraph Issues in R,"I have generated a few network graphs in igraph.  I like to use tkplot, however, after resizing the window manually and changing the layout, my machine freezes up when I try to export or even take a screenshot of the graph.

Any ideas?  My machine is Windows XP Pro and has 2GB memory.

Many thanks in advance.",r,,,,,,open,0,57,5,"Export igraph Issues in R I have generated a few network graphs in igraph.  I like to use tkplot, however, after resizing the window manually and changing the layout, my machine freezes up when I try to export or even take a screenshot of the graph.

Any ideas?  My machine is Windows XP Pro and has 2GB memory.

Many thanks in advance.",1
4357031,12/05/2010 02:28:02,445639,09/12/2010 15:50:09,1,1,qqnorm and qqline in ggplot2,"Say have a linear model LM that I want a qq plot of the residuals. Normally I would use the R base graphics:

    qqnorm(residuals(LM), ylab=""Residuals"")
    qqline(residuals(LM))

I can figure out how to get the qqnorm part of the plot, but I can't seem to manage the qqline:

    ggplot(LM, aes(sample=.resid)) +
        stat_qq()

I suspect I'm missing something pretty basic, but it seems like there ought to be an easy way of doing this.",r,,,,,,open,0,86,5,"qqnorm and qqline in ggplot2 Say have a linear model LM that I want a qq plot of the residuals. Normally I would use the R base graphics:

    qqnorm(residuals(LM), ylab=""Residuals"")
    qqline(residuals(LM))

I can figure out how to get the qqnorm part of the plot, but I can't seem to manage the qqline:

    ggplot(LM, aes(sample=.resid)) +
        stat_qq()

I suspect I'm missing something pretty basic, but it seems like there ought to be an easy way of doing this.",1
5329964,03/16/2011 18:26:45,663060,03/16/2011 18:24:49,1,0,I need a good Limma tutorial that uses R,I'm trying to get started using some statistical analysis with the limma package that runs out of R. Any one know a good tutorial?,r,,,,,,open,0,24,9,I need a good Limma tutorial that uses R I'm trying to get started using some statistical analysis with the limma package that runs out of R. Any one know a good tutorial?,1
1661479,11/02/2009 14:01:35,143476,07/23/2009 07:11:25,589,17,matplotlib for R user?,"I regularly make figures (the exploratory data analysis type) in R. I also program in Python and was wondering if there are features or concepts in matplotlib that would be worth learning. For instance, I am quite happy with R - but its image() function will produce large files with pixelated output, whereas Matlab's equivalent figure (I also program regularly in Matlab) seems to be manageable in file size and also 'smoothed' - does matplotlib also provide such reductions...? But more generally, I wonder what other advantages matplotlib might confer. I don't mean this to be a trolling question. Thanks.",r,python,scipy,matplotlib,data-visualization,,open,0,100,4,"matplotlib for R user? I regularly make figures (the exploratory data analysis type) in R. I also program in Python and was wondering if there are features or concepts in matplotlib that would be worth learning. For instance, I am quite happy with R - but its image() function will produce large files with pixelated output, whereas Matlab's equivalent figure (I also program regularly in Matlab) seems to be manageable in file size and also 'smoothed' - does matplotlib also provide such reductions...? But more generally, I wonder what other advantages matplotlib might confer. I don't mean this to be a trolling question. Thanks.",5
5902183,05/05/2011 18:02:10,623925,02/18/2011 23:21:40,31,1,Odd behavior with median() ?,"I'm noticing some inconsistent behavior when applying the `median()` function to dataframes.  ""Inconsistent behavior"" usually means that I don't understand something, so I hope someone will be willing to clear this up for me.  

I realize that some functions (e.g., `min()`, `max()`) convert the dataframe into a vector and return the corresponding value for the entire df while `mean()` and `sd()` return a value for each column.  While a bit confusing, those differences in behavior don't cause many problems since most code would break if a numeric is returned instead of a vector.  However, `median()` seems to be inconsistent.  For example:

    dat <- data.frame(x=1:100, y=2:101)
    median(dat)

Returns a vector:`[1] 50.5 51.5`

But, sometimes it breaks:

    dat2 <- data.frame(x=1:100, y=rnorm(100))
    median(dat)

Returns: `[1] NA NA
Warning messages:
1: In mean.default(X[[1L]], ...) :
  argument is not numeric or logical: returning NA
2: In mean.default(X[[2L]], ...) :
  argument is not numeric or logical: returning NA`

However, `median(dat2$x)` and `median(dat2$y)` both yield the correct result.  

Also consider the following:
   
    dat3 <- data.frame(x=1:100, y=1:100)
    dat4 <- data.frame(x=1:100, y=100:199)
 
In the above, `median(dat3)` returns `[1] 50.5   NA`  while `median(dat4)` returns `[1]  50.5 149.5`!  I would expect both or neither of these to work.  So, I clearly am not understanding just how the `median()` function is working.
    
Further, functions like `sd`, `mean()`, `min()` and `max()` all yield their expected (if seemingly inconsistent) results in all of the above cases.

I know that I can use something like `sapply(dat2, median)` to get the necessary result, but am wondering why the R gods chose to implement these core stats functions in a way that, at least on the surface, seems inconsistent.  I suspect that I, and probably other neophytes, are probably not understanding some fundamental concept, and I'd appreciate your insight.


    ",r,,,,,,open,0,327,5,"Odd behavior with median() ? I'm noticing some inconsistent behavior when applying the `median()` function to dataframes.  ""Inconsistent behavior"" usually means that I don't understand something, so I hope someone will be willing to clear this up for me.  

I realize that some functions (e.g., `min()`, `max()`) convert the dataframe into a vector and return the corresponding value for the entire df while `mean()` and `sd()` return a value for each column.  While a bit confusing, those differences in behavior don't cause many problems since most code would break if a numeric is returned instead of a vector.  However, `median()` seems to be inconsistent.  For example:

    dat <- data.frame(x=1:100, y=2:101)
    median(dat)

Returns a vector:`[1] 50.5 51.5`

But, sometimes it breaks:

    dat2 <- data.frame(x=1:100, y=rnorm(100))
    median(dat)

Returns: `[1] NA NA
Warning messages:
1: In mean.default(X[[1L]], ...) :
  argument is not numeric or logical: returning NA
2: In mean.default(X[[2L]], ...) :
  argument is not numeric or logical: returning NA`

However, `median(dat2$x)` and `median(dat2$y)` both yield the correct result.  

Also consider the following:
   
    dat3 <- data.frame(x=1:100, y=1:100)
    dat4 <- data.frame(x=1:100, y=100:199)
 
In the above, `median(dat3)` returns `[1] 50.5   NA`  while `median(dat4)` returns `[1]  50.5 149.5`!  I would expect both or neither of these to work.  So, I clearly am not understanding just how the `median()` function is working.
    
Further, functions like `sd`, `mean()`, `min()` and `max()` all yield their expected (if seemingly inconsistent) results in all of the above cases.

I know that I can use something like `sapply(dat2, median)` to get the necessary result, but am wondering why the R gods chose to implement these core stats functions in a way that, at least on the surface, seems inconsistent.  I suspect that I, and probably other neophytes, are probably not understanding some fundamental concept, and I'd appreciate your insight.


    ",1
10939484,06/07/2012 20:34:50,1443183,06/07/2012 20:37:26,1,0,betareg package for R 2.15.0,"I had been using betareg previously on an older version of R (2.13 I think) prior to upgrading to version 2.15.0 (I'm running Ubuntu so I simply added the UCLA mirror to my sources.list.) Now things don't work. Here's a sample dialogue with the prompt:

    > require(betareg)
    Loading required package: betareg
    Warning message:
    In library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE,  :
    there is no package called ‘betareg’
    > install.packages(""betareg"")
    Installing package(s) into ‘/home/tim/R/i686-pc-linux-gnu-library/2.15’
    (as ‘lib’ is unspecified)
    Warning: unable to access index for repository http://cran.sixsigmaonline.org/src/contrib
    Warning message: 
    package ‘betareg’ is not available (for R version 2.15.0) 

I'm not really sure what to do. Ideally I would be able to use this package in 2.15 (as I'll soon be working on a remote server using it - I am not admin.)  However, I would appreciate hearing of any fix that comes to mind. Thanks.

edit: Sorry to the CrossValidated folks for mixing things up.",r,,,,,,open,0,192,5,"betareg package for R 2.15.0 I had been using betareg previously on an older version of R (2.13 I think) prior to upgrading to version 2.15.0 (I'm running Ubuntu so I simply added the UCLA mirror to my sources.list.) Now things don't work. Here's a sample dialogue with the prompt:

    > require(betareg)
    Loading required package: betareg
    Warning message:
    In library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE,  :
    there is no package called ‘betareg’
    > install.packages(""betareg"")
    Installing package(s) into ‘/home/tim/R/i686-pc-linux-gnu-library/2.15’
    (as ‘lib’ is unspecified)
    Warning: unable to access index for repository http://cran.sixsigmaonline.org/src/contrib
    Warning message: 
    package ‘betareg’ is not available (for R version 2.15.0) 

I'm not really sure what to do. Ideally I would be able to use this package in 2.15 (as I'll soon be working on a remote server using it - I am not admin.)  However, I would appreciate hearing of any fix that comes to mind. Thanks.

edit: Sorry to the CrossValidated folks for mixing things up.",1
11732452,07/31/2012 02:53:40,456227,09/23/2010 13:51:34,43,1,Using index number in Regression,"I have a large data frame consisting of a bunch of percentages sorted from 0 to 1.

I want to run a regression that will predict the value based on the index percentile (index/total rows).

Something like this:

    line <- lm(q$value ~ log(q$index))

How can I go about this?",r,,,,,,open,0,49,5,"Using index number in Regression I have a large data frame consisting of a bunch of percentages sorted from 0 to 1.

I want to run a regression that will predict the value based on the index percentile (index/total rows).

Something like this:

    line <- lm(q$value ~ log(q$index))

How can I go about this?",1
2851327,05/17/2010 17:38:24,37751,11/14/2008 18:18:35,2126,55,R: Converting a list of data frames into one data frame,"I have code that at one place ends up with a list of data frames which I really want to convert to a single big data frame. 

I got some pointers from an [earlier question][1] which was trying to do something similar but more complex. 

Here's an example of what I am starting with (this is grossly simplified for illustration):

    listOfDataFrames <- NULL
    
    for (i in 1:100) {
        listOfDataFrames[[i]] <- data.frame(a=sample(letters, 500, rep=T),
                                 b=rnorm(500), c=rnorm(500))
    }

Thanks to Hadley's answer in the aforementioned question I am currently using this to turn my list of data frames into a single data frame:

      list <- unlist(listOfDataFrames, recursive = FALSE)
      df <- do.call(""rbind"", list)

This works but I am wondering about performance. Is there a faster way to do this?



  [1]: http://stackoverflow.com/questions/1652522/rbind-dataframes-in-a-list-of-lists

",r,data.frame,list,,,,open,0,190,11,"R: Converting a list of data frames into one data frame I have code that at one place ends up with a list of data frames which I really want to convert to a single big data frame. 

I got some pointers from an [earlier question][1] which was trying to do something similar but more complex. 

Here's an example of what I am starting with (this is grossly simplified for illustration):

    listOfDataFrames <- NULL
    
    for (i in 1:100) {
        listOfDataFrames[[i]] <- data.frame(a=sample(letters, 500, rep=T),
                                 b=rnorm(500), c=rnorm(500))
    }

Thanks to Hadley's answer in the aforementioned question I am currently using this to turn my list of data frames into a single data frame:

      list <- unlist(listOfDataFrames, recursive = FALSE)
      df <- do.call(""rbind"", list)

This works but I am wondering about performance. Is there a faster way to do this?



  [1]: http://stackoverflow.com/questions/1652522/rbind-dataframes-in-a-list-of-lists

",3
7070574,08/15/2011 20:39:07,894274,08/14/2011 21:45:13,1,0,Sweave not finding dataframes,"I am just beginning to use Sweave. I am using Rstudio and trying to get a latex document with are output. I keep getting an error message suggesting that there is a problem finding the dataframe on which the analysis is to be run.

I am also having problems getting figures (ggplot2) to appear in the pdfs.

I have searched for posts on this problem but found nothing that helps.

Any guidance is appreciated.",r,sweave,,,,09/06/2011 13:52:26,not a real question,1,71,4,"Sweave not finding dataframes I am just beginning to use Sweave. I am using Rstudio and trying to get a latex document with are output. I keep getting an error message suggesting that there is a problem finding the dataframe on which the analysis is to be run.

I am also having problems getting figures (ggplot2) to appear in the pdfs.

I have searched for posts on this problem but found nothing that helps.

Any guidance is appreciated.",2
5025237,02/17/2011 05:03:20,602599,02/04/2011 03:58:29,33,0,Convert ASCII to decimal and hexadecimal in R,"I have a ASCII score in the following format:

feffefdfbefdfffcfdeTddaYddffbfcI``S\_KKX_]]MR[D_TY[VTVXQ]`Q_BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB

How to convert this to hexadecimal and/or decimal in R?

Thank you
San
",r,ascii,,,,02/19/2011 05:02:19,not a real question,1,19,8,"Convert ASCII to decimal and hexadecimal in R I have a ASCII score in the following format:

feffefdfbefdfffcfdeTddaYddffbfcI``S\_KKX_]]MR[D_TY[VTVXQ]`Q_BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB

How to convert this to hexadecimal and/or decimal in R?

Thank you
San
",2
2615907,04/11/2010 03:05:37,742,08/08/2008 13:33:41,888,11,Operate on pairs of rows of a data frame,"I've got a data frame in R, and I'd like to perform a calculation on all pairs of rows. Is there a simpler way to do this than using a nested for loop?

To make this concrete, consider a data frame with ten rows, and I want to calculate the difference of scores between all (45) possible pairs. 

    > data.frame(ID=1:10,Score=4*10:1)
       ID Score
    1   1    40
    2   2    36
    3   3    32
    4   4    28
    5   5    24
    6   6    20
    7   7    16
    8   8    12
    9   9     8
    10 10     4


I know I could do this calculation with a nested for loop, but is there a better (more R-ish) way to do it? ",r,,,,,,open,0,204,9,"Operate on pairs of rows of a data frame I've got a data frame in R, and I'd like to perform a calculation on all pairs of rows. Is there a simpler way to do this than using a nested for loop?

To make this concrete, consider a data frame with ten rows, and I want to calculate the difference of scores between all (45) possible pairs. 

    > data.frame(ID=1:10,Score=4*10:1)
       ID Score
    1   1    40
    2   2    36
    3   3    32
    4   4    28
    5   5    24
    6   6    20
    7   7    16
    8   8    12
    9   9     8
    10 10     4


I know I could do this calculation with a nested for loop, but is there a better (more R-ish) way to do it? ",1
9572518,03/05/2012 19:15:14,183251,10/02/2009 17:16:39,2867,132,ggplot2/R output pdf too large,"I'm generating a 32x32 heatmap in ggplot2 in my MacBook Pro, this is relatively simple stuff. However, the pdf output for this is huge (something like 7MB) and when I load it in pdflatex, loading and changing pages in the document becomes very slow. What are my options? Is there a better way to save a PDF in R that plays nicely with ggplot2 and pdflatex?

",r,ggplot2,pdflatex,,,,open,0,66,5,"ggplot2/R output pdf too large I'm generating a 32x32 heatmap in ggplot2 in my MacBook Pro, this is relatively simple stuff. However, the pdf output for this is huge (something like 7MB) and when I load it in pdflatex, loading and changing pages in the document becomes very slow. What are my options? Is there a better way to save a PDF in R that plays nicely with ggplot2 and pdflatex?

",3
10826357,05/31/2012 01:25:24,1363746,04/29/2012 04:34:58,28,0,Efficeint way to creat matrix in R,"I am trying to creat a matrix like this from vec:

        vec =c(1, 2, 3)

        > A
              [,1] [,2] [,3] [,4] [,5]
        [1,]    1    1    0    0    0
        [2,]    1    2    1    0    0
        [3,]    1    3    2    1    0

and each time the length and value of vector vec is changing. How can i write a function to create this matrix?
",r,matrix,,,,06/04/2012 14:48:28,not a real question,1,154,7,"Efficeint way to creat matrix in R I am trying to creat a matrix like this from vec:

        vec =c(1, 2, 3)

        > A
              [,1] [,2] [,3] [,4] [,5]
        [1,]    1    1    0    0    0
        [2,]    1    2    1    0    0
        [3,]    1    3    2    1    0

and each time the length and value of vector vec is changing. How can i write a function to create this matrix?
",2
5262332,03/10/2011 16:04:50,653825,03/10/2011 15:40:21,1,0,Parallel processing and temporary files,"I'm using the `mclapply` function in the `multicore` package to do parallel processing. It seems that all child processes started produce the same names for temporary files given by the `tempfile` function. i.e. if I have four processors,

    library(multicore)
    mclapply(1:4, function(x) tempfile())

will give four exactly same filenames. Obviously I need the temporary files to be different so that the child processes don't overwrite each others' files. When using `tempfile` indirectly, i.e. calling some function that calls `tempfile` I have no control over the filename.

Is there a way around this? Do other parallel processing packages for R (e.g. `foreach`) have the same problem?",r,random,parallel-processing,multicore,,,open,0,108,5,"Parallel processing and temporary files I'm using the `mclapply` function in the `multicore` package to do parallel processing. It seems that all child processes started produce the same names for temporary files given by the `tempfile` function. i.e. if I have four processors,

    library(multicore)
    mclapply(1:4, function(x) tempfile())

will give four exactly same filenames. Obviously I need the temporary files to be different so that the child processes don't overwrite each others' files. When using `tempfile` indirectly, i.e. calling some function that calls `tempfile` I have no control over the filename.

Is there a way around this? Do other parallel processing packages for R (e.g. `foreach`) have the same problem?",4
6722140,07/17/2011 05:45:07,707145,04/14/2011 02:08:22,200,1,Simulation of Experimental Design Data in R,"I wonder how to simulate data for experimental designs, like Completely Randomized Design, Randomized Block Design, Latin Square design etc., in _R_. Any help will be highly appreciated. Thanks",r,simulation,,,,07/18/2011 06:23:47,not constructive,1,29,7,"Simulation of Experimental Design Data in R I wonder how to simulate data for experimental designs, like Completely Randomized Design, Randomized Block Design, Latin Square design etc., in _R_. Any help will be highly appreciated. Thanks",2
4789836,01/25/2011 03:58:34,438602,09/03/2010 04:17:17,45,0,how to resize heatmap without rescaling?,"I am using R version 2.12.1 on Win XP.

I have done a heatmap with dendrogram using the heatmap.2 function.

The heatmap basically looks like I want it to be, but the labels of the columns are cut off.

I.e. the textual labels of the columns, although they are not very long (less than 12 characters), do not fit into the window and can not be read entirely.

If I manually resize the graphics window, the entire hetmap is rescaled, so this doesn't help at all.

How can I change the output size so that I can read the labels infull? (final goal is PNG and PDF).

Thanks.",r,heatmap,,,,,open,0,102,6,"how to resize heatmap without rescaling? I am using R version 2.12.1 on Win XP.

I have done a heatmap with dendrogram using the heatmap.2 function.

The heatmap basically looks like I want it to be, but the labels of the columns are cut off.

I.e. the textual labels of the columns, although they are not very long (less than 12 characters), do not fit into the window and can not be read entirely.

If I manually resize the graphics window, the entire hetmap is rescaled, so this doesn't help at all.

How can I change the output size so that I can read the labels infull? (final goal is PNG and PDF).

Thanks.",2
3692563,09/11/2010 20:33:22,296427,03/18/2010 11:01:55,302,12,How to return 5 topmost values from vector in R?,"I have a vector and I'm able to return highest and lowest value, but how to return 5 topmost values? Is there a simple one-line solution for this?",r,vector,topmost,,,,open,0,28,10,"How to return 5 topmost values from vector in R? I have a vector and I'm able to return highest and lowest value, but how to return 5 topmost values? Is there a simple one-line solution for this?",3
7989315,11/03/2011 01:23:16,927589,09/04/2011 13:38:07,245,1,R: matrix  reciprocal element operation,"I need work on a n x n type of matrix ( n > 100 usually), a small example is as follows:

    mymat <- as.matrix(cbind(V1 = c(1, 1, 2, 3),V2 = c(2, 2, 3, 3),V3 = c( 4, 1, 1, 3), V4 = c(5, 1, 1, 2) ) )
    row.names (mymat) <- c(""a"", ""b"", ""c"", ""d"")
    
      V1 V2 V3 V4
    a  1  2  4  5
    b  1  2  1  1
    c  2  3  1  1
    d  3  3  3  2

I need to work on reciprocal elements, for example: the [] include row and column indices 

    [1,2] * ([1,2] + [2,1]) = 
    
     2 *( 2 * 1)

The complete operation will include Yij (Yij + Yji), for elements that are in diagonal of the matrix it would be:

    [2,2] * ([2,2] + [2,2])

Thus complete operation will be:

    [1,1] * ([1,1] + [1,1]) + [1,2] * ([1,2] + [2,1]) + ...................+ [1,2] * ([1,2] + [2,1]) + [4,4] * ([4,4] + [4,4])

Sorry I could not figure out, need your help. 
 



",r,matrix,,,,11/16/2011 08:17:35,not a real question,1,231,6,"R: matrix  reciprocal element operation I need work on a n x n type of matrix ( n > 100 usually), a small example is as follows:

    mymat <- as.matrix(cbind(V1 = c(1, 1, 2, 3),V2 = c(2, 2, 3, 3),V3 = c( 4, 1, 1, 3), V4 = c(5, 1, 1, 2) ) )
    row.names (mymat) <- c(""a"", ""b"", ""c"", ""d"")
    
      V1 V2 V3 V4
    a  1  2  4  5
    b  1  2  1  1
    c  2  3  1  1
    d  3  3  3  2

I need to work on reciprocal elements, for example: the [] include row and column indices 

    [1,2] * ([1,2] + [2,1]) = 
    
     2 *( 2 * 1)

The complete operation will include Yij (Yij + Yji), for elements that are in diagonal of the matrix it would be:

    [2,2] * ([2,2] + [2,2])

Thus complete operation will be:

    [1,1] * ([1,1] + [1,1]) + [1,2] * ([1,2] + [2,1]) + ...................+ [1,2] * ([1,2] + [2,1]) + [4,4] * ([4,4] + [4,4])

Sorry I could not figure out, need your help. 
 



",2
11300385,07/02/2012 20:03:13,1470004,06/20/2012 17:19:14,20,1,How to display my data in a visually pleasing way?,"I have some funky data that I'd like to put into a final presentation form and I was hoping to get some creative ideas on how to do it. My data compares measurements of different lengths, so for example I have 52 week long samples of 6 different chemicals (times several duplicates), 26 2 week long samples, 12 month long samples, 4 quarterly samples, 2 semiannuals and an annual sample. I have compared the accuracy of each duration with one another for a total of 15 comparisons (week vs 2 week, week vs month, etc) to find how much one deviates from another on average by computing the % bias, where negative bias indicates that the longer of the 2 samples under-reports and positive bias indicates that the shorter of the 2 samples under-reported. So I have an average % bias and a 95% confidence interval for each of 6 chemicals for each duration comparison. My confidence intervals are of varying widths because I obviously have many fewer data points for comparisons involving annual samples. How might I pack this into one nice looking graphic (or color coded table, or something) that can convey the average bias for each chem and the accompanying confidence interval (maybe highlighting those with greater power?) and also for each duration comparison (ideally maintaining some visual representation of relative time lengths). I would be content having 6 graphics (one for each chemical), but I don't think I want 15 (one for each comparison). Any creative solutions? Please keep in mind that I'm working in R and I'd say my R skills are about intermediate. Thanks a ton!",r,graph,3d,charts,,07/02/2012 22:08:56,off topic,1,272,10,"How to display my data in a visually pleasing way? I have some funky data that I'd like to put into a final presentation form and I was hoping to get some creative ideas on how to do it. My data compares measurements of different lengths, so for example I have 52 week long samples of 6 different chemicals (times several duplicates), 26 2 week long samples, 12 month long samples, 4 quarterly samples, 2 semiannuals and an annual sample. I have compared the accuracy of each duration with one another for a total of 15 comparisons (week vs 2 week, week vs month, etc) to find how much one deviates from another on average by computing the % bias, where negative bias indicates that the longer of the 2 samples under-reports and positive bias indicates that the shorter of the 2 samples under-reported. So I have an average % bias and a 95% confidence interval for each of 6 chemicals for each duration comparison. My confidence intervals are of varying widths because I obviously have many fewer data points for comparisons involving annual samples. How might I pack this into one nice looking graphic (or color coded table, or something) that can convey the average bias for each chem and the accompanying confidence interval (maybe highlighting those with greater power?) and also for each duration comparison (ideally maintaining some visual representation of relative time lengths). I would be content having 6 graphics (one for each chemical), but I don't think I want 15 (one for each comparison). Any creative solutions? Please keep in mind that I'm working in R and I'd say my R skills are about intermediate. Thanks a ton!",4
10067590,04/09/2012 00:05:57,906592,08/22/2011 20:13:19,519,3,"Fraction in legend, multiple colors","I need to create a complex legend, which includes a fraction and the nominator and denominator are in different colors ![Legend][1]. Is it possible to create such a legend with (base) R?

Since I have to add this legend in several plots different plots, do not want to manually program the text, but be able to add it automatically as legend.

Any ideas?!


  [1]: http://i.stack.imgur.com/Md4DV.png",r,graph,,,,,open,0,64,5,"Fraction in legend, multiple colors I need to create a complex legend, which includes a fraction and the nominator and denominator are in different colors ![Legend][1]. Is it possible to create such a legend with (base) R?

Since I have to add this legend in several plots different plots, do not want to manually program the text, but be able to add it automatically as legend.

Any ideas?!


  [1]: http://i.stack.imgur.com/Md4DV.png",2
10749910,05/25/2012 07:00:23,438498,09/03/2010 00:25:44,326,0,creating a word master document from the existing subdocuments,"I am trying to put together a number of word sub documents into a master document and I heard from someone that he used R to compile his word documents. I was just wondering if it someone out there has done it or uses it alot and how long/simple the scripts are? 

Many thanks,

Baz",r,ms-word,,,,05/25/2012 19:51:27,not a real question,1,54,9,"creating a word master document from the existing subdocuments I am trying to put together a number of word sub documents into a master document and I heard from someone that he used R to compile his word documents. I was just wondering if it someone out there has done it or uses it alot and how long/simple the scripts are? 

Many thanks,

Baz",2
2331237,02/25/2010 02:41:24,148621,07/31/2009 17:36:46,423,19,When will simple parallization not offer a speedup?,"I have a simple program that breaks a dataset (a CSV file) into 4 chunks, reads each chunk in, does some calculations, and then appends the output together.  Think of it as a simple map-reduce operation.  Processing a single chunk uses about 1GB of memory.  I'm running the program on a quad core PC, with 4GB of ram, running Windows XP.  I happen to have coded it up using R, but I don't think it's relevant.

I coded up two versions.  One version processes each chunk in sequence.  The other version processes chunks two at a time in parallel.  Both versions take nearly the same amount of time to finish.

Under what circumstances would you expect to see this performance result?

My current hypothesis is that the processes are bounded by the memory performance, but I don't know the best way to investigate this further.  Any suggestions or guesses?",r,parallel-processing,performance,memory,,,open,0,154,8,"When will simple parallization not offer a speedup? I have a simple program that breaks a dataset (a CSV file) into 4 chunks, reads each chunk in, does some calculations, and then appends the output together.  Think of it as a simple map-reduce operation.  Processing a single chunk uses about 1GB of memory.  I'm running the program on a quad core PC, with 4GB of ram, running Windows XP.  I happen to have coded it up using R, but I don't think it's relevant.

I coded up two versions.  One version processes each chunk in sequence.  The other version processes chunks two at a time in parallel.  Both versions take nearly the same amount of time to finish.

Under what circumstances would you expect to see this performance result?

My current hypothesis is that the processes are bounded by the memory performance, but I don't know the best way to investigate this further.  Any suggestions or guesses?",4
10597857,05/15/2012 09:28:49,67405,02/17/2009 14:43:49,2041,4,Renaming Matrix Name in R,"I have the following matrix:

    > dat

           [,1]      [,2]       [,3]        [,4]
    foo 0.7574657 0.2104075 0.02922241 0.002705617
    foo 0.0000000 0.0000000 0.00000000 0.000000000
    foo 0.0000000 0.0000000 0.00000000 0.000000000
    foo 0.0000000 0.0000000 0.00000000 0.000000000
    foo 0.0000000 0.0000000 0.00000000 0.000000000
    foo 0.0000000 0.0000000 0.00000000 0.000000000

and given this:

    > new_names <- c(""f0"",""f1"",""f2,""f3"",""f4"",""f5"");
    # number of new_names member is guaranteed the same with rows of the matrix


How can I get this?


            [,1]      [,2]       [,3]        [,4]
    f0 0.7574657 0.2104075 0.02922241 0.002705617
    f1 0.0000000 0.0000000 0.00000000 0.000000000
    f2 0.0000000 0.0000000 0.00000000 0.000000000
    f3 0.0000000 0.0000000 0.00000000 0.000000000
    f4 0.0000000 0.0000000 0.00000000 0.000000000
    f5 0.0000000 0.0000000 0.00000000 0.000000000

",r,data,matrix,,,,open,0,201,5,"Renaming Matrix Name in R I have the following matrix:

    > dat

           [,1]      [,2]       [,3]        [,4]
    foo 0.7574657 0.2104075 0.02922241 0.002705617
    foo 0.0000000 0.0000000 0.00000000 0.000000000
    foo 0.0000000 0.0000000 0.00000000 0.000000000
    foo 0.0000000 0.0000000 0.00000000 0.000000000
    foo 0.0000000 0.0000000 0.00000000 0.000000000
    foo 0.0000000 0.0000000 0.00000000 0.000000000

and given this:

    > new_names <- c(""f0"",""f1"",""f2,""f3"",""f4"",""f5"");
    # number of new_names member is guaranteed the same with rows of the matrix


How can I get this?


            [,1]      [,2]       [,3]        [,4]
    f0 0.7574657 0.2104075 0.02922241 0.002705617
    f1 0.0000000 0.0000000 0.00000000 0.000000000
    f2 0.0000000 0.0000000 0.00000000 0.000000000
    f3 0.0000000 0.0000000 0.00000000 0.000000000
    f4 0.0000000 0.0000000 0.00000000 0.000000000
    f5 0.0000000 0.0000000 0.00000000 0.000000000

",3
5826703,04/29/2011 00:49:16,450889,09/17/2010 18:01:59,160,3,[R] How to replicate a Monthly Cycle Chart in R,"I'd like to output a chart similar to the one represented on this page (on the right) using R and any package that would make it look good:

http://processtrends.com/pg_charts_monthly_cycle_chart.htm

Anyone up to the challenge? :)

Thanks!",r,,,,,,open,0,33,10,"[R] How to replicate a Monthly Cycle Chart in R I'd like to output a chart similar to the one represented on this page (on the right) using R and any package that would make it look good:

http://processtrends.com/pg_charts_monthly_cycle_chart.htm

Anyone up to the challenge? :)

Thanks!",1
10102350,04/11/2012 08:23:07,1326028,04/11/2012 08:17:49,1,0,Memory problems with large-scale social network visualization using R and Cytoscape,"I'm relatively new to R and am trying to solve the following problem:

I work on a Windows 7 Enterprise platform with the 32bit version of R
and have about 3GB of RAM on my machine. I have large-scale social
network data (c. 7,000 vertices and c. 30,000 edges) which are
currently stored in my SQL database. I have managed to pull this data
(omitting vertex and edge attributes) into an R dataframe and then
into an igraph object. For further analysis and visualization, I would
now like to push this igraph into Cytoscape using RCytoscape.
Currently, my approach is to convert the igraph object into an
graphNEL object since RCytoscape seems to work well with this object
type. (The igraph plotting functions are much too slow and lack
further analysis functionality.)

Unfortunately, I always run into memory issues when running this
script. It has worked previously with smaller networks though.

Does anyone have an idea on how to solve this issue? Or can you
recommend any other visualization and analysis tools that work well
with R and can handle such large-scale data?

Any help would be much appreciated. Thanks so much in advance!

Best, Ignacio ",r,social-networking,social,large-scale,cytoscape,,open,0,179,11,"Memory problems with large-scale social network visualization using R and Cytoscape I'm relatively new to R and am trying to solve the following problem:

I work on a Windows 7 Enterprise platform with the 32bit version of R
and have about 3GB of RAM on my machine. I have large-scale social
network data (c. 7,000 vertices and c. 30,000 edges) which are
currently stored in my SQL database. I have managed to pull this data
(omitting vertex and edge attributes) into an R dataframe and then
into an igraph object. For further analysis and visualization, I would
now like to push this igraph into Cytoscape using RCytoscape.
Currently, my approach is to convert the igraph object into an
graphNEL object since RCytoscape seems to work well with this object
type. (The igraph plotting functions are much too slow and lack
further analysis functionality.)

Unfortunately, I always run into memory issues when running this
script. It has worked previously with smaller networks though.

Does anyone have an idea on how to solve this issue? Or can you
recommend any other visualization and analysis tools that work well
with R and can handle such large-scale data?

Any help would be much appreciated. Thanks so much in advance!

Best, Ignacio ",5
6557977,07/02/2011 15:48:54,826178,07/02/2011 15:48:54,1,0,add mean value to histogram in R!,"I would like to plot a histogram with mean (average) value on it (mark it for example by blue, bold line).
I try to do it using plot command, but even if I add parameter: ""add=TRUE""
it desn't work.",r,statistics,histogram,,,,open,0,37,7,"add mean value to histogram in R! I would like to plot a histogram with mean (average) value on it (mark it for example by blue, bold line).
I try to do it using plot command, but even if I add parameter: ""add=TRUE""
it desn't work.",3
8435053,12/08/2011 17:27:18,345660,05/20/2010 02:17:37,1556,24,"Error with the train function in caret, when using glmnet and 2 classes","The following block of code fails, for no reason I can discern.

    library(caret)
    data(iris)
    TrainData <- iris[,1:4]
    TrainClasses <- factor(ifelse(iris[,5]=='versicolor','versicolor','other'))
    model1 <- train(TrainData,TrainClasses,method='glmnet')

With the following error:

    Error in { : task 1 failed - ""'n' must be a positive integer >= 'x'""

If I sub in a different model, such as `glm` it runs fine.  If I uses 3 classes, `TrainClasses <- iris[,5]`, it also works fine.

What about 2 classes is uniquely causing the glmnet method to fail?

This is R version 2.14.0, caret version 5.09-006, on windows.  The same error happens on my mac and on linux.",r,caret,glmnet,,,,open,0,116,13,"Error with the train function in caret, when using glmnet and 2 classes The following block of code fails, for no reason I can discern.

    library(caret)
    data(iris)
    TrainData <- iris[,1:4]
    TrainClasses <- factor(ifelse(iris[,5]=='versicolor','versicolor','other'))
    model1 <- train(TrainData,TrainClasses,method='glmnet')

With the following error:

    Error in { : task 1 failed - ""'n' must be a positive integer >= 'x'""

If I sub in a different model, such as `glm` it runs fine.  If I uses 3 classes, `TrainClasses <- iris[,5]`, it also works fine.

What about 2 classes is uniquely causing the glmnet method to fail?

This is R version 2.14.0, caret version 5.09-006, on windows.  The same error happens on my mac and on linux.",3
8830472,01/12/2012 05:19:10,1144724,01/12/2012 05:10:17,1,0,create an OHLC series from ticker data using R,"This seems like it should be a common thing, but all my searching comes up with half or unfinished answers.

I have a set of data in a csv.  But the data is set up so it is time, price, volume.  To properly analyze my data I need it in OHLCV format: open, high, low, close, volume.

Does anyone have an idea how to reformat into OHLCV?

A sample of a data set is at the link:
http://www.mediafire.com/?fyunce685ekuyo3",r,finance,,,,01/12/2012 11:10:51,too localized,1,76,9,"create an OHLC series from ticker data using R This seems like it should be a common thing, but all my searching comes up with half or unfinished answers.

I have a set of data in a csv.  But the data is set up so it is time, price, volume.  To properly analyze my data I need it in OHLCV format: open, high, low, close, volume.

Does anyone have an idea how to reformat into OHLCV?

A sample of a data set is at the link:
http://www.mediafire.com/?fyunce685ekuyo3",2
6301412,06/10/2011 02:36:15,471448,10/10/2010 09:38:12,98,0,R how can I read text document and quote them in the script?,I have a text document like qq.txt. Inside this qq.tex is the text I want to quote in R. But I don't which function of R can read this text document and meanwhile quote all the text in this documents,r,text,,,,06/10/2011 09:26:54,not a real question,1,40,13,R how can I read text document and quote them in the script? I have a text document like qq.txt. Inside this qq.tex is the text I want to quote in R. But I don't which function of R can read this text document and meanwhile quote all the text in this documents,2
9981929,04/02/2012 18:38:56,963031,09/24/2011 21:17:22,246,1,How to display all x labels in R barplot?,This is a basic question but i am unable to find an answer. I am generating about 9 barplots within one panel and each barplot has about 12 bars. I am providing all the 12 labels in my input but R is naming only alternate bars. This is obviously due to to some default setting in R which needs to be changed but i am unable to find it.,r,graph,,,,,open,0,69,9,How to display all x labels in R barplot? This is a basic question but i am unable to find an answer. I am generating about 9 barplots within one panel and each barplot has about 12 bars. I am providing all the 12 labels in my input but R is naming only alternate bars. This is obviously due to to some default setting in R which needs to be changed but i am unable to find it.,2
10166246,04/15/2012 21:26:09,784876,06/05/2011 14:40:28,11,0,Urgent R help! Please,"I'm working on a project in which I have three factors and I'm measuring the length it takes for a candle to burn. 
This is my data: 
     Size    Brand   Scent	  time
       1	    1	   1        255
       1	    1        2        225
       1	    2	   1        283
       1	    2        2        338
       1	    3	   1        192
       1	    3        2        229
       2	    1	   1        1278
       2	    1        2        1496
       2	    2	   1        3897
       2	    2        2        2781
       2	    3	   1        1038
       2	    3        2        1439

This is what I'm doing in R for analysis bur for some reason it will not give me the F statistics and p-values. Can someone help me please? I have no idea why it's doing this. 
> attach(data)
> fsize <- factor(Size)
> fbrand <- factor(Brand)
> fscent <- factor(Scent)
> model1 <- aov(time~fsize*fbrand*fscent)
> summary(model1)
                    Df Sum Sq Mean Sq
fsize                1 2507.1  2507.1
fbrand               2  829.8   414.9
fscent               1    4.4     4.4
fsize:fbrand         2  700.0   350.0
fsize:fscent         1    7.3     7.3
fbrand:fscent        2   89.5    44.8
fsize:fbrand:fscent  2  101.4    50.7
",r,,,,,04/18/2012 04:19:37,off topic,1,528,4,"Urgent R help! Please I'm working on a project in which I have three factors and I'm measuring the length it takes for a candle to burn. 
This is my data: 
     Size    Brand   Scent	  time
       1	    1	   1        255
       1	    1        2        225
       1	    2	   1        283
       1	    2        2        338
       1	    3	   1        192
       1	    3        2        229
       2	    1	   1        1278
       2	    1        2        1496
       2	    2	   1        3897
       2	    2        2        2781
       2	    3	   1        1038
       2	    3        2        1439

This is what I'm doing in R for analysis bur for some reason it will not give me the F statistics and p-values. Can someone help me please? I have no idea why it's doing this. 
> attach(data)
> fsize <- factor(Size)
> fbrand <- factor(Brand)
> fscent <- factor(Scent)
> model1 <- aov(time~fsize*fbrand*fscent)
> summary(model1)
                    Df Sum Sq Mean Sq
fsize                1 2507.1  2507.1
fbrand               2  829.8   414.9
fscent               1    4.4     4.4
fsize:fbrand         2  700.0   350.0
fsize:fscent         1    7.3     7.3
fbrand:fscent        2   89.5    44.8
fsize:fbrand:fscent  2  101.4    50.7
",1
7935067,10/28/2011 21:23:25,1019128,10/28/2011 21:12:15,1,0,Trouble plotting with dates in R,"I am relatively new to R and am having trouble plotting grouped data against date. I have count data grouped by month over 4 years. I don't want May of 2008 grouped with May 2009 but rather points for each month of each year with standard errors. Here is my code so far but I get a blank graph with no points. I can get rid of the axis.POSIXct line and I get a graph with points and error bars. The problem seems to be around the scaling or data format of the plot vs. the axis. Can anyone help me here?

 
    > r <- as.POSIXct(range(refmCount$mo.yr), ""month"")
    > 
    > ############# can get plot and points to line up on the x-axis##########################
    > plot(refmCount$mo.yr, refmCount$count, type = ""n"", xaxt = ""n"",
    +      xlab = ""Date"",
    +      ylab = ""Mean number of salamanders per night"",
    +      xlim = c(r[1], r[2]))
    > axis.POSIXct(1, at = seq(r[1], r[2], by = ""month""), format = ""%b"")
    > points(refmCount$mo.yr, refmCount$count, type = ""p"", pch = 19)
    points(depmCount$mo.yr, depmCount$count, type = ""p"", pch = 24)
    > arrows(refmCount$mo.yr, refmCount$count+mCount$se, refmCount$mo.yr, refmCount$count-  refmCount$se, angle=90, code=3, length=0)
     > 
    > str(refmCount)
    'data.frame':	19 obs. of  7 variables:
    $ mo.yr:Class 'Date'  num [1:19] 14000 14031 14061 14092 14123 ...
    $ trt  : Factor w/ 2 levels ""Depletion"",""Reference"": 2 2 2 2 2 2 2 2 2 2 ...
    $ N    : num  75 110 15 10 34 20 20 10 40 15 ...
    $ count: num  3.6 5.95 3.47 6.7 11.12 ...
    $ sd   : num  8.58 8.4 4.42 3.47 11.88 ...
    $ se   : num  0.99 0.801 1.142 1.096 2.037 ...
    $ ci   : num  1.97 1.59 2.45 2.48 4.14 ...
    > r
    [1] ""2008-04-30 20:00:00 EDT"" ""2011-05-31 20:00:00 EDT""
    >",r,graph,plot,date,posixct,,open,0,397,6,"Trouble plotting with dates in R I am relatively new to R and am having trouble plotting grouped data against date. I have count data grouped by month over 4 years. I don't want May of 2008 grouped with May 2009 but rather points for each month of each year with standard errors. Here is my code so far but I get a blank graph with no points. I can get rid of the axis.POSIXct line and I get a graph with points and error bars. The problem seems to be around the scaling or data format of the plot vs. the axis. Can anyone help me here?

 
    > r <- as.POSIXct(range(refmCount$mo.yr), ""month"")
    > 
    > ############# can get plot and points to line up on the x-axis##########################
    > plot(refmCount$mo.yr, refmCount$count, type = ""n"", xaxt = ""n"",
    +      xlab = ""Date"",
    +      ylab = ""Mean number of salamanders per night"",
    +      xlim = c(r[1], r[2]))
    > axis.POSIXct(1, at = seq(r[1], r[2], by = ""month""), format = ""%b"")
    > points(refmCount$mo.yr, refmCount$count, type = ""p"", pch = 19)
    points(depmCount$mo.yr, depmCount$count, type = ""p"", pch = 24)
    > arrows(refmCount$mo.yr, refmCount$count+mCount$se, refmCount$mo.yr, refmCount$count-  refmCount$se, angle=90, code=3, length=0)
     > 
    > str(refmCount)
    'data.frame':	19 obs. of  7 variables:
    $ mo.yr:Class 'Date'  num [1:19] 14000 14031 14061 14092 14123 ...
    $ trt  : Factor w/ 2 levels ""Depletion"",""Reference"": 2 2 2 2 2 2 2 2 2 2 ...
    $ N    : num  75 110 15 10 34 20 20 10 40 15 ...
    $ count: num  3.6 5.95 3.47 6.7 11.12 ...
    $ sd   : num  8.58 8.4 4.42 3.47 11.88 ...
    $ se   : num  0.99 0.801 1.142 1.096 2.037 ...
    $ ci   : num  1.97 1.59 2.45 2.48 4.14 ...
    > r
    [1] ""2008-04-30 20:00:00 EDT"" ""2011-05-31 20:00:00 EDT""
    >",5
10434108,05/03/2012 15:04:07,927589,09/04/2011 13:38:07,1458,6,keep selected lines in x axis and label at particular segment in Y using ggplot2 in R,"Here is my plot


    dat <- data.frame(
      pos = c(1, 3, 5, 8, 10, 12),
      start = c(1,3, 6, 7, 10, 11),
      end = c(5, 6, 9, 9, 13, 12)
    )
    
    library(ggplot2)
    p <- ggplot(dat) + geom_segment(aes(x=start, y=pos, xend=end, yend=pos), color=""blue"", size=2) + ylab(""Fragments)"") +  xlab(""Position"")
        scale_y_reverse() + theme_bw()
    
    p1 <- p + opts(legend.position=""left"",
            panel.background=theme_blank(),panel.border=theme_blank(),panel.grid.major=theme_blank(),
            panel.grid.minor=theme_blank(),plot.background=theme_blank())
            
            
    p1

![enter image description here][1]

Bitmapped desired version is, with axis line and labels near the segments. [ just exta-note: Note that bitmap changed the line to round ended (it would be interesting to see if we can do in the ggplot2)] 

![enter image description here][2]


  [1]: http://i.stack.imgur.com/hd6Hm.jpg
  [2]: http://i.stack.imgur.com/PeOU1.jpg",r,formatting,ggplot2,axis,,,open,0,201,17,"keep selected lines in x axis and label at particular segment in Y using ggplot2 in R Here is my plot


    dat <- data.frame(
      pos = c(1, 3, 5, 8, 10, 12),
      start = c(1,3, 6, 7, 10, 11),
      end = c(5, 6, 9, 9, 13, 12)
    )
    
    library(ggplot2)
    p <- ggplot(dat) + geom_segment(aes(x=start, y=pos, xend=end, yend=pos), color=""blue"", size=2) + ylab(""Fragments)"") +  xlab(""Position"")
        scale_y_reverse() + theme_bw()
    
    p1 <- p + opts(legend.position=""left"",
            panel.background=theme_blank(),panel.border=theme_blank(),panel.grid.major=theme_blank(),
            panel.grid.minor=theme_blank(),plot.background=theme_blank())
            
            
    p1

![enter image description here][1]

Bitmapped desired version is, with axis line and labels near the segments. [ just exta-note: Note that bitmap changed the line to round ended (it would be interesting to see if we can do in the ggplot2)] 

![enter image description here][2]


  [1]: http://i.stack.imgur.com/hd6Hm.jpg
  [2]: http://i.stack.imgur.com/PeOU1.jpg",4
9413457,02/23/2012 12:52:08,428790,08/23/2010 20:05:06,21711,427,Count the number of valid observations (no NA) in any combination of variables,"Say I have a data frame like this:

    Df <- data.frame(
        V1 = c(1,2,3,NA,5),
        V2 = c(1,2,NA,4,5),
        V3 = c(NA,2,NA,4,NA)
    )

Now I want to count the number of valid observations for every combination of two variables. For that, I wrote a function `sharedcount`:

    sharedcount <- function(x,...){
        nx <- names(x)
        alln <- combn(nx,2)
        out <- apply(alln,2,
          function(y)sum(complete.cases(x[y]))
        )
        data.frame(t(alln),out)
    }

This gives the output:

    > sharedcount(Df)
      X1 X2 out
    1 V1 V2   3
    2 V1 V3   1
    3 V2 V3   2

All fine, but the function itself takes pretty long on big dataframes (600 variables and about 10000 observations). I have the feeling I'm overseeing an easier approach, especially since cor(...,use='pairwise') is running still a whole lot faster while it has to do something similar :

    > require(rbenchmark)    
    > benchmark(sharedcount(TestDf),cor(TestDf,use='pairwise'),
    +     columns=c('test','elapsed','relative'),
    +     replications=1
    + )
                               test elapsed relative
    2 cor(TestDf, use = ""pairwise"")    0.25     1.0
    1           sharedcount(TestDf)    1.90     7.6

Any tips are appreciated.",r,data.frame,missing-data,,,,open,0,337,13,"Count the number of valid observations (no NA) in any combination of variables Say I have a data frame like this:

    Df <- data.frame(
        V1 = c(1,2,3,NA,5),
        V2 = c(1,2,NA,4,5),
        V3 = c(NA,2,NA,4,NA)
    )

Now I want to count the number of valid observations for every combination of two variables. For that, I wrote a function `sharedcount`:

    sharedcount <- function(x,...){
        nx <- names(x)
        alln <- combn(nx,2)
        out <- apply(alln,2,
          function(y)sum(complete.cases(x[y]))
        )
        data.frame(t(alln),out)
    }

This gives the output:

    > sharedcount(Df)
      X1 X2 out
    1 V1 V2   3
    2 V1 V3   1
    3 V2 V3   2

All fine, but the function itself takes pretty long on big dataframes (600 variables and about 10000 observations). I have the feeling I'm overseeing an easier approach, especially since cor(...,use='pairwise') is running still a whole lot faster while it has to do something similar :

    > require(rbenchmark)    
    > benchmark(sharedcount(TestDf),cor(TestDf,use='pairwise'),
    +     columns=c('test','elapsed','relative'),
    +     replications=1
    + )
                               test elapsed relative
    2 cor(TestDf, use = ""pairwise"")    0.25     1.0
    1           sharedcount(TestDf)    1.90     7.6

Any tips are appreciated.",3
11233875,06/27/2012 19:50:05,1480928,06/25/2012 19:39:44,1,0,compiling minimal knitr example fails,"I'm working on getting knitr setup. I installed the latest version of R (2.15.1), Lyx 2.0 including the MiKTex 2.9 distribution, and RStudio 0.96.304 on a Windows 7 Enterprise box. I can get pdflatex output if I open up Lyx and simply view the tutorial, so the basic system is working. I then downloaded the [minimal Rnw example](https://github.com/yihui/knitr/blob/master/inst/examples/knitr-minimal.Rnw) saved it in my working directory as testknitr.Snw, opened that file in RStudio, and pressed the compile PDF button. The knitr output completes with a single warning about the parser package, and produces a file testknitr.tex. pdflatex.exe then runs, but fails, and the particular error in the log file seems to be  

     ! LaTeX Error: Environment alltt undefined.  

I received the same error when attempting to compile testknitr.tex using TexWorks. I created that file from inside R using knit(""testknitr.Rnw"") - different extension because of the default in RStudio. I did tell MikTek to update packages automatically when compiling. A bit of research on [CTAN](http://www.ctan.org/pkg/alltt) suggests that the alltt environment is part of the ltxbase package, which is installed when I look at the package manager. In fact it seems like a pretty core part of the whole thing! 

I was able to use knitr to do the minimal html example from within R - so I'm not sure what's missing. I'd like to compile the minimal examples before trying anything serious.  ",r,knitr,,,,,open,0,238,5,"compiling minimal knitr example fails I'm working on getting knitr setup. I installed the latest version of R (2.15.1), Lyx 2.0 including the MiKTex 2.9 distribution, and RStudio 0.96.304 on a Windows 7 Enterprise box. I can get pdflatex output if I open up Lyx and simply view the tutorial, so the basic system is working. I then downloaded the [minimal Rnw example](https://github.com/yihui/knitr/blob/master/inst/examples/knitr-minimal.Rnw) saved it in my working directory as testknitr.Snw, opened that file in RStudio, and pressed the compile PDF button. The knitr output completes with a single warning about the parser package, and produces a file testknitr.tex. pdflatex.exe then runs, but fails, and the particular error in the log file seems to be  

     ! LaTeX Error: Environment alltt undefined.  

I received the same error when attempting to compile testknitr.tex using TexWorks. I created that file from inside R using knit(""testknitr.Rnw"") - different extension because of the default in RStudio. I did tell MikTek to update packages automatically when compiling. A bit of research on [CTAN](http://www.ctan.org/pkg/alltt) suggests that the alltt environment is part of the ltxbase package, which is installed when I look at the package manager. In fact it seems like a pretty core part of the whole thing! 

I was able to use knitr to do the minimal html example from within R - so I'm not sure what's missing. I'd like to compile the minimal examples before trying anything serious.  ",2
11331023,07/04/2012 14:28:03,735764,05/03/2011 08:36:05,477,2,Binning the data & plotting the histogram,"I have a list of values (these are positive as well as negative values). As an example say I have 35000 numbers (+ve and -ve both in it).

What I want to do is to bin them, i.e. the number values between 0-200 (also from -200 to 0), 201-400 (-400 to 201), .... and so on till 48,800-50000 (-50000 to 48,500).

Once I have these values, the plotting of histogram or any other representation is easier. I can take this to excel or plot it in python or PERL or R.

But first stage itself is bit tricky.

As an example, you may consider following data:

 -9030
    
  -75

    8005
     -251
    65994
    -12111
       -11643
    19749
     -23324
    10012
     -77	
    	
 	
 Thank you	

",r,statistics,python-2.7,data-analysis,,,open,0,158,7,"Binning the data & plotting the histogram I have a list of values (these are positive as well as negative values). As an example say I have 35000 numbers (+ve and -ve both in it).

What I want to do is to bin them, i.e. the number values between 0-200 (also from -200 to 0), 201-400 (-400 to 201), .... and so on till 48,800-50000 (-50000 to 48,500).

Once I have these values, the plotting of histogram or any other representation is easier. I can take this to excel or plot it in python or PERL or R.

But first stage itself is bit tricky.

As an example, you may consider following data:

 -9030
    
  -75

    8005
     -251
    65994
    -12111
       -11643
    19749
     -23324
    10012
     -77	
    	
 	
 Thank you	

",4
10923470,06/06/2012 22:57:45,1440928,06/06/2012 22:31:54,1,0,How do I convert table formats in R,"Specifically, 


I used the following set up:
> newdata <- tapply(mydata(#), list(mydata(X), mydata(Y)), sum)


I currently have a table that currently is listed as follows:
> X= State, Y= County within State, #= a numerical total of something


> - __ Y1 Y2 Y3 Yn
> - X1 ## ## ## ##
> - X2 ## ## ## ##
> - X3 ## ## ## ##
> - Xn ## ## ## ##


What I need is a table listed as follows:

> - X1 Y1 ##
> - X1 Y2 ##
> - X1 Y3 ##
> - X1 Yn ##
> - X2 Y1 ##
> - X2 Y2 ##
> - X2 Y3 ##
> - X2 Yn ##
> - Xn Y1 ##
> - Xn Y2 ##
> - Xn Y3 ##
> - Xn Yn ##",r,table,tapply,,,,open,0,121,8,"How do I convert table formats in R Specifically, 


I used the following set up:
> newdata <- tapply(mydata(#), list(mydata(X), mydata(Y)), sum)


I currently have a table that currently is listed as follows:
> X= State, Y= County within State, #= a numerical total of something


> - __ Y1 Y2 Y3 Yn
> - X1 ## ## ## ##
> - X2 ## ## ## ##
> - X3 ## ## ## ##
> - Xn ## ## ## ##


What I need is a table listed as follows:

> - X1 Y1 ##
> - X1 Y2 ##
> - X1 Y3 ##
> - X1 Yn ##
> - X2 Y1 ##
> - X2 Y2 ##
> - X2 Y3 ##
> - X2 Yn ##
> - Xn Y1 ##
> - Xn Y2 ##
> - Xn Y3 ##
> - Xn Yn ##",3
8538038,12/16/2011 17:41:23,155406,08/12/2009 20:54:53,1404,22,Return Seconds from Sys.Date in R,"I am sure this is pretty basic, but I am not able to extract various peices of time from the system date.  Simply, I am not sure why I can not accurately extract the current minutes and seconds using %M or %S.  Any thoughts?


It currently is 12:38 pm on my machine.
    
    > format(Sys.Date(), ""%c"")
    [1] ""12/16/2011 12:00:00 AM""

    > R.Version()
    $platform
    [1] ""i386-pc-mingw32""
    
    $arch
    [1] ""i386""
    
    $os
    [1] ""mingw32""
    
    $system
    [1] ""i386, mingw32""
    
    $status
    [1] """"
    
    $major
    [1] ""2""
    
    $minor
    [1] ""14.0""
    
    $year
    [1] ""2011""
    
    $month
    [1] ""10""
    
    $day
    [1] ""31""
    
    $`svn rev`
    [1] ""57496""
    
    $language
    [1] ""R""
    
    $version.string
    [1] ""R version 2.14.0 (2011-10-31)""
",r,datetime-format,,,,,open,0,245,6,"Return Seconds from Sys.Date in R I am sure this is pretty basic, but I am not able to extract various peices of time from the system date.  Simply, I am not sure why I can not accurately extract the current minutes and seconds using %M or %S.  Any thoughts?


It currently is 12:38 pm on my machine.
    
    > format(Sys.Date(), ""%c"")
    [1] ""12/16/2011 12:00:00 AM""

    > R.Version()
    $platform
    [1] ""i386-pc-mingw32""
    
    $arch
    [1] ""i386""
    
    $os
    [1] ""mingw32""
    
    $system
    [1] ""i386, mingw32""
    
    $status
    [1] """"
    
    $major
    [1] ""2""
    
    $minor
    [1] ""14.0""
    
    $year
    [1] ""2011""
    
    $month
    [1] ""10""
    
    $day
    [1] ""31""
    
    $`svn rev`
    [1] ""57496""
    
    $language
    [1] ""R""
    
    $version.string
    [1] ""R version 2.14.0 (2011-10-31)""
",2
6564477,07/03/2011 17:46:00,317773,04/15/2010 17:16:06,762,35,R: {reshape}: (melt.data.frame) How do I replicate a column?,"I have an array of iterations in an MCMC algorithm.  The rows represent draws from a distribution.  The columns represent parameters (variables) in the distribution.  For ease of exposition:  assume two variables, five iterations.  So I have:

    > draws <- data.frame( iteration = c(1:5),
                           alpha     = rnorm(5,0,1),
                           beta      = rnorm(5,0,1))
    
      iteration      alpha       beta
    1         1 -0.3157940  0.2122465
    2         2  1.0087298 -0.2346733
    3         3  1.0366165  0.3472915
    4         4 -2.4256564  0.9863279
    5         5 -0.6089072 -1.1213000

When I melt the dataset, I get:

    > melt(draws)
    Using  as id variables
        variable      value
    1  iteration  1.0000000
    2  iteration  2.0000000
    3  iteration  3.0000000
    4  iteration  4.0000000
    5  iteration  5.0000000
    6      alpha -0.1042616
    7      alpha  1.0707001
    8      alpha  0.2166865
    9      alpha  0.0771617
    10     alpha -0.8893614
    11      beta -0.4846693
    12      beta -1.5950729
    13      beta -0.7178340
    14      beta  1.0149766
    15      beta -0.3128256

But I want to **hold iteration out** so that I get the equivalent of (hand edited):

    > melt(draws)
    Using  as id variables
       iteration  variable      value
    1          1     alpha -0.1042616
    2          2     alpha  1.0707001
    3          3     alpha  0.2166865
    4          4     alpha  0.0771617
    5          5     alpha -0.8893614
    6          1      beta -0.4846693
    7          2      beta -1.5950729
    8          3      beta -0.7178340
    9          4      beta  1.0149766
    10         5      beta -0.3128256

",r,reshape,melt,,,,open,0,659,9,"R: {reshape}: (melt.data.frame) How do I replicate a column? I have an array of iterations in an MCMC algorithm.  The rows represent draws from a distribution.  The columns represent parameters (variables) in the distribution.  For ease of exposition:  assume two variables, five iterations.  So I have:

    > draws <- data.frame( iteration = c(1:5),
                           alpha     = rnorm(5,0,1),
                           beta      = rnorm(5,0,1))
    
      iteration      alpha       beta
    1         1 -0.3157940  0.2122465
    2         2  1.0087298 -0.2346733
    3         3  1.0366165  0.3472915
    4         4 -2.4256564  0.9863279
    5         5 -0.6089072 -1.1213000

When I melt the dataset, I get:

    > melt(draws)
    Using  as id variables
        variable      value
    1  iteration  1.0000000
    2  iteration  2.0000000
    3  iteration  3.0000000
    4  iteration  4.0000000
    5  iteration  5.0000000
    6      alpha -0.1042616
    7      alpha  1.0707001
    8      alpha  0.2166865
    9      alpha  0.0771617
    10     alpha -0.8893614
    11      beta -0.4846693
    12      beta -1.5950729
    13      beta -0.7178340
    14      beta  1.0149766
    15      beta -0.3128256

But I want to **hold iteration out** so that I get the equivalent of (hand edited):

    > melt(draws)
    Using  as id variables
       iteration  variable      value
    1          1     alpha -0.1042616
    2          2     alpha  1.0707001
    3          3     alpha  0.2166865
    4          4     alpha  0.0771617
    5          5     alpha -0.8893614
    6          1      beta -0.4846693
    7          2      beta -1.5950729
    8          3      beta -0.7178340
    9          4      beta  1.0149766
    10         5      beta -0.3128256

",3
1738087,11/15/2009 16:59:10,60628,01/30/2009 14:28:28,3271,113,What can Matlab do that R cannot do?,I often hear people complain how expensive Matlab licenses are. Then I wonder why they don't just use `R`. But is it right? Can you use `R` to replace Matlab?,r,matlab,scientific-computing,,,02/16/2012 03:10:45,not constructive,1,30,8,What can Matlab do that R cannot do? I often hear people complain how expensive Matlab licenses are. Then I wonder why they don't just use `R`. But is it right? Can you use `R` to replace Matlab?,3
11401052,07/09/2012 18:38:37,1444754,06/08/2012 14:28:11,15,0,"R How to plot ""samples"" on the right and ""variables"" on the top in gplots heatmap2?","By using the command:

    heatmap.2(exp, col = greenred(100), scale=""none"", ColSideColors = Carcolors, 
    	# dendrogram = ""row"",
    	key=T, symkey=FALSE, density.info=""none"", trace=""none"", cexRow=1, cexCol=0.9)


The heatmap2 plots ""samples"" as columns and variables as rows. How can I rotate the heatmap counterclock wise 90 degree so the sample names are listed on the right and variables are listed on the top (with RowSideColors on the right also)? Thanks!",r,rotation,axis,heatmap,,,open,0,74,16,"R How to plot ""samples"" on the right and ""variables"" on the top in gplots heatmap2? By using the command:

    heatmap.2(exp, col = greenred(100), scale=""none"", ColSideColors = Carcolors, 
    	# dendrogram = ""row"",
    	key=T, symkey=FALSE, density.info=""none"", trace=""none"", cexRow=1, cexCol=0.9)


The heatmap2 plots ""samples"" as columns and variables as rows. How can I rotate the heatmap counterclock wise 90 degree so the sample names are listed on the right and variables are listed on the top (with RowSideColors on the right also)? Thanks!",4
1429907,09/15/2009 22:14:26,173990,09/15/2009 22:14:26,1,0,Workflow for statistical analysis and report writing,"Does anyone have any wisdom on workflows for data analysis related to custom report writing?  The use-case is basically this:

1.  Client commissions a report that uses data analysis, e.g. a population estimate and related maps for a water district.

2.  The analyst downloads some data, munges the data and saves the result (e.g. adding a column for population per unit, or subsetting the data based on district boundaries).

3.  The analyst analyzes the data created in (2), gets close to her goal, but sees that needs more data and so goes back to (1).

4.  Rinse repeat until the tables and graphics meet QA/QC and satisfy the client.

5.  Write report incorporating tables and graphics.

6.  Next year, the happy client comes back and wants an update.  This should be as simple as updating the upstream data by a new download (e.g. get the building permits from the last year), and pressing a ""RECALCULATE"" button, unless specifications change.

At the moment, I just start a directory and ad-hoc it the best I can.  I would like a more systematic approach, so I am hoping someone has figured this out...  I use a mix of spreadsheets, SQL, ARCGIS, R, and Unix tools.

Thanks!",r,data,statistics,data-visualization,,03/19/2012 15:10:08,not constructive,1,205,7,"Workflow for statistical analysis and report writing Does anyone have any wisdom on workflows for data analysis related to custom report writing?  The use-case is basically this:

1.  Client commissions a report that uses data analysis, e.g. a population estimate and related maps for a water district.

2.  The analyst downloads some data, munges the data and saves the result (e.g. adding a column for population per unit, or subsetting the data based on district boundaries).

3.  The analyst analyzes the data created in (2), gets close to her goal, but sees that needs more data and so goes back to (1).

4.  Rinse repeat until the tables and graphics meet QA/QC and satisfy the client.

5.  Write report incorporating tables and graphics.

6.  Next year, the happy client comes back and wants an update.  This should be as simple as updating the upstream data by a new download (e.g. get the building permits from the last year), and pressing a ""RECALCULATE"" button, unless specifications change.

At the moment, I just start a directory and ad-hoc it the best I can.  I would like a more systematic approach, so I am hoping someone has figured this out...  I use a mix of spreadsheets, SQL, ARCGIS, R, and Unix tools.

Thanks!",4
10489586,05/07/2012 21:44:22,496488,11/03/2010 21:06:18,76,0,Conditional assignment of one variable to the value of one of two other variables,"I want to create a new variable that is equal to the value of one of two other variables, conditional on the values of still other variables. Here's a toy example with fake data.

Each row of the data frame represents a student. Each student can be studying up to two subjects (`subj1` and `subj2`), and can be pursuing a degree (""BA"") or a minor (""MN"") in each subject. My real data includes thousands of students, several types of degree, about 50 subjects, and students can have up to five majors/minors.

       ID  subj1 degree1  subj2 degree2
    1   1    BUS      BA   <NA>    <NA>
    2   2    SCI      BA    ENG      BA
    3   3    BUS      MN    ENG      BA
    4   4    SCI      MN    BUS      BA
    5   5    ENG      BA    BUS      MN
    6   6    SCI      MN   <NA>    <NA>
    7   7    ENG      MN    SCI      BA
    8   8    BUS      BA    ENG      MN
    ...

Now I want to create a sixth variable, `df$major`, that equals the value of `subj1` if `subj1` is the student's primary major, or the value of `subj2` if `subj2` is the primary major. The primary major is the first subject with degree equal to ""BA"". I tried the following code:

    df$major[df$degree1 == ""BA""] = df$subj1
    df$major[df$degree1 != ""BA"" & df$degree2 == ""BA""] = df$subj2

Unfortunately, I got an error message:

    > df$major[df$degree1 == ""BA""] = df$subj1
    Error in df$major[df$degree1 == ""BA""] = df$subj1 : 
      NAs are not allowed in subscripted assignments
I assume this means that a vectorized assignment can't be used if the assignment evaluates to NA for at least one row. 

I feel like I must be missing something basic here, but the code above seemed like the obvious thing to do and I haven't been able to come up with an alternative.

In case it would be helpful in writing an answer, here's sample data, created using `dput()`, in the same format as the fake data listed above:

    structure(list(ID = 1:20, subj1 = structure(c(3L, NA, 1L, 2L, 
    2L, 3L, 2L, 1L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, 3L, 3L, 1L, 2L, 1L
    ), .Label = c(""BUS"", ""ENG"", ""SCI""), class = ""factor""), degree1 = structure(c(2L, 
    NA, 1L, 1L, 1L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L), .Label = c(""BA"", ""MN""), class = ""factor""), subj2 = structure(c(1L, 
    2L, NA, NA, 1L, NA, 3L, 2L, NA, 2L, 2L, 1L, 3L, NA, 2L, 1L, 1L, 
    NA, 2L, 2L), .Label = c(""BUS"", ""ENG"", ""SCI""), class = ""factor""), 
        degree2 = structure(c(2L, 2L, NA, NA, 2L, NA, 1L, 2L, NA, 
        2L, 1L, 1L, 2L, NA, 1L, 2L, 2L, NA, 1L, 2L), .Label = c(""BA"", 
        ""MN""), class = ""factor"")), .Names = c(""ID"", ""subj1"", ""degree1"", 
    ""subj2"", ""degree2""), row.names = c(NA, -20L), class = ""data.frame"")",r,,,,,,open,0,692,14,"Conditional assignment of one variable to the value of one of two other variables I want to create a new variable that is equal to the value of one of two other variables, conditional on the values of still other variables. Here's a toy example with fake data.

Each row of the data frame represents a student. Each student can be studying up to two subjects (`subj1` and `subj2`), and can be pursuing a degree (""BA"") or a minor (""MN"") in each subject. My real data includes thousands of students, several types of degree, about 50 subjects, and students can have up to five majors/minors.

       ID  subj1 degree1  subj2 degree2
    1   1    BUS      BA   <NA>    <NA>
    2   2    SCI      BA    ENG      BA
    3   3    BUS      MN    ENG      BA
    4   4    SCI      MN    BUS      BA
    5   5    ENG      BA    BUS      MN
    6   6    SCI      MN   <NA>    <NA>
    7   7    ENG      MN    SCI      BA
    8   8    BUS      BA    ENG      MN
    ...

Now I want to create a sixth variable, `df$major`, that equals the value of `subj1` if `subj1` is the student's primary major, or the value of `subj2` if `subj2` is the primary major. The primary major is the first subject with degree equal to ""BA"". I tried the following code:

    df$major[df$degree1 == ""BA""] = df$subj1
    df$major[df$degree1 != ""BA"" & df$degree2 == ""BA""] = df$subj2

Unfortunately, I got an error message:

    > df$major[df$degree1 == ""BA""] = df$subj1
    Error in df$major[df$degree1 == ""BA""] = df$subj1 : 
      NAs are not allowed in subscripted assignments
I assume this means that a vectorized assignment can't be used if the assignment evaluates to NA for at least one row. 

I feel like I must be missing something basic here, but the code above seemed like the obvious thing to do and I haven't been able to come up with an alternative.

In case it would be helpful in writing an answer, here's sample data, created using `dput()`, in the same format as the fake data listed above:

    structure(list(ID = 1:20, subj1 = structure(c(3L, NA, 1L, 2L, 
    2L, 3L, 2L, 1L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, 3L, 3L, 1L, 2L, 1L
    ), .Label = c(""BUS"", ""ENG"", ""SCI""), class = ""factor""), degree1 = structure(c(2L, 
    NA, 1L, 1L, 1L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L), .Label = c(""BA"", ""MN""), class = ""factor""), subj2 = structure(c(1L, 
    2L, NA, NA, 1L, NA, 3L, 2L, NA, 2L, 2L, 1L, 3L, NA, 2L, 1L, 1L, 
    NA, 2L, 2L), .Label = c(""BUS"", ""ENG"", ""SCI""), class = ""factor""), 
        degree2 = structure(c(2L, 2L, NA, NA, 2L, NA, 1L, 2L, NA, 
        2L, 1L, 1L, 2L, NA, 1L, 2L, 2L, NA, 1L, 2L), .Label = c(""BA"", 
        ""MN""), class = ""factor"")), .Names = c(""ID"", ""subj1"", ""degree1"", 
    ""subj2"", ""degree2""), row.names = c(NA, -20L), class = ""data.frame"")",1
7482736,09/20/2011 09:10:04,952708,09/19/2011 12:58:48,3,1,Aggregating daily data using quantmod 'to.weekly' function creates weekly data ending on Monday not Friday,"I am trying to aggregate daily share price data (close only) to weekly share price data using the ""to.weekly"" function in quantmod. The xts object `foo` holds daily share price data for a stock starting from Monday 3 January 2011 and ending on Monday 20 September 2011. To aggregate this daily data I used:

`tmp <- to.weekly(foo)`

The above approach succeeds in that `tmp` now holds a series of weekly OHLC data points, as per the quantmod docs. The problem is that the series begins on Monday 3 January 2011 and each subsequent week also begins on Monday e.g. Monday 10 January, Monday 17 January and so on. I had expected the week to default to ending on Friday so that the weekly series started on Friday 7 January and ended on Friday 16 September.

I have experimented with adjusting the start and end of the data and using 'endof' or 'startof' together with the indexAt parameter but I cannot get it to return a week ending in Friday.

I am grateful for any insights received.
(Sorry, I could not find any way to attach dput file so data appears below)

foo:

    2011-01-03 2802
    2011-01-04 2841
    2011-01-05 2883
    2011-01-06 2948
    2011-01-07 2993
    2011-01-10 2993
    2011-01-11 3000
    2011-01-12 3000
    2011-01-13 3025
    2011-01-14 2970
    2011-01-17 2954
    2011-01-18 2976
    2011-01-19 2992
    2011-01-20 2966
    2011-01-21 2940
    2011-01-24 2969
    2011-01-25 2996
    2011-01-26 2982
    2011-01-27 3035
    2011-01-28 3075
    2011-01-31 3020

tmp:

               foo.Open foo.High foo.Low foo.Close
    2011-01-03     2802     2802    2802      2802
    2011-01-10     2841     2993    2841      2993
    2011-01-17     3000     3025    2954      2954
    2011-01-24     2976     2992    2940      2969
    2011-01-31     2996     3075    2982      3020

",r,xts,quantmod,,,,open,0,429,15,"Aggregating daily data using quantmod 'to.weekly' function creates weekly data ending on Monday not Friday I am trying to aggregate daily share price data (close only) to weekly share price data using the ""to.weekly"" function in quantmod. The xts object `foo` holds daily share price data for a stock starting from Monday 3 January 2011 and ending on Monday 20 September 2011. To aggregate this daily data I used:

`tmp <- to.weekly(foo)`

The above approach succeeds in that `tmp` now holds a series of weekly OHLC data points, as per the quantmod docs. The problem is that the series begins on Monday 3 January 2011 and each subsequent week also begins on Monday e.g. Monday 10 January, Monday 17 January and so on. I had expected the week to default to ending on Friday so that the weekly series started on Friday 7 January and ended on Friday 16 September.

I have experimented with adjusting the start and end of the data and using 'endof' or 'startof' together with the indexAt parameter but I cannot get it to return a week ending in Friday.

I am grateful for any insights received.
(Sorry, I could not find any way to attach dput file so data appears below)

foo:

    2011-01-03 2802
    2011-01-04 2841
    2011-01-05 2883
    2011-01-06 2948
    2011-01-07 2993
    2011-01-10 2993
    2011-01-11 3000
    2011-01-12 3000
    2011-01-13 3025
    2011-01-14 2970
    2011-01-17 2954
    2011-01-18 2976
    2011-01-19 2992
    2011-01-20 2966
    2011-01-21 2940
    2011-01-24 2969
    2011-01-25 2996
    2011-01-26 2982
    2011-01-27 3035
    2011-01-28 3075
    2011-01-31 3020

tmp:

               foo.Open foo.High foo.Low foo.Close
    2011-01-03     2802     2802    2802      2802
    2011-01-10     2841     2993    2841      2993
    2011-01-17     3000     3025    2954      2954
    2011-01-24     2976     2992    2940      2969
    2011-01-31     2996     3075    2982      3020

",3
7047159,08/12/2011 22:53:15,778480,05/31/2011 22:48:57,6,0,Stata and R results not matched in Logistic Regression with two categorical predictors and their interaction,"I am getting confused when i am trying to compare the results of Stata and R. I am using example given on the webpage http://www.ats.ucla.edu/stat/stata/webbooks/logistic/chapter2/default.htm
First run the following command in Stata 

    use http://www.ats.ucla.edu/stat/stata/webbooks/logistic/apilog, clear

and then use following commands given in the section (2.2.2 A 2 by 2 Layout with Main Effects and Interaction)

    generate cred_ed = cred_hl*pared_hl
    logit hiqual cred_hl pared_hl cred_ed

These two command will produce the results given on the webpage.

And then i have used following  R code to reproduce same example

    Data<- read.csv(""Book1.csv"",header=T)
    data.glm<-glm(hiqual~cred_hl + pared_hl + cred_hl*pared_hl,family=binomial,  data=Data)
    summary(data.glm)

But results are not matched!

Data file for R can be download from following link

https://spreadsheets.google.com/spreadsheet/ccc?key=0Ajt182RLsguldFlLQmd6Z1ZoczJCenJIdmREUkhxTFE&hl=en_US

Note: Results for model with only main effects are matched but when we include interaction, it is not matched. 

Thanks in Advance.


 
",r,stata,glm,,,08/13/2011 20:01:01,not a real question,1,148,16,"Stata and R results not matched in Logistic Regression with two categorical predictors and their interaction I am getting confused when i am trying to compare the results of Stata and R. I am using example given on the webpage http://www.ats.ucla.edu/stat/stata/webbooks/logistic/chapter2/default.htm
First run the following command in Stata 

    use http://www.ats.ucla.edu/stat/stata/webbooks/logistic/apilog, clear

and then use following commands given in the section (2.2.2 A 2 by 2 Layout with Main Effects and Interaction)

    generate cred_ed = cred_hl*pared_hl
    logit hiqual cred_hl pared_hl cred_ed

These two command will produce the results given on the webpage.

And then i have used following  R code to reproduce same example

    Data<- read.csv(""Book1.csv"",header=T)
    data.glm<-glm(hiqual~cred_hl + pared_hl + cred_hl*pared_hl,family=binomial,  data=Data)
    summary(data.glm)

But results are not matched!

Data file for R can be download from following link

https://spreadsheets.google.com/spreadsheet/ccc?key=0Ajt182RLsguldFlLQmd6Z1ZoczJCenJIdmREUkhxTFE&hl=en_US

Note: Results for model with only main effects are matched but when we include interaction, it is not matched. 

Thanks in Advance.


 
",3
6982751,08/08/2011 13:19:36,840460,07/12/2011 10:01:43,128,6,Preserving indentation in examples section,"Roxygen have made my work a lot easier and is in most cases nice and intuitive. One thing that I have never figured out though is how to preserve indentation in @examples sections so that the result of `roxygenize(""myPackage"")` containing

    #' @examples
    #' sapply(1:10, function(i){
    #'     x <- rbind(matrix(rnorm(20), 10, 2),
    #'                matrix(rnorm(20), 10, 2) + i)
    #'     myFunc(x)
    #' }

would be 

    \examples{sapply(1:10, function(i){
        x <- rbind(matrix(rnorm(20), 10, 2),
                   matrix(rnorm(20), 10, 2) + i)
        myFunc(x)
    }}

instead of 

    \examples{sapply(1:10, function(i){
    x <- rbind(matrix(rnorm(20), 10, 2),
    matrix(rnorm(20), 10, 2) + i)
    myFunc(x)
    }}

It is a small detail but it makes all but the simplest examples unnecessarily hard to read so I'd appreciate it if someone can help me.",r,roxygen,,,,,open,0,212,5,"Preserving indentation in examples section Roxygen have made my work a lot easier and is in most cases nice and intuitive. One thing that I have never figured out though is how to preserve indentation in @examples sections so that the result of `roxygenize(""myPackage"")` containing

    #' @examples
    #' sapply(1:10, function(i){
    #'     x <- rbind(matrix(rnorm(20), 10, 2),
    #'                matrix(rnorm(20), 10, 2) + i)
    #'     myFunc(x)
    #' }

would be 

    \examples{sapply(1:10, function(i){
        x <- rbind(matrix(rnorm(20), 10, 2),
                   matrix(rnorm(20), 10, 2) + i)
        myFunc(x)
    }}

instead of 

    \examples{sapply(1:10, function(i){
    x <- rbind(matrix(rnorm(20), 10, 2),
    matrix(rnorm(20), 10, 2) + i)
    myFunc(x)
    }}

It is a small detail but it makes all but the simplest examples unnecessarily hard to read so I'd appreciate it if someone can help me.",2
11458845,07/12/2012 19:04:32,1451371,06/12/2012 13:38:48,1,0,Adjust first few intervals of Y-axis in R,"I have an R plot. The current Y-axis intervals are as follows

`0, 0.001, 0.003, 0.005, 0.007, 0.009`

I would like to adjust them in such a way that the first three intervals are different but the final maximum value is the same. Like this

`0, 0.0001, 0.0002, 0.0003, 0.0004, 0.001, 0.003, 0.005, 0.007, 0.009`

Basically, I am looking to add additional point to my Y-axis.",r,intervals,,,,07/13/2012 03:34:23,not a real question,1,62,8,"Adjust first few intervals of Y-axis in R I have an R plot. The current Y-axis intervals are as follows

`0, 0.001, 0.003, 0.005, 0.007, 0.009`

I would like to adjust them in such a way that the first three intervals are different but the final maximum value is the same. Like this

`0, 0.0001, 0.0002, 0.0003, 0.0004, 0.001, 0.003, 0.005, 0.007, 0.009`

Basically, I am looking to add additional point to my Y-axis.",2
10031894,04/05/2012 15:54:47,714319,04/18/2011 23:49:11,277,3,communicating with SAS datasets from R,"I have a bunch of datasets that are in SAS format.  I would like to avoid using SAS since I think R provides more than enough functionality for me.  Therefore, is there a package that would allow me to interact with the SAS datasets from R?  I have the SAS software installed but I would like to avoid coding things in multiple languages.",r,sas,,,,,open,0,66,6,"communicating with SAS datasets from R I have a bunch of datasets that are in SAS format.  I would like to avoid using SAS since I think R provides more than enough functionality for me.  Therefore, is there a package that would allow me to interact with the SAS datasets from R?  I have the SAS software installed but I would like to avoid coding things in multiple languages.",2
11655441,07/25/2012 17:39:03,805391,06/19/2011 13:49:47,123,1,Odd failure in eval(parse(...)). Simple answer?,"I've been playing with `eval(parse(...))` in R and I've run into a bump.

In short:

    foo <- 1:36
    eval(parse(text = paste(""pos <- expand.grid("", paste(rep(""c(TRUE, FALSE)"", length(foo)), collapse = "",""), "")"", sep = """")) )

Gives me:

    Error in rep.int(rep.int(seq_len(nx), rep.int(rep.fac, nx)), orep) : 
    invalid 'times' value
    In addition: Warning message:
    In rep.int(rep.int(seq_len(nx), rep.int(rep.fac, nx)), orep) :
    NAs introduced by coercion

As opposed to my lovely combinations of TRUE/FALSE for each item in `foo` in a variable `pos`. `foo` is of arbitrary length.

I am overlooking something simple, or maybe not...?",r,,,,,,open,0,108,6,"Odd failure in eval(parse(...)). Simple answer? I've been playing with `eval(parse(...))` in R and I've run into a bump.

In short:

    foo <- 1:36
    eval(parse(text = paste(""pos <- expand.grid("", paste(rep(""c(TRUE, FALSE)"", length(foo)), collapse = "",""), "")"", sep = """")) )

Gives me:

    Error in rep.int(rep.int(seq_len(nx), rep.int(rep.fac, nx)), orep) : 
    invalid 'times' value
    In addition: Warning message:
    In rep.int(rep.int(seq_len(nx), rep.int(rep.fac, nx)), orep) :
    NAs introduced by coercion

As opposed to my lovely combinations of TRUE/FALSE for each item in `foo` in a variable `pos`. `foo` is of arbitrary length.

I am overlooking something simple, or maybe not...?",1
5877234,05/04/2011 00:28:34,642337,03/03/2011 03:48:55,31,2,"R: transforming ""short form"" data to ""long form"" data without for loops?","Suppose I have an R dataframe like this:

      Subject Session  Property.A Property.B Property.C
    1     100       1 -1.22527548 -0.9193751 -1.7501693
    2     100      10  2.30627980  1.8940830 -0.8443976
    3     100       2  2.33243332 -0.5860868 -4.2074489
    4     100       3  0.38130810 -0.7336206  4.8016230
    5     100       4  1.44685875  0.5066249  2.0138624
    6     100       5  0.08907721 -0.3715202  1.4983700

I have heard this style of data frame referred to as ""short form"" or ""wide form"". Now suppose I want to make it look like this, which I have heard called ""long form"":

      Subject Session  Property    Value
    1     100       1         A   -1.2252754
    2     100       1         B   -0.9193751
    3     100       1         C   -1.7501693
    4     100       2         A    2.3324333
    5     100       2         B   -0.5860868
    6     100       2         C   -4.2074489

That is, I have N columns that I want to reduce to just two ""name/value"" columns, with any other columns in the dataframe extended with repeated values as necessary.

Obviously I could perform this conversion with a bunch of for loops, but that seems really ugly, and it would be a pain to maintain if/when I add more property columns.

Is there a way to do this in R with just a few lines of code? Some magic combination of functions I haven't discovered yet?",r,data.frame,,,,,open,0,438,12,"R: transforming ""short form"" data to ""long form"" data without for loops? Suppose I have an R dataframe like this:

      Subject Session  Property.A Property.B Property.C
    1     100       1 -1.22527548 -0.9193751 -1.7501693
    2     100      10  2.30627980  1.8940830 -0.8443976
    3     100       2  2.33243332 -0.5860868 -4.2074489
    4     100       3  0.38130810 -0.7336206  4.8016230
    5     100       4  1.44685875  0.5066249  2.0138624
    6     100       5  0.08907721 -0.3715202  1.4983700

I have heard this style of data frame referred to as ""short form"" or ""wide form"". Now suppose I want to make it look like this, which I have heard called ""long form"":

      Subject Session  Property    Value
    1     100       1         A   -1.2252754
    2     100       1         B   -0.9193751
    3     100       1         C   -1.7501693
    4     100       2         A    2.3324333
    5     100       2         B   -0.5860868
    6     100       2         C   -4.2074489

That is, I have N columns that I want to reduce to just two ""name/value"" columns, with any other columns in the dataframe extended with repeated values as necessary.

Obviously I could perform this conversion with a bunch of for loops, but that seems really ugly, and it would be a pain to maintain if/when I add more property columns.

Is there a way to do this in R with just a few lines of code? Some magic combination of functions I haven't discovered yet?",2
10077415,04/09/2012 17:53:30,144642,07/24/2009 17:26:01,1307,26,Replacing NAs in R with nearest value,"I'm looking for something similar to na.locf() in the zoo package, but instead of always using the previous non-NA value I'd like to use the nearest NA value, so for example:

    1 3 NA NA 5 7

with na.locf:

    1 3 3 5 7

but I'm looking for:

    1 3 3 5 5 7

I have a solution coded up, but wanted to make sure that I wasn't reinventing the wheel. Is there something already floating around?",r,,,,,,open,0,83,7,"Replacing NAs in R with nearest value I'm looking for something similar to na.locf() in the zoo package, but instead of always using the previous non-NA value I'd like to use the nearest NA value, so for example:

    1 3 NA NA 5 7

with na.locf:

    1 3 3 5 7

but I'm looking for:

    1 3 3 5 5 7

I have a solution coded up, but wanted to make sure that I wasn't reinventing the wheel. Is there something already floating around?",1
10576463,05/14/2012 00:57:32,67405,02/17/2009 14:43:49,2010,4,Multiplying Product Via Loop in R,"I want to multiply all the element in the `fact` vector,
namely:

    final_prod = 0.01 * 0.05 * 0.02

This is how I do it in loop.


    fact <- c(0.01,0.05,0.02)
    
    final_prod <- 1;
    for (i in 1:range(fact)){
       all_prod <- all_prod * fact[i];
    }
    
    print(final_prod)


But the final product it gave is wrong. It should be 
0.00001 instead of 0.01.

What's wrong with my approach above?


I understand there is R'ish way. But the reason
I want to do it in loop is because there is more complex
computation involved.",r,math,product,multiplication,,,open,0,114,6,"Multiplying Product Via Loop in R I want to multiply all the element in the `fact` vector,
namely:

    final_prod = 0.01 * 0.05 * 0.02

This is how I do it in loop.


    fact <- c(0.01,0.05,0.02)
    
    final_prod <- 1;
    for (i in 1:range(fact)){
       all_prod <- all_prod * fact[i];
    }
    
    print(final_prod)


But the final product it gave is wrong. It should be 
0.00001 instead of 0.01.

What's wrong with my approach above?


I understand there is R'ish way. But the reason
I want to do it in loop is because there is more complex
computation involved.",4
5681630,04/15/2011 19:49:06,697363,04/07/2011 18:20:01,11,2,Market Mix Modelling with R,Can anybody suggest me some literature on Market Mix Mode,r,,,,,04/15/2011 20:36:58,off topic,1,10,5,Market Mix Modelling with R Can anybody suggest me some literature on Market Mix Mode,1
7722493,10/11/2011 07:09:02,313163,04/09/2010 20:31:46,1135,25,How does one aggregate and summarize data quickly?,"I have a dataset whose headers look like so:

    PID Time Site Rep Count

I want sum the `Count` by `Rep` for each `PID x Time x Site combo`

on the resulting data.frame, I want to get the mean value of `Count` for `PID x Time x Site` combo.

Current function is as follows:

    dummy <- function (data)
    {
    A<-aggregate(Count~PID+Time+Site+Rep,data=data,function(x){sum(na.omit(x))})
    B<-aggregate(Count~PID+Time+Site,data=A,mean)
    return (B)
    }


This is painfully slow (original data.frame is `510000     20)`. Is there a way to speed this up with plyr?",r,plyr,,,,,open,0,104,8,"How does one aggregate and summarize data quickly? I have a dataset whose headers look like so:

    PID Time Site Rep Count

I want sum the `Count` by `Rep` for each `PID x Time x Site combo`

on the resulting data.frame, I want to get the mean value of `Count` for `PID x Time x Site` combo.

Current function is as follows:

    dummy <- function (data)
    {
    A<-aggregate(Count~PID+Time+Site+Rep,data=data,function(x){sum(na.omit(x))})
    B<-aggregate(Count~PID+Time+Site,data=A,mean)
    return (B)
    }


This is painfully slow (original data.frame is `510000     20)`. Is there a way to speed this up with plyr?",2
9214819,02/09/2012 16:32:23,1000343,10/18/2011 03:41:52,1138,49,"supply a vector to ""classes"" of dataframe","You know how you can supply a vector of names to a data frame to change the col or row names of a dataframe.  Is there a similar method to supply a vector of names that alters the class of each column in a dataframe?  You can do this when you read in a dataframe with read.table using colClasses.  What about if the dataframe is created inside R?

    DF <- as.data.frame(matrix(rnorm(25), 5, 5))
    str(DF)  #all numeric modes
    
    names(DF) <- c(""A"", ""A2"", ""B"", ""B2"", ""Z"") #I want something like this for classes
    some_classes_function_like_names(DF) <- c(rep(""character"", 3), rep(""factor"", 2))
    
    #I can do it like this but this seems inefficient 
    DF[, 1:3] <- sapply(DF[, 1:3], as.character)
    DF[, 4] <- as.factor(DF[, 4])
    DF[, 5] <- as.factor(DF[, 5])
    
    str(DF)

",r,,,,,,open,0,168,7,"supply a vector to ""classes"" of dataframe You know how you can supply a vector of names to a data frame to change the col or row names of a dataframe.  Is there a similar method to supply a vector of names that alters the class of each column in a dataframe?  You can do this when you read in a dataframe with read.table using colClasses.  What about if the dataframe is created inside R?

    DF <- as.data.frame(matrix(rnorm(25), 5, 5))
    str(DF)  #all numeric modes
    
    names(DF) <- c(""A"", ""A2"", ""B"", ""B2"", ""Z"") #I want something like this for classes
    some_classes_function_like_names(DF) <- c(rep(""character"", 3), rep(""factor"", 2))
    
    #I can do it like this but this seems inefficient 
    DF[, 1:3] <- sapply(DF[, 1:3], as.character)
    DF[, 4] <- as.factor(DF[, 4])
    DF[, 5] <- as.factor(DF[, 5])
    
    str(DF)

",1
10327267,04/26/2012 04:30:23,1000343,10/18/2011 03:41:52,3803,140,Annotation above bars:,"dodged bar plot in ggplot again has me stumped.  I asked about annotating text above bars on here a few weeks back ([LINK][1]) and got a terrific response to use `+ stat_bin(geom=""text"", aes(label=..count.., vjust=-1))`.  I figured since I already have the counts I'll just supply them with out the `..` before and after and I told `stat_bin` that the `position` was `dodge`.  It lines them up over the center of the group and adjusts up and down.  Probably something minor.  Please help me to get the text over the bars.

![enter image description here][2]

    mtcars2 <- data.frame(type=factor(mtcars$cyl), 
        group=factor(mtcars$gear))
    library(plyr); library(ggplot)
    dat <- rbind(ddply(mtcars2,.(type,group), summarise,
        count = length(group)),c(8,4,NA))
    
    p2 <- ggplot(dat,aes(x = type,y = count,fill = group)) + 
        geom_bar(colour = ""black"",position = ""dodge"",stat = ""identity"") +
        stat_bin(geom=""text"", aes(position='dodge', label=count, vjust=-.6)) 


  [1]: http://stackoverflow.com/questions/9815226/annotate-values-above-bars-ggplot-faceted
  [2]: http://i.stack.imgur.com/kgYmF.png",r,ggplot2,,,,,open,0,186,3,"Annotation above bars: dodged bar plot in ggplot again has me stumped.  I asked about annotating text above bars on here a few weeks back ([LINK][1]) and got a terrific response to use `+ stat_bin(geom=""text"", aes(label=..count.., vjust=-1))`.  I figured since I already have the counts I'll just supply them with out the `..` before and after and I told `stat_bin` that the `position` was `dodge`.  It lines them up over the center of the group and adjusts up and down.  Probably something minor.  Please help me to get the text over the bars.

![enter image description here][2]

    mtcars2 <- data.frame(type=factor(mtcars$cyl), 
        group=factor(mtcars$gear))
    library(plyr); library(ggplot)
    dat <- rbind(ddply(mtcars2,.(type,group), summarise,
        count = length(group)),c(8,4,NA))
    
    p2 <- ggplot(dat,aes(x = type,y = count,fill = group)) + 
        geom_bar(colour = ""black"",position = ""dodge"",stat = ""identity"") +
        stat_bin(geom=""text"", aes(position='dodge', label=count, vjust=-.6)) 


  [1]: http://stackoverflow.com/questions/9815226/annotate-values-above-bars-ggplot-faceted
  [2]: http://i.stack.imgur.com/kgYmF.png",2
7416949,09/14/2011 13:19:13,944702,09/14/2011 13:19:13,1,0,R-forge vs Rforge?,"I am a novice to R package development.

I just wanted to know which one is better R-forge or R forge? What is the main difference between them ? 

",r,,,,,09/14/2011 15:34:30,not constructive,1,29,3,"R-forge vs Rforge? I am a novice to R package development.

I just wanted to know which one is better R-forge or R forge? What is the main difference between them ? 

",1
11605147,07/23/2012 00:29:09,1469535,06/20/2012 14:19:31,13,0,R - Merging data.frames in the same column containing NAs,"I have four data.frames which all have the same columns, being the first the same to all. In the variable columns there are some NAs.

First, I'd like to replace any value (which is not an NA) in each data.frame by the name of the data.frame.
Second, I'd like to merge the data.frames. In this case, for each NA, there will be some other data.frame which would have a value for it, so that I'd end with every cell filled with values (or names of the data.frames).


Here's an example with two data.frames:

     >A
     name Q  W  E  R  T
     g1   NA NA 4  NA 0
     g2   3  2  NA 4  5
     g3   NA 1  NA 0  0
     g4   0  NA NA 1  9
     
     >B
     name Q  W  E  R  T
     g1   2  4  NA 1  NA
     g2   NA NA 5  NA NA
     g3   5  NA 0  NA NA
     g4   NA 6  4  NA NA

     >result
     name Q  W  E  R  T
     g1   B  B  A  B  A
     g2   A  A  B  A  A
     g3   B  A  B  A  A
     g4   A  B  B  A  A

I've tried some merge() and union() options differently. Also, I've tried to adapt answers to similar questions but I can't seem to solve this. 

http://stackoverflow.com/questions/8349909/creating-a-function-to-replace-nas-from-one-data-frame-with-values-from-another

http://stackoverflow.com/questions/5871916/merging-data-frames-with-missing-values-in-r?rq=1

Thank you in advance!",r,merge,data.frame,,,,open,0,355,10,"R - Merging data.frames in the same column containing NAs I have four data.frames which all have the same columns, being the first the same to all. In the variable columns there are some NAs.

First, I'd like to replace any value (which is not an NA) in each data.frame by the name of the data.frame.
Second, I'd like to merge the data.frames. In this case, for each NA, there will be some other data.frame which would have a value for it, so that I'd end with every cell filled with values (or names of the data.frames).


Here's an example with two data.frames:

     >A
     name Q  W  E  R  T
     g1   NA NA 4  NA 0
     g2   3  2  NA 4  5
     g3   NA 1  NA 0  0
     g4   0  NA NA 1  9
     
     >B
     name Q  W  E  R  T
     g1   2  4  NA 1  NA
     g2   NA NA 5  NA NA
     g3   5  NA 0  NA NA
     g4   NA 6  4  NA NA

     >result
     name Q  W  E  R  T
     g1   B  B  A  B  A
     g2   A  A  B  A  A
     g3   B  A  B  A  A
     g4   A  B  B  A  A

I've tried some merge() and union() options differently. Also, I've tried to adapt answers to similar questions but I can't seem to solve this. 

http://stackoverflow.com/questions/8349909/creating-a-function-to-replace-nas-from-one-data-frame-with-values-from-another

http://stackoverflow.com/questions/5871916/merging-data-frames-with-missing-values-in-r?rq=1

Thank you in advance!",3
5728066,04/20/2011 09:18:03,582695,01/18/2011 09:10:00,146,0,Data following Gaussian Distribution in R..??,"How do we generate data (points) following a Gaussian Distribution in R..?? Suppose I want to generate points in 2d space that follow a Gaussian Distribution .. How do I do this using R..??

I am just getting used to R and I would like some help:)

Thanks...

Pradeep",r,data,gaussian,,,,open,0,46,6,"Data following Gaussian Distribution in R..?? How do we generate data (points) following a Gaussian Distribution in R..?? Suppose I want to generate points in 2d space that follow a Gaussian Distribution .. How do I do this using R..??

I am just getting used to R and I would like some help:)

Thanks...

Pradeep",3
6576666,07/04/2011 22:53:04,296155,03/18/2010 01:11:33,633,10,Extract non-alphabet characters from paste() function in R,"The following variable:

    x <- ""^howdy""

Passed into the paste function like thus:

    paste(x, ""there"", sep=""."")

Returns the string `""^howdy.there""` as expected. How can you eliminate the caret from `howdy` so the string returns only `""howdy.there""`? ",r,concatenation,,,,,open,0,41,8,"Extract non-alphabet characters from paste() function in R The following variable:

    x <- ""^howdy""

Passed into the paste function like thus:

    paste(x, ""there"", sep=""."")

Returns the string `""^howdy.there""` as expected. How can you eliminate the caret from `howdy` so the string returns only `""howdy.there""`? ",2
11406608,07/10/2012 04:28:37,568508,01/09/2011 02:46:19,1684,78,R <- how to return boolean?,"How do I return a Boolean value in R? I don't know what's going on - I'm coming from the Java world.

    Installer.isInstalled <- function(nameOfPackage){
    	return(FALSE)
    }

    result <- Installer.isInstalled(""hintszz"")
    result

    > result
    [1] TRUE",r,,,,,07/11/2012 00:08:00,too localized,1,56,6,"R <- how to return boolean? How do I return a Boolean value in R? I don't know what's going on - I'm coming from the Java world.

    Installer.isInstalled <- function(nameOfPackage){
    	return(FALSE)
    }

    result <- Installer.isInstalled(""hintszz"")
    result

    > result
    [1] TRUE",1
5024630,02/17/2011 02:59:19,296155,03/18/2010 01:11:33,393,7,How do I pass elements of a matrix row to the parameters of an R function?,"Starting with a simple matrix and a simple function:

    numbers <- matrix(c(1:10), nrow = 5, ncol=2)

    numbers
         [,1] [,2]
    [1,]    1    6
    [2,]    2    7
    [3,]    3    8
    [4,]    4    9
    [5,]    5   10

    add <- function(x,y){
    	sum <- x + y
    	return(sum)
        }

I'd like to add a third column that applies the add function by taking the first two elements of each row. 

    cheet_sheet <- cbind(numbers, apply(numbers,<MARGIN=first_2_elements_of_row>, add))

The MARGIN seems like a natural place to specify this, but MARGIN=1 is not enough, as it seems to only take one variable from each row while I need two. 
",r,,,,,,open,0,177,16,"How do I pass elements of a matrix row to the parameters of an R function? Starting with a simple matrix and a simple function:

    numbers <- matrix(c(1:10), nrow = 5, ncol=2)

    numbers
         [,1] [,2]
    [1,]    1    6
    [2,]    2    7
    [3,]    3    8
    [4,]    4    9
    [5,]    5   10

    add <- function(x,y){
    	sum <- x + y
    	return(sum)
        }

I'd like to add a third column that applies the add function by taking the first two elements of each row. 

    cheet_sheet <- cbind(numbers, apply(numbers,<MARGIN=first_2_elements_of_row>, add))

The MARGIN seems like a natural place to specify this, but MARGIN=1 is not enough, as it seems to only take one variable from each row while I need two. 
",1
1776014,11/21/2009 16:53:30,66549,02/15/2009 01:01:06,1,0,Importing Functions into Current Namespace,"Let's say I have an R source file comprised of some functions, doesn't matter what they are, e.g.,

    fnx = function(x){(x - mean(x))/sd(x)}

I would like to be able to access them in my current R session (without typing them in obviously). It would be nice if library(""/path/to/file/my_fn_lib1.r"") worked, as ""import"" works in Python, but it doesn't. One obvious solution is to create an R Package, but i want to avoid that overhead just to import a few functions.",r,,,,,,open,0,81,5,"Importing Functions into Current Namespace Let's say I have an R source file comprised of some functions, doesn't matter what they are, e.g.,

    fnx = function(x){(x - mean(x))/sd(x)}

I would like to be able to access them in my current R session (without typing them in obviously). It would be nice if library(""/path/to/file/my_fn_lib1.r"") worked, as ""import"" works in Python, but it doesn't. One obvious solution is to create an R Package, but i want to avoid that overhead just to import a few functions.",1
10612971,05/16/2012 06:14:39,1397780,05/16/2012 05:46:38,1,0,"R- Query: Order_count<-aggregate(Order_level[""order_no""],by=Order_level[2:i],FUN=length)","Query: 
i=  #input#
Order_count<-aggregate(Order_level[""order_no""],by=Order_level[2:i],FUN=length)

Error:
Error in `[[<-.data.frame`(`*tmp*`, len + i, value = c(25L, 55L, 1L, 5L, : replacement has 172 rows, data has 173

There are no null values in the data and the error occurs when the range in by list is more than 60.

Please, help me on the same.
Thanks!",r,,,,,05/17/2012 12:05:41,not a real question,1,49,3,"R- Query: Order_count<-aggregate(Order_level[""order_no""],by=Order_level[2:i],FUN=length) Query: 
i=  #input#
Order_count<-aggregate(Order_level[""order_no""],by=Order_level[2:i],FUN=length)

Error:
Error in `[[<-.data.frame`(`*tmp*`, len + i, value = c(25L, 55L, 1L, 5L, : replacement has 172 rows, data has 173

There are no null values in the data and the error occurs when the range in by list is more than 60.

Please, help me on the same.
Thanks!",1
8072512,11/09/2011 22:14:42,415690,09/23/2009 19:11:08,5236,122,"Sweave,R,Beamer : How to convert the LaTex text in an Rnw file to R comments?","Say I have a `.Rnw` file containing the usual LaTex mixed in with R code chunks. (I'm especially interested in converting a `.Rnw` slides document, but this question applies to any `.Rnw` document). Now I want to convert this to a file which contains all of the R code, *plus* all of the text that would normally be generated by LaTex, as *R comments*. In other words, the functionality I want is similar to what `Stangle()` does, but I also want all the text part of the LaTex converted to plain text that's commented out in the resulting .R file. 

This would be a very convenient way to automatically generate a commented R file that's easy to look at in your favorite syntax-highlighting editor (e.g. emacs). This might not sound like a great idea for an Sweave document that's a long article with only a bit of R code, but it starts to look appealing when the `.Rnw` document is actually a slide presentation (e.g. using `beamer`) -- then the text portion of the slides would make perfect comments for the R code.  

Anyone have any ideas on how to do this? Thanks in advance.

",r,latex,sweave,beamer,noweb,,open,0,196,15,"Sweave,R,Beamer : How to convert the LaTex text in an Rnw file to R comments? Say I have a `.Rnw` file containing the usual LaTex mixed in with R code chunks. (I'm especially interested in converting a `.Rnw` slides document, but this question applies to any `.Rnw` document). Now I want to convert this to a file which contains all of the R code, *plus* all of the text that would normally be generated by LaTex, as *R comments*. In other words, the functionality I want is similar to what `Stangle()` does, but I also want all the text part of the LaTex converted to plain text that's commented out in the resulting .R file. 

This would be a very convenient way to automatically generate a commented R file that's easy to look at in your favorite syntax-highlighting editor (e.g. emacs). This might not sound like a great idea for an Sweave document that's a long article with only a bit of R code, but it starts to look appealing when the `.Rnw` document is actually a slide presentation (e.g. using `beamer`) -- then the text portion of the slides would make perfect comments for the R code.  

Anyone have any ideas on how to do this? Thanks in advance.

",5
8417215,12/07/2011 14:47:58,1912,08/19/2008 14:07:17,3410,34,Calculate difference of two xts time series when values are at different times,"I have two xts time series in R and want to calculate the differences between the values of the time series at the times which are closest to each other. That is, if my two indices are:

    [1] (10/10/05 13:00:00) (10/10/05 14:00:00) (10/10/05 14:23:00)

and

    [1] (10/10/05 12:38:00) (10/10/05 12:53:00) (10/10/05 12:59:00) (10/10/05 13:08:00) (10/10/05 13:23:00)
    [6] (10/10/05 13:38:00) (10/10/05 13:53:00) (10/10/05 14:23:00) (10/10/05 15:05:00) (10/10/05 15:11:00)

I want to calculate the differences of the values at:

 - 13:00 and 12:59
 - 14:00 and 13:53
 - 14:30 and 14:23

How should I do this? The standard `merge` method from `zoo` with `all=FALSE` won't do what I want because the indices have to be exactly equal for it to merge properly.

Any ideas?",r,time-series,xts,zoo,,,open,0,127,13,"Calculate difference of two xts time series when values are at different times I have two xts time series in R and want to calculate the differences between the values of the time series at the times which are closest to each other. That is, if my two indices are:

    [1] (10/10/05 13:00:00) (10/10/05 14:00:00) (10/10/05 14:23:00)

and

    [1] (10/10/05 12:38:00) (10/10/05 12:53:00) (10/10/05 12:59:00) (10/10/05 13:08:00) (10/10/05 13:23:00)
    [6] (10/10/05 13:38:00) (10/10/05 13:53:00) (10/10/05 14:23:00) (10/10/05 15:05:00) (10/10/05 15:11:00)

I want to calculate the differences of the values at:

 - 13:00 and 12:59
 - 14:00 and 13:53
 - 14:30 and 14:23

How should I do this? The standard `merge` method from `zoo` with `all=FALSE` won't do what I want because the indices have to be exactly equal for it to merge properly.

Any ideas?",4
8370455,12/03/2011 19:44:33,847773,07/16/2011 13:02:36,87,0,How to use random forests in R?,"    library(randomForest)
    rf.model <- randomForest(WIN ~ ., data = learn)

I would like to a random forest model, but I get this error:

    Error in na.fail.default(list(WIN = c(2L, 1L, 1L, 2L, 1L, 2L, 2L, 1L,  : 
    missing values in object

I have data frame learn with 16 numeric atributes and WIN is a factor with levels 0 1.
",r,random-forest,,,,12/04/2011 06:10:27,not a real question,1,70,7,"How to use random forests in R?     library(randomForest)
    rf.model <- randomForest(WIN ~ ., data = learn)

I would like to a random forest model, but I get this error:

    Error in na.fail.default(list(WIN = c(2L, 1L, 1L, 2L, 1L, 2L, 2L, 1L,  : 
    missing values in object

I have data frame learn with 16 numeric atributes and WIN is a factor with levels 0 1.
",2
7828248,10/19/2011 21:14:23,815408,06/25/2011 14:33:19,119,0,Make y-axis logarithmic in histogram using R,"Hi I'm making histogram using R, but the number of Y axis is so large that I need to turn it into logarithmic.See below my script:

    hplot<-read.table(""libl"")
    hplot
    pdf(""first_end"")
    hist(hplot$V1, breaks=24, xlim=c(0,250000000), ylim=c(0,2000000),main=""first end mapping"", xlab=""Coordinates"")
    dev.off()

So how should I change my script?
thx",r,histogram,logarithm,,,,open,0,58,7,"Make y-axis logarithmic in histogram using R Hi I'm making histogram using R, but the number of Y axis is so large that I need to turn it into logarithmic.See below my script:

    hplot<-read.table(""libl"")
    hplot
    pdf(""first_end"")
    hist(hplot$V1, breaks=24, xlim=c(0,250000000), ylim=c(0,2000000),main=""first end mapping"", xlab=""Coordinates"")
    dev.off()

So how should I change my script?
thx",3
7903414,10/26/2011 13:37:23,271616,02/12/2010 05:39:33,20002,456,"How are environments, (en)closures, and frames related?","I want to better understand how [envrionments](http://cran.r-project.org/doc/manuals/R-lang.html#Environment-objects), [closures](http://cran.r-project.org/doc/manuals/R-lang.html#Function-objects), and [frames](http://cran.r-project.org/doc/manuals/R-lang.html#Lexical-environment) are related.  I understand function closures contain an environment, environments contain a frame and an enclosure, and frames contain variables, but I'm a bit fuzzy on how they interact with one another.

Perhaps a detailed step-by-step explanation of what's going on during a function call would help?  Or maybe a diagram?",r,function,closures,frame,environment,,open,0,62,7,"How are environments, (en)closures, and frames related? I want to better understand how [envrionments](http://cran.r-project.org/doc/manuals/R-lang.html#Environment-objects), [closures](http://cran.r-project.org/doc/manuals/R-lang.html#Function-objects), and [frames](http://cran.r-project.org/doc/manuals/R-lang.html#Lexical-environment) are related.  I understand function closures contain an environment, environments contain a frame and an enclosure, and frames contain variables, but I'm a bit fuzzy on how they interact with one another.

Perhaps a detailed step-by-step explanation of what's going on during a function call would help?  Or maybe a diagram?",5
10695979,05/22/2012 05:06:00,1407131,05/21/2012 05:47:13,1,0,error while using zoo package r,"I am trying to analyze high frequency data of nifty using ""zoo"" package in r and i am  following the methods given in  the zoo documentation for reading a csv file as a zoo object . 

I have written the following code but i am getting an error .

    > Lines <- ""
    + Date,Time,Open,High,Low,Close,Up,Down
    + 05.02.2001,00:30,421.20,421.20,421.20,421.20,11,0
    + 05.02.2001,01:30,421.20,421.40,421.20,421.40,7,0
    + 05.02.2001,02:00,421.30,421.30,421.30,421.30,0,5""
    > f <- function(d, t) chron(d, paste(t, ""00"", sep = "":""),
    + format = c(""m.d.y"", ""h:m:s""))
    > z <- read.zoo(textConnection(Lines), sep = "","", header = TRUE,
    + index = 1:2, FUN = f)
    Error in .C(NAME = ""cnt_flds_str"", strings = as.character(str), nstrings =    as.integer(n),  : 
    supplied argument name 'NAME' does not match 'name'
 
I have imported the library zoo and chron as required but i am still getting an error. I have searched over the net but i did not find any help of this regard neither this sort of error any help would be appreciated .",r,zoo,chron,,,05/22/2012 14:47:34,too localized,1,198,6,"error while using zoo package r I am trying to analyze high frequency data of nifty using ""zoo"" package in r and i am  following the methods given in  the zoo documentation for reading a csv file as a zoo object . 

I have written the following code but i am getting an error .

    > Lines <- ""
    + Date,Time,Open,High,Low,Close,Up,Down
    + 05.02.2001,00:30,421.20,421.20,421.20,421.20,11,0
    + 05.02.2001,01:30,421.20,421.40,421.20,421.40,7,0
    + 05.02.2001,02:00,421.30,421.30,421.30,421.30,0,5""
    > f <- function(d, t) chron(d, paste(t, ""00"", sep = "":""),
    + format = c(""m.d.y"", ""h:m:s""))
    > z <- read.zoo(textConnection(Lines), sep = "","", header = TRUE,
    + index = 1:2, FUN = f)
    Error in .C(NAME = ""cnt_flds_str"", strings = as.character(str), nstrings =    as.integer(n),  : 
    supplied argument name 'NAME' does not match 'name'
 
I have imported the library zoo and chron as required but i am still getting an error. I have searched over the net but i did not find any help of this regard neither this sort of error any help would be appreciated .",3
7468675,09/19/2011 09:13:49,203420,11/05/2009 12:38:28,2869,77,Giving an overview of R,"I have been asked to give a 5-10 minute talk on R. There will be around 60 people in the audience from a variety of computing backgrounds. The majority of the audience will be people involved with web-orientated technologies. The purpose of the talk is to introduce a new language to the audience. This is a bit different from ""selling R"". 

My general idea so far is:

 * Where R came from;
 * Who is using R;
 * A couple of slides giving an overview of the language;
 * Some pretty pictures constructed using R.

What other points should I include (or remove)?",r,conferences,overview,talk,,09/19/2011 13:35:26,off topic,1,102,5,"Giving an overview of R I have been asked to give a 5-10 minute talk on R. There will be around 60 people in the audience from a variety of computing backgrounds. The majority of the audience will be people involved with web-orientated technologies. The purpose of the talk is to introduce a new language to the audience. This is a bit different from ""selling R"". 

My general idea so far is:

 * Where R came from;
 * Who is using R;
 * A couple of slides giving an overview of the language;
 * Some pretty pictures constructed using R.

What other points should I include (or remove)?",4
9694994,03/14/2012 02:08:50,1241424,02/29/2012 22:56:02,28,0,How to measure smoothness of a time series in R,"Is there a good way to measure smoothness of a time series in R? For example,

    -1, -0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1.0

is much smoother than 

    -1, 0.8, -0.6, 0.4, -0.2, 0, 0.2, -0.4, 0.6, -0.8, 1.0

although they have same mean and standard deviation.

Thanks a lot

",r,statistics,,,,03/14/2012 14:45:06,off topic,1,57,10,"How to measure smoothness of a time series in R Is there a good way to measure smoothness of a time series in R? For example,

    -1, -0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1.0

is much smoother than 

    -1, 0.8, -0.6, 0.4, -0.2, 0, 0.2, -0.4, 0.6, -0.8, 1.0

although they have same mean and standard deviation.

Thanks a lot

",2
11001047,06/12/2012 16:44:49,1451717,06/12/2012 16:13:13,1,0,Dropped factor levels with GAM fits,"Dropped factor levels with GAM fits
-----------------------------------

I'm using a GAM model to predict species abundance based on some environmental conditions at a given point.  I have created a generalized additive model ( GAM ) to do this and base my predictions from.  However, I have one categorical variable (sediment 
type=[1,2,3,4]) in the model equation.   The equation seems to work just fine,however
the results of the fit seem to absorb the factor level '1' into the intercept.  See below.  >Can anyone explain what is happening with this model?   I do not fully understand.  This >was run in R with the mgcv package.   Thanks!

Parametric coefficients:  
Estimate Std. Error z value Pr(>|z|)

(Intercept)   ------------_7.138 ----- 0.000 ------7541.26   2e-16  
        factor(Sediment)2 -0.2496868  0.0016749 -149.08   2e-16  
        factor(Sediment)3 -0.5128687  0.0058931  -87.03   2e-16  
        factor(Sediment)4 -0.1467369  0.0034606  -42.40   2e-16

Approximate significance of smooth terms:  
              _________   _edf Ref.df  Chi.sq p-value    
s(x) 3.983      4   69264  2e-16  
s(y)  3.998      4 1147536  2e-16   
s(z)  3.995      4  197458  2e-16  
s(w)   3.999      4  340085  2e-16



",r,level,factor,factors,gam,06/12/2012 17:51:11,off topic,1,267,6,"Dropped factor levels with GAM fits Dropped factor levels with GAM fits
-----------------------------------

I'm using a GAM model to predict species abundance based on some environmental conditions at a given point.  I have created a generalized additive model ( GAM ) to do this and base my predictions from.  However, I have one categorical variable (sediment 
type=[1,2,3,4]) in the model equation.   The equation seems to work just fine,however
the results of the fit seem to absorb the factor level '1' into the intercept.  See below.  >Can anyone explain what is happening with this model?   I do not fully understand.  This >was run in R with the mgcv package.   Thanks!

Parametric coefficients:  
Estimate Std. Error z value Pr(>|z|)

(Intercept)   ------------_7.138 ----- 0.000 ------7541.26   2e-16  
        factor(Sediment)2 -0.2496868  0.0016749 -149.08   2e-16  
        factor(Sediment)3 -0.5128687  0.0058931  -87.03   2e-16  
        factor(Sediment)4 -0.1467369  0.0034606  -42.40   2e-16

Approximate significance of smooth terms:  
              _________   _edf Ref.df  Chi.sq p-value    
s(x) 3.983      4   69264  2e-16  
s(y)  3.998      4 1147536  2e-16   
s(z)  3.995      4  197458  2e-16  
s(w)   3.999      4  340085  2e-16



",5
5012516,02/16/2011 04:22:52,573546,01/13/2011 00:53:35,635,28,Count how many consecutive values are true.,"I have an hourly value. I want to count how many consecutive hours the value has been zero since the last time it was not zero. This is an easy job for a spreadsheet or for loop, but I am hoping for a snappy vectorized one-liner to accomplish the task.

    x <- c(1, 0, 1, 0, 0, 0, 1, 1, 0, 0)
    df <- data.frame(x, zcount = NA)
 
    count <- 0
    df$zcount[1] <- ifelse(df$x[1] == 0, 1, 0)
    for(i in 2:nrow(df)) {
      if(df$x[i] == 0) {
        count <- count + 1
        df$zcount[i] <- df$zcount[i - 1] + 1
      } else {
        count <- 0
        df$zcount[i] <- 0
      }
    }


Desired output:

    R> df
       x zcount
    1  1      0
    2  0      1
    3  1      0
    4  0      1
    5  0      2
    6  0      3
    7  1      0
    8  1      0
    9  0      1
    10 0      2",r,,,,,,open,0,304,7,"Count how many consecutive values are true. I have an hourly value. I want to count how many consecutive hours the value has been zero since the last time it was not zero. This is an easy job for a spreadsheet or for loop, but I am hoping for a snappy vectorized one-liner to accomplish the task.

    x <- c(1, 0, 1, 0, 0, 0, 1, 1, 0, 0)
    df <- data.frame(x, zcount = NA)
 
    count <- 0
    df$zcount[1] <- ifelse(df$x[1] == 0, 1, 0)
    for(i in 2:nrow(df)) {
      if(df$x[i] == 0) {
        count <- count + 1
        df$zcount[i] <- df$zcount[i - 1] + 1
      } else {
        count <- 0
        df$zcount[i] <- 0
      }
    }


Desired output:

    R> df
       x zcount
    1  1      0
    2  0      1
    3  1      0
    4  0      1
    5  0      2
    6  0      3
    7  1      0
    8  1      0
    9  0      1
    10 0      2",1
9083536,01/31/2012 16:51:44,339761,05/12/2010 21:35:41,29,1,"Data frame ""expand"" procedure in R?","This is not a real statistical question, but rather a data preparation question before performing the actual statistical analysis. I have a data frame which consists of sparse data. I would like to ""expand"" this data to include zeroes for missing values, group by group.

Here is an example of the data (`a` and `b` are two factors defining the group, `t` is the sparse timestamp and `x`is the value):

	test <- data.frame(
		a=c(1,1,1,1,1,1,1,1,1,1,1),
		b=c(1,1,1,1,1,2,2,2,2,2,2),
		t=c(0,2,3,4,7,3,4,6,7,8,9),
		x=c(1,2,1,2,2,1,1,2,1,1,3))

Assuming I would like to expand the values between `t=0` and `t=9`, this is the result I'm hoping for:

	test.expanded <- data.frame(
		a=c(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1),
		b=c(1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2),
		t=c(0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9),
		x=c(1,0,2,1,2,0,0,2,0,0,0,0,0,1,1,0,2,1,1,3))

Zeroes have been inserted for all missing values of `t`. This makes it easier to use.

I have a quick and dirty implementation which sorts the dataframe and loops through each of its lines, adding missing lines one at a time. But I'm not entirely satisfied by the solution. Is there a better way to do it?

For those who are familiar with SAS, it is similar to the `proc expand`.

Thanks!",r,time-series,,,,,open,0,159,6,"Data frame ""expand"" procedure in R? This is not a real statistical question, but rather a data preparation question before performing the actual statistical analysis. I have a data frame which consists of sparse data. I would like to ""expand"" this data to include zeroes for missing values, group by group.

Here is an example of the data (`a` and `b` are two factors defining the group, `t` is the sparse timestamp and `x`is the value):

	test <- data.frame(
		a=c(1,1,1,1,1,1,1,1,1,1,1),
		b=c(1,1,1,1,1,2,2,2,2,2,2),
		t=c(0,2,3,4,7,3,4,6,7,8,9),
		x=c(1,2,1,2,2,1,1,2,1,1,3))

Assuming I would like to expand the values between `t=0` and `t=9`, this is the result I'm hoping for:

	test.expanded <- data.frame(
		a=c(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1),
		b=c(1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2),
		t=c(0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9),
		x=c(1,0,2,1,2,0,0,2,0,0,0,0,0,1,1,0,2,1,1,3))

Zeroes have been inserted for all missing values of `t`. This makes it easier to use.

I have a quick and dirty implementation which sorts the dataframe and loops through each of its lines, adding missing lines one at a time. But I'm not entirely satisfied by the solution. Is there a better way to do it?

For those who are familiar with SAS, it is similar to the `proc expand`.

Thanks!",2
7463654,09/18/2011 18:53:24,951553,09/18/2011 18:53:24,1,0,First time R user...cant get data into R,"I'm a first time R user and I'm trying to run some data. The data consists of two columns. I copied it from the class website and pasted it on notepad. 
I saved the data on my desktop as : act.txt 
Now I'm trying to upload it into R in the same method our professor tought us : 

data1<- read.table(file=""act.txt"",header=TRUE)

but it's giving me this error : 

Error in file(file, ""rt"") : cannot open the connection
In addition: Warning message:
In file(file, ""rt"") : cannot open file 'act.txt': No such file or directory

Can you please point me in the right direction? 
Thank you! ",r,,,,,09/19/2011 13:35:34,not a real question,1,101,8,"First time R user...cant get data into R I'm a first time R user and I'm trying to run some data. The data consists of two columns. I copied it from the class website and pasted it on notepad. 
I saved the data on my desktop as : act.txt 
Now I'm trying to upload it into R in the same method our professor tought us : 

data1<- read.table(file=""act.txt"",header=TRUE)

but it's giving me this error : 

Error in file(file, ""rt"") : cannot open the connection
In addition: Warning message:
In file(file, ""rt"") : cannot open file 'act.txt': No such file or directory

Can you please point me in the right direction? 
Thank you! ",1
11400002,07/09/2012 17:23:36,1460352,06/16/2012 08:21:40,125,5,Tools for architecture & design of medium-big R projects,"As I'm starting to build relatively big projects in R (>30 functions), I feel the need for more systematic architecture & design thinking. I'm looking for tools that would make it easier for me to have a high-level overview of my code, as well as explaining to others what my code does.

To give you a more concrete idea of what I do: I typically work on the automation of data mining experiments that have been successful. Such a project usually involves: 1. importing data
2. data cleaning & transformation
3. report with descriptive statistics
4. processing of a battery of algorithms & models
5. report that summarizes the results.

What kind of software tools and what kind of methodology should I use ?",r,design,architecture,,,07/10/2012 14:31:18,not constructive,1,118,9,"Tools for architecture & design of medium-big R projects As I'm starting to build relatively big projects in R (>30 functions), I feel the need for more systematic architecture & design thinking. I'm looking for tools that would make it easier for me to have a high-level overview of my code, as well as explaining to others what my code does.

To give you a more concrete idea of what I do: I typically work on the automation of data mining experiments that have been successful. Such a project usually involves: 1. importing data
2. data cleaning & transformation
3. report with descriptive statistics
4. processing of a battery of algorithms & models
5. report that summarizes the results.

What kind of software tools and what kind of methodology should I use ?",3
10476713,05/07/2012 04:38:20,243755,01/05/2010 08:48:52,1108,13,How to upgrade R in ubuntu?,"I have R 2.12.1 installed in my ubuntu, and I'd like upgrade to lastest version 2.15, how can achieve that ? Thanks 
",r,ubuntu,,,,05/07/2012 23:41:37,off topic,1,23,6,"How to upgrade R in ubuntu? I have R 2.12.1 installed in my ubuntu, and I'd like upgrade to lastest version 2.15, how can achieve that ? Thanks 
",2
1429476,09/15/2009 20:36:36,143813,07/23/2009 15:15:37,482,13,longest common substring problem,"Does anyone know of an R package that solves [the longest common substring problem][1]? I am looking for something fast that could work on vectors.


  [1]: http://en.wikipedia.org/wiki/Longest_common_substring_problem",r,string,,,,,open,0,28,4,"longest common substring problem Does anyone know of an R package that solves [the longest common substring problem][1]? I am looking for something fast that could work on vectors.


  [1]: http://en.wikipedia.org/wiki/Longest_common_substring_problem",2
6170053,05/29/2011 19:54:16,734124,05/02/2011 08:20:01,81,0,density of the cauchy distribution,"does anybody know, how to adjust an Estimation based on the ML method in the way, that you give R the assumption of Cauchy distributed error terms? It should be constructed around the survreg.distribution area.

References: [Cauchy Distribution area][1]


  [1]: http://127.0.0.1:27336/library/survival/html/survreg.distributions.html",r,statistics,regression,,,05/30/2011 19:42:39,off topic,1,41,5,"density of the cauchy distribution does anybody know, how to adjust an Estimation based on the ML method in the way, that you give R the assumption of Cauchy distributed error terms? It should be constructed around the survreg.distribution area.

References: [Cauchy Distribution area][1]


  [1]: http://127.0.0.1:27336/library/survival/html/survreg.distributions.html",3
9044789,01/28/2012 11:07:59,373908,06/23/2010 05:52:19,1281,9,using Amazon-EC2 or Linode for hosting a debian server with R regularly extracting information from the web,"I have hacked my NAS-01g into a debian server and use it to regularly download stock quote and earthquake information from the web. I was on a trip last week and I turned off my sever at home, but when I come back, I can no long gain access to the server. Considering re-configuring the server as very time-consuming, I am thinking of migrating my existing server to cloud.

I have a few requirements here:

- server on 24/7
- use cron to hourly call R to extract data from somewhere, say yahoo finance
- (optional) backup and encrypt my gmail account
- (optional) host django server and I am learning to use it now

I am thinking of using amazon-EC2 or Linode. I have tried amazon-EC2 a bit, but the pricing scheme seems very complicated for me, and I want the server to be as cheap as possible as it is not really that mission critical work. I wonder Linode is simpler for a non-system admin like me.

Hope my question won't be considered as off-topic here.

Thanks in advance.",r,amazon-ec2,linode,,,01/30/2012 01:03:03,off topic,1,173,17,"using Amazon-EC2 or Linode for hosting a debian server with R regularly extracting information from the web I have hacked my NAS-01g into a debian server and use it to regularly download stock quote and earthquake information from the web. I was on a trip last week and I turned off my sever at home, but when I come back, I can no long gain access to the server. Considering re-configuring the server as very time-consuming, I am thinking of migrating my existing server to cloud.

I have a few requirements here:

- server on 24/7
- use cron to hourly call R to extract data from somewhere, say yahoo finance
- (optional) backup and encrypt my gmail account
- (optional) host django server and I am learning to use it now

I am thinking of using amazon-EC2 or Linode. I have tried amazon-EC2 a bit, but the pricing scheme seems very complicated for me, and I want the server to be as cheap as possible as it is not really that mission critical work. I wonder Linode is simpler for a non-system admin like me.

Hope my question won't be considered as off-topic here.

Thanks in advance.",3
11508573,07/16/2012 16:31:01,1527107,07/15/2012 15:55:00,6,0,r SCRIPTS for plotting graphs,"I have two columns in a table. How do i script in R to  make a plot of all values of column 1 in x axis and column 2 in y axis?  

The graph should have column 2 in x axis and column 1 in y axis",r,script,,,,07/17/2012 01:47:14,not a real question,1,49,5,"r SCRIPTS for plotting graphs I have two columns in a table. How do i script in R to  make a plot of all values of column 1 in x axis and column 2 in y axis?  

The graph should have column 2 in x axis and column 1 in y axis",2
11428521,07/11/2012 08:27:38,969113,09/28/2011 12:55:06,57,0,Linear Model Overfit due to too many Covariates?,"my study design involves a control and 2 test groups plus some covariates.
Each group consists of around 20 observations. In total I look at around a 1000 variables.

I created a linear model using the lm function in R including 2 covariates. After that
I thought I include another covariate because doing a PCA plot earlier showed a slight effect on that covariate. However, after adding this covariate to the model 50% of the significant hits are now different. I was actually assuming that it would pretty much identical as the effect was hardly seen in the PCA. 

Could it be that I have overfitted the model? Or is the effect simple just not shown in the PCA plot but is there? 

I just compared the two models using anova(lm1, lm2) and the p-val is significant which I think means that the third covariate adds significant information to the model?

    lm1 <- lm(var ~ factor_of_interest + cov1 + cov2)
    lm2 <- lm(var ~ factor_of_interest + cov1 + cov2 + cov3)

    anova(lm1, lm2)

Best
Jacky",r,lm,anova,,,07/11/2012 11:05:30,off topic,1,179,8,"Linear Model Overfit due to too many Covariates? my study design involves a control and 2 test groups plus some covariates.
Each group consists of around 20 observations. In total I look at around a 1000 variables.

I created a linear model using the lm function in R including 2 covariates. After that
I thought I include another covariate because doing a PCA plot earlier showed a slight effect on that covariate. However, after adding this covariate to the model 50% of the significant hits are now different. I was actually assuming that it would pretty much identical as the effect was hardly seen in the PCA. 

Could it be that I have overfitted the model? Or is the effect simple just not shown in the PCA plot but is there? 

I just compared the two models using anova(lm1, lm2) and the p-val is significant which I think means that the third covariate adds significant information to the model?

    lm1 <- lm(var ~ factor_of_interest + cov1 + cov2)
    lm2 <- lm(var ~ factor_of_interest + cov1 + cov2 + cov3)

    anova(lm1, lm2)

Best
Jacky",3
6370383,06/16/2011 10:26:17,366256,06/14/2010 11:03:44,1587,58,What's the R way to do the following group by?,"I have some dataset like this:
    
    # date     # value    class
    1984-04-01 95.32384   A
    1984-04-01 39.86818   B
    1984-07-01 43.57983   A
    1984-07-01 10.83754   B

Now I would like to group the data by data and subtract the value of class B from class A.
I looked into ddply, summarize, melt and aggregate but cannot quite get what I want. Is there a way to do it easily. Note that I have exactly two values per date one of class A and one of class B. I mean i could re-arrange it into two dfs order it by date and class and merge it again, but I feel there is a more R way to do it. 

",r,group-by,,,,,open,0,148,10,"What's the R way to do the following group by? I have some dataset like this:
    
    # date     # value    class
    1984-04-01 95.32384   A
    1984-04-01 39.86818   B
    1984-07-01 43.57983   A
    1984-07-01 10.83754   B

Now I would like to group the data by data and subtract the value of class B from class A.
I looked into ddply, summarize, melt and aggregate but cannot quite get what I want. Is there a way to do it easily. Note that I have exactly two values per date one of class A and one of class B. I mean i could re-arrange it into two dfs order it by date and class and merge it again, but I feel there is a more R way to do it. 

",2
6129585,05/25/2011 19:12:56,764849,05/22/2011 14:02:04,1,0,ticks and text - facet_grid ggplot,"I want to plot a group of time series with ggplot, to be printed on a B5 paper.

Are able to make this plot:

qplot(trade.date, value, data = meltreg2, geom='line', group = variable) + facet_grid(variable ~ ., scale = 'free_y')  +
scale_x_date(expand=c(0,0), name='') +
opts(strip.text.y = theme_blank(), strip.background = theme_blank(), panel.background = theme_rect(), panel.grid.major=theme_blank(), panel.grid.minor=theme_blank(), axis.title.x = theme_blank(), axis.title.y = theme_blank(), panel.margin = unit(0.5, 'line')) 

but is there any way I can get ticks and text on all graphs (three graphs)?

and how should I save the plot to best suit a B5 (landscape) page?",r,,,,,,open,0,91,6,"ticks and text - facet_grid ggplot I want to plot a group of time series with ggplot, to be printed on a B5 paper.

Are able to make this plot:

qplot(trade.date, value, data = meltreg2, geom='line', group = variable) + facet_grid(variable ~ ., scale = 'free_y')  +
scale_x_date(expand=c(0,0), name='') +
opts(strip.text.y = theme_blank(), strip.background = theme_blank(), panel.background = theme_rect(), panel.grid.major=theme_blank(), panel.grid.minor=theme_blank(), axis.title.x = theme_blank(), axis.title.y = theme_blank(), panel.margin = unit(0.5, 'line')) 

but is there any way I can get ticks and text on all graphs (three graphs)?

and how should I save the plot to best suit a B5 (landscape) page?",1
6940022,08/04/2011 10:46:20,878399,08/04/2011 10:46:20,1,0,Running R interactively from windows command prompt,"I need to launch R interactively (to be able to show a plot) from command prompt in win.
Any idea how I can do this?

Thank you.",r,command-line,,,,08/04/2011 11:18:27,off topic,1,25,7,"Running R interactively from windows command prompt I need to launch R interactively (to be able to show a plot) from command prompt in win.
Any idea how I can do this?

Thank you.",2
6230768,06/03/2011 17:51:57,771449,05/26/2011 14:10:42,13,0,Programming a Bivariate Normal CDF in R,"I have a question regarding coding a function that contains a bivariate normal CDF in R.  The function I am trying to code requires one bivariate normal CDF, that should be calculated differently depending on the observation.  Specifically, depending upon the value of a certain variable, the correlation should ""switch"" between being positive and negative, but there should be no difference in the call.  

This style of function has been coded in LIMDEP and I am trying to replicate it, but have not been able to get it to work in R.  The command in LIMDEP to calculate a bivariate normal CDF is ""BVN(x1, x2, r)"", which explicitly requires the two variables used for calculation (x1, x2) and the correlation (r).  LIMDEP uses the Gauss-Laguerre 15 point quadrature to calculate the bivariate normal CDF.  

In R, it appears that two packages calculate the multivariate normal CDF.  I have been trying the mnormt package (though there is the mvtnorm package as well--I do not see a major difference) that uses the Genz method, which seems to be similar, but more general than the Gauss-Laguerre 15 quadrature method used in LIMDEP (referencing the papers under ?pmnorm).  

Everytime that I have tried to use the mnormt package, the command pmnorm(), requires the form: pmnorm(data, mean, varcov), which I have not been able to code for correlation switching.

Any ideas how to get this to work??

Here is an example of some trivial code to explain what I am talking about what I would like to do (except inside of a function without the for loop): 

     A <- c(0,1, 1, 1, 0, 1, 0, 1, 0, 1)
     q <- 2*A-1 
     set.seed(1234)
     x <- rnorm(10)
     y <- rnorm(10, 2, 2)

     #Need to return a value of the CDF for each row of data:
     cdf.results <- 0
     for(i in 1:length(A)){
     vc.mat <- matrix(c(1, q[i]*.7, q[i]*.7, 1.3), 2, 2)
     cdf.results[i] <- pmnorm(cbind(x[i], y[i]), c(0, 0), vc.mat) 
     }
     cdf.results

Thanks for your help!
",r,statistics,,,,,open,0,381,7,"Programming a Bivariate Normal CDF in R I have a question regarding coding a function that contains a bivariate normal CDF in R.  The function I am trying to code requires one bivariate normal CDF, that should be calculated differently depending on the observation.  Specifically, depending upon the value of a certain variable, the correlation should ""switch"" between being positive and negative, but there should be no difference in the call.  

This style of function has been coded in LIMDEP and I am trying to replicate it, but have not been able to get it to work in R.  The command in LIMDEP to calculate a bivariate normal CDF is ""BVN(x1, x2, r)"", which explicitly requires the two variables used for calculation (x1, x2) and the correlation (r).  LIMDEP uses the Gauss-Laguerre 15 point quadrature to calculate the bivariate normal CDF.  

In R, it appears that two packages calculate the multivariate normal CDF.  I have been trying the mnormt package (though there is the mvtnorm package as well--I do not see a major difference) that uses the Genz method, which seems to be similar, but more general than the Gauss-Laguerre 15 quadrature method used in LIMDEP (referencing the papers under ?pmnorm).  

Everytime that I have tried to use the mnormt package, the command pmnorm(), requires the form: pmnorm(data, mean, varcov), which I have not been able to code for correlation switching.

Any ideas how to get this to work??

Here is an example of some trivial code to explain what I am talking about what I would like to do (except inside of a function without the for loop): 

     A <- c(0,1, 1, 1, 0, 1, 0, 1, 0, 1)
     q <- 2*A-1 
     set.seed(1234)
     x <- rnorm(10)
     y <- rnorm(10, 2, 2)

     #Need to return a value of the CDF for each row of data:
     cdf.results <- 0
     for(i in 1:length(A)){
     vc.mat <- matrix(c(1, q[i]*.7, q[i]*.7, 1.3), 2, 2)
     cdf.results[i] <- pmnorm(cbind(x[i], y[i]), c(0, 0), vc.mat) 
     }
     cdf.results

Thanks for your help!
",2
11592470,07/21/2012 13:11:11,1199861,02/09/2012 14:32:36,25,0,Generalized least square model giving auto correlated residuals,"I work for a financial institution, we often times run into analysis of time series data. A lot of times we end up doing regression using time series variables. As this happens, we often encounter residuals with time series structure that violates basic assumption of independent errors in OLS regression.  We are building another model in which I believe we have regression with auto correlated errors. The residuals from linear model (lm(object))  clearly has a AR(1) structure, as evident from ACF and PACF. I fit the model using Generalized least square model using gls() in R. My expectation was that the residuals from gls(object) is white noise,but residuals from gls(object) still have the same ARIMA structure as in the ordinary regression. Unfortunately there is something wrong in what I am doing which I could not figure out.  Hence I decided to manually adjust the regression coefficients from the linear model (OLS estimates). Surprisingly that seems to be working when I plotted the residuals from adjusted regression (residuals are white noise). I really want to use gls() in ‘nlme’ package so that coding will be lot simpler and easier. 

lm.bk_ai <- lm( PRNP_BK_actINV~ PRM_BK_INV_ENDING + NPRM_BK_INV_ENDING + PRM_BK_INV_OUTFLOW + NPRM_BK_INV_OUTFLOW +
                PRM_BK_INV_OUTFLOW + NPRM_BK_INV_OUTFLOW,
                na.action =na.exclude, data = fit.cap01A)

summary(lm.bk_ai)

a <- stepAIC(lm.bk_ai, direction=""backward"") #-- selecting the best model based on least AIC value

a.upd <- update.formula(lm.bk_ai$call, a$call)

lm.bk_ai <- lm(as.formula(a.upd), data=fit.cap01A, na.action=na.exclude)

summary(lm.bk_ai)

lm.bk_ai <- update(lm.bk_ai, . ~ . -NPRM_BK_INV_OUTFLOW, data=fit.cap01A, na.action=na.exclude) ## the non-significant variable was droped
summary(lm.bk_ai)

acf2(residuals(lm.bk_ai))  ## acf2() function from Stoffer and Shumway is used here,....AR(1) will be the plausible model for residuals

arma.bk_ai <- sarima(residuals(lm.bk_ai), 1,0,0 ) ## sarima() function from Stoffer and Shumway is used here...arima() from stat package can also be used

phi <- coefficients(arma.bk_ai$fit)[1]

gls.bk_ai <- gls(PRNP_BK_actINV ~ PRM_BK_INV_ENDING + NPRM_BK_INV_ENDING, correlation=corARMA(p=1), method='ML',  data  = fit.cap01A) # -Fitting Generalized least square
 

summary(gls.bk_ai)

gls2.bk_ai  <- update(gls.bk_ai, correlation = corARMA(p=2)) ## testing different (gls) model 

gls3.bk_ai <- update(gls.bk_ai, correlation = corARMA(p=3))

gls0.bk_ai <- update(gls.bk_ai, correlation = NULL)


anova(gls.bk_ai, gls2.bk_ai, gls3.bk_ai, gls0.bk_ai)  ## looking at the AIC value, gls model with AR(1) will be the best bet

acf2(residuals(gls.bk_ai)) # residuals are not white noise-------- There is something wrong in what I am doing???????
",r,,,,,07/28/2012 13:49:30,off topic,1,391,8,"Generalized least square model giving auto correlated residuals I work for a financial institution, we often times run into analysis of time series data. A lot of times we end up doing regression using time series variables. As this happens, we often encounter residuals with time series structure that violates basic assumption of independent errors in OLS regression.  We are building another model in which I believe we have regression with auto correlated errors. The residuals from linear model (lm(object))  clearly has a AR(1) structure, as evident from ACF and PACF. I fit the model using Generalized least square model using gls() in R. My expectation was that the residuals from gls(object) is white noise,but residuals from gls(object) still have the same ARIMA structure as in the ordinary regression. Unfortunately there is something wrong in what I am doing which I could not figure out.  Hence I decided to manually adjust the regression coefficients from the linear model (OLS estimates). Surprisingly that seems to be working when I plotted the residuals from adjusted regression (residuals are white noise). I really want to use gls() in ‘nlme’ package so that coding will be lot simpler and easier. 

lm.bk_ai <- lm( PRNP_BK_actINV~ PRM_BK_INV_ENDING + NPRM_BK_INV_ENDING + PRM_BK_INV_OUTFLOW + NPRM_BK_INV_OUTFLOW +
                PRM_BK_INV_OUTFLOW + NPRM_BK_INV_OUTFLOW,
                na.action =na.exclude, data = fit.cap01A)

summary(lm.bk_ai)

a <- stepAIC(lm.bk_ai, direction=""backward"") #-- selecting the best model based on least AIC value

a.upd <- update.formula(lm.bk_ai$call, a$call)

lm.bk_ai <- lm(as.formula(a.upd), data=fit.cap01A, na.action=na.exclude)

summary(lm.bk_ai)

lm.bk_ai <- update(lm.bk_ai, . ~ . -NPRM_BK_INV_OUTFLOW, data=fit.cap01A, na.action=na.exclude) ## the non-significant variable was droped
summary(lm.bk_ai)

acf2(residuals(lm.bk_ai))  ## acf2() function from Stoffer and Shumway is used here,....AR(1) will be the plausible model for residuals

arma.bk_ai <- sarima(residuals(lm.bk_ai), 1,0,0 ) ## sarima() function from Stoffer and Shumway is used here...arima() from stat package can also be used

phi <- coefficients(arma.bk_ai$fit)[1]

gls.bk_ai <- gls(PRNP_BK_actINV ~ PRM_BK_INV_ENDING + NPRM_BK_INV_ENDING, correlation=corARMA(p=1), method='ML',  data  = fit.cap01A) # -Fitting Generalized least square
 

summary(gls.bk_ai)

gls2.bk_ai  <- update(gls.bk_ai, correlation = corARMA(p=2)) ## testing different (gls) model 

gls3.bk_ai <- update(gls.bk_ai, correlation = corARMA(p=3))

gls0.bk_ai <- update(gls.bk_ai, correlation = NULL)


anova(gls.bk_ai, gls2.bk_ai, gls3.bk_ai, gls0.bk_ai)  ## looking at the AIC value, gls model with AR(1) will be the best bet

acf2(residuals(gls.bk_ai)) # residuals are not white noise-------- There is something wrong in what I am doing???????
",1
10252065,04/20/2012 19:04:31,1231039,02/24/2012 15:01:30,2,3,Create a Code Repository in RStudio,"I have code that will read in and process data that I would like to share with multiple users, but I do not want allow them to see the code.

Is there a way to setup a code repository using R or RStudio?",r,rstudio,,,,04/21/2012 18:37:01,not a real question,1,42,6,"Create a Code Repository in RStudio I have code that will read in and process data that I would like to share with multiple users, but I do not want allow them to see the code.

Is there a way to setup a code repository using R or RStudio?",2
5705564,04/18/2011 15:54:24,365444,06/12/2010 23:35:17,107,3,Is there an existing ANTLR or IRONY grammar for R?,"Does anyone know if there is an existing existing ANTLR or IRONY grammar for R?

Many thanks.",r,language,antlr,grammar,irony,,open,0,16,10,"Is there an existing ANTLR or IRONY grammar for R? Does anyone know if there is an existing existing ANTLR or IRONY grammar for R?

Many thanks.",5
10547487,05/11/2012 07:49:49,459839,09/27/2010 19:04:54,181,7,R: removing facet_wrap labels completely in ggplot2,"I'd like to remove the labels for the facets completely to create a sort of sparkline effect, as for the audience the labels are irrelevant, the best I can come up with is:

    library(MASS)
    library(ggplot2)
    qplot(week,y,data=bacteria,group=ID, geom=c('point','line'), xlab='', ylab='') + 
         facet_wrap(~ID) + 
         opts(strip.text.x = theme_text(size=0))

So can I get rid of the (now blank) strip.background completely to allow more space for the ""sparklines""?

Or alternatively is there a better way to get this ""sparkline"" effect for a large number of binary valued time-series like this?
",r,graphics,ggplot2,,,,open,0,110,7,"R: removing facet_wrap labels completely in ggplot2 I'd like to remove the labels for the facets completely to create a sort of sparkline effect, as for the audience the labels are irrelevant, the best I can come up with is:

    library(MASS)
    library(ggplot2)
    qplot(week,y,data=bacteria,group=ID, geom=c('point','line'), xlab='', ylab='') + 
         facet_wrap(~ID) + 
         opts(strip.text.x = theme_text(size=0))

So can I get rid of the (now blank) strip.background completely to allow more space for the ""sparklines""?

Or alternatively is there a better way to get this ""sparkline"" effect for a large number of binary valued time-series like this?
",3
9023830,01/26/2012 19:11:57,984532,10/07/2011 18:26:21,3,0,What is the first and second best thing/site/book out there to search when you have an R language difficulty?,"I have used R for about 3 years now.
I read couple of books but I run into questions every day.
I sometimes try to navigate through so many copies of r-help listserv (I still have not found a preferred mirror site with nice features)
I read help files but recently I started using SO as my primary resource when I don't know something (e.g., how to create a random subset from a data.frame), but often I don't find the answer.

Can you reply with a list of top 2 or 3 places you go to when you have an R question?

(If your first is SO, list it, but try to provide your second best source as well.)",r,,,,,01/26/2012 20:18:31,not constructive,1,114,19,"What is the first and second best thing/site/book out there to search when you have an R language difficulty? I have used R for about 3 years now.
I read couple of books but I run into questions every day.
I sometimes try to navigate through so many copies of r-help listserv (I still have not found a preferred mirror site with nice features)
I read help files but recently I started using SO as my primary resource when I don't know something (e.g., how to create a random subset from a data.frame), but often I don't find the answer.

Can you reply with a list of top 2 or 3 places you go to when you have an R question?

(If your first is SO, list it, but try to provide your second best source as well.)",1
6196733,06/01/2011 05:35:34,275002,02/17/2010 06:33:40,97,1,Learning R with real examples,"I have got ""R Cookbook"". It is cool but its like a reference book with all commands and basic things. What I am interested to work on some real data by practicing R. How can I do it? Also, I read stats in varsity(around 10 years back) therefore I would request you guys to recommend some stat book as well for programmers.

Thanks",r,statistics,information-retrieval,information,,06/02/2011 16:30:25,not a real question,1,62,5,"Learning R with real examples I have got ""R Cookbook"". It is cool but its like a reference book with all commands and basic things. What I am interested to work on some real data by practicing R. How can I do it? Also, I read stats in varsity(around 10 years back) therefore I would request you guys to recommend some stat book as well for programmers.

Thanks",4
6632171,07/09/2011 01:44:33,836305,07/09/2011 01:31:25,1,0,R error caused by commenting after #s,"I am generating code in R studio, it works fine. I send the code to a friend and when she runs it errors are caused, and these errors are eliminated by removing commentary on code. 

text following hashtags is causing errors. I am lost and it is really difficult to try googling using the number sign. 

Any help would be greatly appreciated. 

Sean",r,error-message,commenting,,,07/10/2011 01:34:41,not a real question,1,63,7,"R error caused by commenting after #s I am generating code in R studio, it works fine. I send the code to a friend and when she runs it errors are caused, and these errors are eliminated by removing commentary on code. 

text following hashtags is causing errors. I am lost and it is really difficult to try googling using the number sign. 

Any help would be greatly appreciated. 

Sean",3
2330169,02/24/2010 22:35:10,280761,02/24/2010 22:35:09,1,0,How do I position a central subtitle in my two-sided gplot-pyramid? ,"I created an age-sex-pyramid using gplots. I would like to center a subtitle between the two sides of the pyramid.

However, I can only get the subtitle aligned with one of the two sides of the pyramid:

    library(gplots)
    agetable <- as.data.frame(cbind (c(2, 4, 7, 8, 10, 8, 6, 4, 2, 1), c(1, 3, 5, 9, 11, 6, 4, 1, 0, 1)))
    names(agetable) <- c(""Male"", ""Female"")
    maxdir <- max(agetable)
    subtitle <- ""34% of data are mising age or sex""

    agegraph.general.bysex <- function(agetable, maxdir, varname, miss){
    agegroups <- 
    if (varname=='Male') {
      datavec <- agetable[,'Male']
      lim = c(maxdir,0)
      agelabels = c('0-9','10-19','20-29','30-39','40-49','50-59',
        '60-69','70-79','80-89','90+')
    } else {
      datavec <- agetable[,'Female']
      lim = c(0, maxdir)
         agelabels = ''
    }
    barplot2(
           datavec, horiz=TRUE, space=0, xlab = varname, xlim=lim, col='grey85', axisnames=TRUE, cex.axis = 1,
           cex.names = 1, names.arg=agelabels, plot.grid=TRUE, cex.sub = 0.8, sub = miss)
    }


    layout(matrix(1:2, ncol=2, nrow=1))
    par(las=1, adj=0.5, omi=c(0.1, 0.1, 0.1 ,0.1), cex=0.8 , cex.lab=0.8)
    par(mar=c(7,5,0,0))
    agegraph.general.bysex(agetable, maxdir, 'Male', subtitle)
    par(mar=c(7,0,0,5))
    agegraph.general.bysex(agetable, maxdir, 'Female', '')

I would be grateful for any suggestions!
",r,,,,,,open,0,280,12,"How do I position a central subtitle in my two-sided gplot-pyramid?  I created an age-sex-pyramid using gplots. I would like to center a subtitle between the two sides of the pyramid.

However, I can only get the subtitle aligned with one of the two sides of the pyramid:

    library(gplots)
    agetable <- as.data.frame(cbind (c(2, 4, 7, 8, 10, 8, 6, 4, 2, 1), c(1, 3, 5, 9, 11, 6, 4, 1, 0, 1)))
    names(agetable) <- c(""Male"", ""Female"")
    maxdir <- max(agetable)
    subtitle <- ""34% of data are mising age or sex""

    agegraph.general.bysex <- function(agetable, maxdir, varname, miss){
    agegroups <- 
    if (varname=='Male') {
      datavec <- agetable[,'Male']
      lim = c(maxdir,0)
      agelabels = c('0-9','10-19','20-29','30-39','40-49','50-59',
        '60-69','70-79','80-89','90+')
    } else {
      datavec <- agetable[,'Female']
      lim = c(0, maxdir)
         agelabels = ''
    }
    barplot2(
           datavec, horiz=TRUE, space=0, xlab = varname, xlim=lim, col='grey85', axisnames=TRUE, cex.axis = 1,
           cex.names = 1, names.arg=agelabels, plot.grid=TRUE, cex.sub = 0.8, sub = miss)
    }


    layout(matrix(1:2, ncol=2, nrow=1))
    par(las=1, adj=0.5, omi=c(0.1, 0.1, 0.1 ,0.1), cex=0.8 , cex.lab=0.8)
    par(mar=c(7,5,0,0))
    agegraph.general.bysex(agetable, maxdir, 'Male', subtitle)
    par(mar=c(7,0,0,5))
    agegraph.general.bysex(agetable, maxdir, 'Female', '')

I would be grateful for any suggestions!
",1
10467322,05/06/2012 01:17:11,1096545,12/13/2011 20:14:19,374,8,"Loops in R - Need to use index, anyway to avoid 'for'?","I know it's not the best practice in R to use the `for` loop because it doesn't have an enhanced performance. For almost all cases there is a function of the family `*apply` that solves our problems.

However I'm facing a situation where I don't see a workaround.

I need to calculate percent variation for consecutive values:

    pv[1] <- 0
    for(i in 2:length(x)) {
      pv[i] <- (x[i] - x[i-1])/x[i-1]
    }

So, as you can see, I have to use both the `x[i]` element, but also  the `x[i-1]` element. By using the `*apply` functions, I just see how to use the `x[i]`. Is there anyway I can avoid the `for`loops?",r,loops,for-loop,,,,open,0,121,12,"Loops in R - Need to use index, anyway to avoid 'for'? I know it's not the best practice in R to use the `for` loop because it doesn't have an enhanced performance. For almost all cases there is a function of the family `*apply` that solves our problems.

However I'm facing a situation where I don't see a workaround.

I need to calculate percent variation for consecutive values:

    pv[1] <- 0
    for(i in 2:length(x)) {
      pv[i] <- (x[i] - x[i-1])/x[i-1]
    }

So, as you can see, I have to use both the `x[i]` element, but also  the `x[i-1]` element. By using the `*apply` functions, I just see how to use the `x[i]`. Is there anyway I can avoid the `for`loops?",3
11093248,06/19/2012 01:27:26,260533,01/27/2010 23:30:08,373,6,geom_vline with Character xintercept,"I have some ggplot code that worked fine in 0.8.9 but not in 0.9.1.

I am going to plot the data in `theDF` and would like to plot a vertical line at `xintercept=""2010 Q1.""`  `theGrid` is merely used to create `theDF`.

    theGrid <- expand.grid(2009:2011, 1:4)
    theDF <- data.frame(YrQtr=sprintf(""%s Q%s"", theGrid$Var1, theGrid$Var2), Minutes=c(1000, 2200, 1450, 1825, 1970, 1770, 1640, 1920, 1790, 1800, 1750, 1600))

The code used is:

    g <- ggplot(theDF, aes(x=YrQtr, y=Minutes)) + geom_point() + opts(axis.text.x=theme_text(angle=90))
    g + geom_vline(data=theVerts, aes(data=data.frame(Vert=""2010 Q2""), xintercept=Vert))

Again, this worked fine in R 2.13.2 with ggplot2 0.8.9, but does not in R 2.14+ with ggplot2 0.9.1.

A workaround is:

    g + geom_vline(data=theVerts, aes(data=data.frame(Vert=4), xintercept=Vert))

But that is not a good solution for my problem.

Maybe messing around with `scale_x_discrete` might help?",r,plot,ggplot2,,,,open,0,136,4,"geom_vline with Character xintercept I have some ggplot code that worked fine in 0.8.9 but not in 0.9.1.

I am going to plot the data in `theDF` and would like to plot a vertical line at `xintercept=""2010 Q1.""`  `theGrid` is merely used to create `theDF`.

    theGrid <- expand.grid(2009:2011, 1:4)
    theDF <- data.frame(YrQtr=sprintf(""%s Q%s"", theGrid$Var1, theGrid$Var2), Minutes=c(1000, 2200, 1450, 1825, 1970, 1770, 1640, 1920, 1790, 1800, 1750, 1600))

The code used is:

    g <- ggplot(theDF, aes(x=YrQtr, y=Minutes)) + geom_point() + opts(axis.text.x=theme_text(angle=90))
    g + geom_vline(data=theVerts, aes(data=data.frame(Vert=""2010 Q2""), xintercept=Vert))

Again, this worked fine in R 2.13.2 with ggplot2 0.8.9, but does not in R 2.14+ with ggplot2 0.9.1.

A workaround is:

    g + geom_vline(data=theVerts, aes(data=data.frame(Vert=4), xintercept=Vert))

But that is not a good solution for my problem.

Maybe messing around with `scale_x_discrete` might help?",3
4244157,11/22/2010 09:54:21,392445,07/15/2010 08:15:10,66,3,ggplot2: add line for average per group,"I want to create a plot like this:

    library(ggplot2)
    
    y <- rnorm(20)
    x <- 1:20
    group <- c(rep(""A"", 8), rep(""B"", 7), rep(""C"", 5))
    df <- data.frame(x, y, group)

    ggplot(data = df, aes(x=x, y=y)) + 
    geom_point(aes(colour = group)) + 
    stat_summary(fun.y=mean, geom=""line"", aes(group=group, colour = group))

I want to create a graph like this:
![graph with averages for each group][1]

However, stats_summary now just adds lines between the points in their respective colour.
what should I do to get horizontal lines representing the average value for each group?

  [1]: http://i.stack.imgur.com/e20c9.png",r,ggplot2,,,,,open,0,115,7,"ggplot2: add line for average per group I want to create a plot like this:

    library(ggplot2)
    
    y <- rnorm(20)
    x <- 1:20
    group <- c(rep(""A"", 8), rep(""B"", 7), rep(""C"", 5))
    df <- data.frame(x, y, group)

    ggplot(data = df, aes(x=x, y=y)) + 
    geom_point(aes(colour = group)) + 
    stat_summary(fun.y=mean, geom=""line"", aes(group=group, colour = group))

I want to create a graph like this:
![graph with averages for each group][1]

However, stats_summary now just adds lines between the points in their respective colour.
what should I do to get horizontal lines representing the average value for each group?

  [1]: http://i.stack.imgur.com/e20c9.png",2
8287224,11/27/2011 16:54:32,676644,10/29/2010 18:40:46,12,1,Rweb Vs. RStudio Web Interface,I am wondering what are the relative merits of Rweb and RStudio web console,r,rstudio,,,,12/01/2011 09:20:10,not constructive,1,14,5,Rweb Vs. RStudio Web Interface I am wondering what are the relative merits of Rweb and RStudio web console,2
3284541,07/19/2010 19:59:38,370756,06/18/2010 23:01:39,688,42,Rough sets in R,"Do you know any R packages implementing rough set based methods? I believe the answer is there are no, but maybe I am missing something subtle.",r,,,,,,open,0,26,4,"Rough sets in R Do you know any R packages implementing rough set based methods? I believe the answer is there are no, but maybe I am missing something subtle.",1
10956582,06/08/2012 22:08:34,403279,07/27/2010 10:53:25,658,19,How to put more information on my R Pie Chart ?,"I have the following simple R code.

    vect <- c(a = 4, b = 7, c = 5)
    pie(vect, labels = c(""A"", ""B"", ""C""), col = c(""#999999"", ""#6F6F6F"", ""#000000""))

that print the following pie chart 

![enter image description here][1]

How do I modify the above code to print more detail on my pie chart, so that it look like this other one ?

![enter image description here][2]

Thanks for any reply !

  [1]: http://i.stack.imgur.com/IJ0lx.png
  [2]: http://i.stack.imgur.com/vFWer.png",r,pie-chart,,,,,open,0,80,11,"How to put more information on my R Pie Chart ? I have the following simple R code.

    vect <- c(a = 4, b = 7, c = 5)
    pie(vect, labels = c(""A"", ""B"", ""C""), col = c(""#999999"", ""#6F6F6F"", ""#000000""))

that print the following pie chart 

![enter image description here][1]

How do I modify the above code to print more detail on my pie chart, so that it look like this other one ?

![enter image description here][2]

Thanks for any reply !

  [1]: http://i.stack.imgur.com/IJ0lx.png
  [2]: http://i.stack.imgur.com/vFWer.png",2
5394370,03/22/2011 16:15:30,391399,07/14/2010 09:37:37,176,6,R: Data structure for a ontology and web extraction,"I want to extract information from a large website and generate an ontology. Something that can be processed with description logic.

What data structure is advisable for the extracted html data?

My ideas yet:  
- Use Data Frames, Table Structures  
- Sets and Relations (sets package and good relations)  
- Graphs


 . 
  
In the End I want to export the data and plan to process it with predicate logic (or description logic) using another programming language.

I want to use R to extraction information from html pages. But as I understand there is no direct support in R (or packages) for predicate logic or RDF/OWL.

So I need to do the extraction, use some data structure in the process and export the data.",r,data-structures,semantic-web,ontology,information-extraction,,open,0,124,9,"R: Data structure for a ontology and web extraction I want to extract information from a large website and generate an ontology. Something that can be processed with description logic.

What data structure is advisable for the extracted html data?

My ideas yet:  
- Use Data Frames, Table Structures  
- Sets and Relations (sets package and good relations)  
- Graphs


 . 
  
In the End I want to export the data and plan to process it with predicate logic (or description logic) using another programming language.

I want to use R to extraction information from html pages. But as I understand there is no direct support in R (or packages) for predicate logic or RDF/OWL.

So I need to do the extraction, use some data structure in the process and export the data.",5
9723348,03/15/2012 15:51:50,1175347,01/28/2012 15:13:08,18,0,What sort of hosting do I need? Using a CGI files with R?,"I have set up a script which is working on my localhost (mac osx machine).

It includes R files which are activated using an R.cgi file from the cgiwithR library.

So if I go to localhost/R.cgi/rscript.r it runs the script.

Now, I'm wondering if I wanted to get hosting to put this online, do I need a provider that specifically has R installed on their systems? Do these exist (if so can you recommend any)?

Might it just be better to try and write the script in a different language? (ruby, perl, php, python)

Thanks.",r,hosting,cgi,,,03/15/2012 18:03:54,off topic,1,90,13,"What sort of hosting do I need? Using a CGI files with R? I have set up a script which is working on my localhost (mac osx machine).

It includes R files which are activated using an R.cgi file from the cgiwithR library.

So if I go to localhost/R.cgi/rscript.r it runs the script.

Now, I'm wondering if I wanted to get hosting to put this online, do I need a provider that specifically has R installed on their systems? Do these exist (if so can you recommend any)?

Might it just be better to try and write the script in a different language? (ruby, perl, php, python)

Thanks.",3
8269009,11/25/2011 12:12:29,1065733,11/25/2011 13:43:57,1,0,delete columns that have binomial variaｂles,"data is a dataset which have variables both categorical and continuious.
I would delete categorical variables .

I wrote 
          
                data.1 <- data[,colnames(data)[[3L]]!=0]

No error is printed but categorical variables stay in data.1 .
where are problems ?
",r,dataset,,,,,open,0,59,6,"delete columns that have binomial variaｂles data is a dataset which have variables both categorical and continuious.
I would delete categorical variables .

I wrote 
          
                data.1 <- data[,colnames(data)[[3L]]!=0]

No error is printed but categorical variables stay in data.1 .
where are problems ?
",2
1249548,08/08/2009 18:16:43,84458,03/30/2009 05:51:22,342,8,Side-by-side plots with ggplot2 in R,"I would like to place two plots side by side using the [`ggplot2` package][1] (ie. do the equivalent of `par(mfrow=c(1,2))`).  For example, I would like to have the following two plots show side-by-side with the same scale.

    x <- rnorm(100)
    eps <- rnorm(100,0,.2)
    qplot(x,3*x+eps)
    qplot(x,2*x+eps)

Do I need to put them in the same data.frame like in this [example][2]?

    qplot(displ, hwy, data=mpg, facets = . ~ year) + geom_smooth()

Thanks!
  [1]: http://crantastic.org/packages/ggplot2
  [2]: http://had.co.nz/ggplot2/book/mastery.r",r,visualization,ggplot2,,,,open,0,90,6,"Side-by-side plots with ggplot2 in R I would like to place two plots side by side using the [`ggplot2` package][1] (ie. do the equivalent of `par(mfrow=c(1,2))`).  For example, I would like to have the following two plots show side-by-side with the same scale.

    x <- rnorm(100)
    eps <- rnorm(100,0,.2)
    qplot(x,3*x+eps)
    qplot(x,2*x+eps)

Do I need to put them in the same data.frame like in this [example][2]?

    qplot(displ, hwy, data=mpg, facets = . ~ year) + geom_smooth()

Thanks!
  [1]: http://crantastic.org/packages/ggplot2
  [2]: http://had.co.nz/ggplot2/book/mastery.r",3
5882706,05/04/2011 11:35:27,177390,09/22/2009 20:22:06,283,7,Predictive Modeling - Software Best Practice,"I would like to know what are the best practices for building Predictive Modeling solutions organically ?
Some of the questions I have are :-

 - If I have multiple R model files, what are efficient ways of storing them ?
    - Save as .Rdata files on file system
    - Serialize to a DB as binary objects
 - Since data is processed to create an interim model specific format, is it helpful to use such paradigms as PMML ?
 - Also, should one consider such practices as MVC (I'm not a trained software developer, so any insights into such development practices would be very helpful)

I apologize for the open-ended nature of this question. I wish to understand even simple things as recommended folder structure for data staging, model store, scripts collection and such other elements of a data mining solution.

I would be very grateful to members of the community for sharing their experiences and recommendations.
Thank you for your time.

 

",r,data-mining,,,,05/04/2011 12:23:23,not a real question,1,165,6,"Predictive Modeling - Software Best Practice I would like to know what are the best practices for building Predictive Modeling solutions organically ?
Some of the questions I have are :-

 - If I have multiple R model files, what are efficient ways of storing them ?
    - Save as .Rdata files on file system
    - Serialize to a DB as binary objects
 - Since data is processed to create an interim model specific format, is it helpful to use such paradigms as PMML ?
 - Also, should one consider such practices as MVC (I'm not a trained software developer, so any insights into such development practices would be very helpful)

I apologize for the open-ended nature of this question. I wish to understand even simple things as recommended folder structure for data staging, model store, scripts collection and such other elements of a data mining solution.

I would be very grateful to members of the community for sharing their experiences and recommendations.
Thank you for your time.

 

",2
10050611,04/07/2012 00:14:33,1187439,02/03/2012 12:03:11,19,0,Working with lots of data and lots of rasters in R?,"G'day, I am working with a large dataset with ~125,000 lon/lat locations with date, for species presence/absence records. For at each location I want to work out what the weather was like at each location on the date and during the 3mths prior to the date. To do this I have downloaded daily weather data for a given weather variable (e.g., max temperature) during the 5yr period the data was taken. I have a total of 1,826 raster files, all between 2-3mb.

I had planned to stack all raster files, then extract a value from every raster (1,826) for each point. This would produce a massive file I could use to search for the dates I need. This is, however, not possible because I can't stack that many rasters. I tried splitting the rasters into stacks of 500, this works, but the files it produces are about 1Gb and very slow (rows, 125,000; columns, 500). Also, when I try to bring all of these files into R to create a big data frame it doesn't work.

I would like to know if there is a way to work with this amount of data in R, or if there is a package that I could use to help. Could I use a package like ff? Does anyone have any suggestions for a less power intensive method to do what I want to do? I have thought about something like a lapply function, but have never used one before and am not really sure where to begin.

Any help would be really great, thanks in advance for your time. The code I am currently using without success is below.

Kind regards,
Adam

    library(raster)
    library(rgdal)
    library (maptools)
    library(shapefiles)
        
    # To create weather data files, first set the working directory to the appropriate location (i.e., maxt)
    # list of raster weather files
    files<- list.files(getwd(), pattern='asc')
    length(files)
    
    memory.size(4000)  
    memory.limit(4000)
   
    # read in lon/lat data
    X<-read.table(file.choose(), header=TRUE, sep=',')
    SP<- SpatialPoints(cbind(X$lon, X$lat)) 
    
    #separate stacks into mannageable sizes
    s1<- stack(files[1:500])
    i1 <- extract( s1,SP, cellnumbers = True, layer = 1, nl = 500)
    write.table(i1, file=""maxt_vals_all_points_all_dates_1.csv"", sep="","", row.names= FALSE, col.names= TRUE)
    rm(s1,i1)
    s2<- stack(files[501:1000])
    i2 <- extract( s2,SP, cellnumbers = True, layer = 1, nl = 500)
    write.table(i2, file=""maxt_vals_all_points_all_dates_2.csv"", sep="","", row.names= FALSE, col.names= TRUE)
    rm(s2,i2)
    s3<- stack(files[1001:1500])
    i3 <- extract( s3,SP, cellnumbers = True, layer = 1, nl = 500)
    write.table(i3, file=""maxt_vals_all_points_all_dates_3.csv"", sep="","", row.names= FALSE, col.names= TRUE)
    rm(s3,i3)
    s4<- stack(files[1501:1826])
    i4 <- extract( s4,SP, cellnumbers = True, layer = 1, nl =325)
    write.table(i4, file=""maxt_vals_all_points_all_dates_4.csv"", sep="","", row.names= FALSE, col.names= TRUE)
    rm(s4,i4)
    
    # read files back in to bind into final file !!! NOT WORKING FILES ARE TOO BIG!!
    i1<-read.table(file.choose(),header=TRUE,sep=',')
    i2<-read.table(file.choose(),header=TRUE,sep=',')
    i3<-read.table(file.choose(),header=TRUE,sep=',')
    i4<-read.table(file.choose(),header=TRUE,sep=',')
    
    vals<-data.frame(X, i1, i2, i3 ,i4)
    write.table(vals, file=""maxt_master_lookup.csv"", sep="","", row.names= FALSE, col.names= TRUE)

   ",r,memory-management,large-files,spatial,raster,,open,0,593,11,"Working with lots of data and lots of rasters in R? G'day, I am working with a large dataset with ~125,000 lon/lat locations with date, for species presence/absence records. For at each location I want to work out what the weather was like at each location on the date and during the 3mths prior to the date. To do this I have downloaded daily weather data for a given weather variable (e.g., max temperature) during the 5yr period the data was taken. I have a total of 1,826 raster files, all between 2-3mb.

I had planned to stack all raster files, then extract a value from every raster (1,826) for each point. This would produce a massive file I could use to search for the dates I need. This is, however, not possible because I can't stack that many rasters. I tried splitting the rasters into stacks of 500, this works, but the files it produces are about 1Gb and very slow (rows, 125,000; columns, 500). Also, when I try to bring all of these files into R to create a big data frame it doesn't work.

I would like to know if there is a way to work with this amount of data in R, or if there is a package that I could use to help. Could I use a package like ff? Does anyone have any suggestions for a less power intensive method to do what I want to do? I have thought about something like a lapply function, but have never used one before and am not really sure where to begin.

Any help would be really great, thanks in advance for your time. The code I am currently using without success is below.

Kind regards,
Adam

    library(raster)
    library(rgdal)
    library (maptools)
    library(shapefiles)
        
    # To create weather data files, first set the working directory to the appropriate location (i.e., maxt)
    # list of raster weather files
    files<- list.files(getwd(), pattern='asc')
    length(files)
    
    memory.size(4000)  
    memory.limit(4000)
   
    # read in lon/lat data
    X<-read.table(file.choose(), header=TRUE, sep=',')
    SP<- SpatialPoints(cbind(X$lon, X$lat)) 
    
    #separate stacks into mannageable sizes
    s1<- stack(files[1:500])
    i1 <- extract( s1,SP, cellnumbers = True, layer = 1, nl = 500)
    write.table(i1, file=""maxt_vals_all_points_all_dates_1.csv"", sep="","", row.names= FALSE, col.names= TRUE)
    rm(s1,i1)
    s2<- stack(files[501:1000])
    i2 <- extract( s2,SP, cellnumbers = True, layer = 1, nl = 500)
    write.table(i2, file=""maxt_vals_all_points_all_dates_2.csv"", sep="","", row.names= FALSE, col.names= TRUE)
    rm(s2,i2)
    s3<- stack(files[1001:1500])
    i3 <- extract( s3,SP, cellnumbers = True, layer = 1, nl = 500)
    write.table(i3, file=""maxt_vals_all_points_all_dates_3.csv"", sep="","", row.names= FALSE, col.names= TRUE)
    rm(s3,i3)
    s4<- stack(files[1501:1826])
    i4 <- extract( s4,SP, cellnumbers = True, layer = 1, nl =325)
    write.table(i4, file=""maxt_vals_all_points_all_dates_4.csv"", sep="","", row.names= FALSE, col.names= TRUE)
    rm(s4,i4)
    
    # read files back in to bind into final file !!! NOT WORKING FILES ARE TOO BIG!!
    i1<-read.table(file.choose(),header=TRUE,sep=',')
    i2<-read.table(file.choose(),header=TRUE,sep=',')
    i3<-read.table(file.choose(),header=TRUE,sep=',')
    i4<-read.table(file.choose(),header=TRUE,sep=',')
    
    vals<-data.frame(X, i1, i2, i3 ,i4)
    write.table(vals, file=""maxt_master_lookup.csv"", sep="","", row.names= FALSE, col.names= TRUE)

   ",5
11021136,06/13/2012 18:30:54,492372,10/30/2010 20:02:03,455,3,Combining plots in R,"I saw that even by writing separate commands, we can combine two different types of plots in R. For example, if a histogram has been created in R where the histogram plots points representing the frequency of the items, we can connect these dots by calling a line() function. Now, these two plots are separate commands. 

How does this work in R? When can it combine plots and what is actually supported here?

Thanks
Abhishek S",r,statistics,,,,06/13/2012 21:19:38,not a real question,1,74,4,"Combining plots in R I saw that even by writing separate commands, we can combine two different types of plots in R. For example, if a histogram has been created in R where the histogram plots points representing the frequency of the items, we can connect these dots by calling a line() function. Now, these two plots are separate commands. 

How does this work in R? When can it combine plots and what is actually supported here?

Thanks
Abhishek S",2
8555823,12/18/2011 23:55:34,1000026,10/17/2011 21:38:26,36,0,"comparing two integers in R: ""longer object length not multiple of shorter object length"" ddply","I'm getting an ""longer object length not multiple of shorter object length"" warning in R when comparing two integers to subset a dataframe in the midst of a udf.

The udf just returns the median of a subset of integers taken from a dataframe:

    function(s){return(median((subset(EDB,as.integer(validSession)==as.integer(s)))$absStudentDeviation))}

(I did not originally have the ""as.integer"" coercions in there. I put them there to debug, text, and I'm still getting an error.)

The specific error I'm getting is:

    In as.integer(validSession) == as.integer(s) :
  longer object length is not a multiple of shorter object length

I get this warning over 50 times when calling:

    mediandf<-ddply(mediandf,.(validSession),transform,grossMed2=medianfuncEDB(as.integer(validSession)))

The goal is to calculate the median of $validSession associated with the given validSession in the large dataframe EDB and attach that vector to mediandf.

I have actually double-checked that all values for validSession in both the mediandf dataframe and the EDB dataframe are integers by subsetting with is.integer(validSession).

Furthermore, it appears that the command actually does what I intend, I get a new column in my dataframe with values I have not verified, but I want to understand the warning. if ""medianfuncEDB"" is being called with an integer as its input, why am I getting a ""longer object length is not multiple of shorter object length"" when s==validSession is called?

Note that simple function calls, like medianfuncEDB(5) work without any problems, so why do I get warnings when using ddply?",r,data.frame,plyr,ddply,,,open,0,233,15,"comparing two integers in R: ""longer object length not multiple of shorter object length"" ddply I'm getting an ""longer object length not multiple of shorter object length"" warning in R when comparing two integers to subset a dataframe in the midst of a udf.

The udf just returns the median of a subset of integers taken from a dataframe:

    function(s){return(median((subset(EDB,as.integer(validSession)==as.integer(s)))$absStudentDeviation))}

(I did not originally have the ""as.integer"" coercions in there. I put them there to debug, text, and I'm still getting an error.)

The specific error I'm getting is:

    In as.integer(validSession) == as.integer(s) :
  longer object length is not a multiple of shorter object length

I get this warning over 50 times when calling:

    mediandf<-ddply(mediandf,.(validSession),transform,grossMed2=medianfuncEDB(as.integer(validSession)))

The goal is to calculate the median of $validSession associated with the given validSession in the large dataframe EDB and attach that vector to mediandf.

I have actually double-checked that all values for validSession in both the mediandf dataframe and the EDB dataframe are integers by subsetting with is.integer(validSession).

Furthermore, it appears that the command actually does what I intend, I get a new column in my dataframe with values I have not verified, but I want to understand the warning. if ""medianfuncEDB"" is being called with an integer as its input, why am I getting a ""longer object length is not multiple of shorter object length"" when s==validSession is called?

Note that simple function calls, like medianfuncEDB(5) work without any problems, so why do I get warnings when using ddply?",4
9739195,03/16/2012 14:33:58,1274212,03/16/2012 14:17:09,1,0,NA in randomForest,"I have a question regarding NA in randomForest (in R). I have a dataset which include both numerical and non-numerical variables, and the data includes some NA. Do anyone have some tips how to deal with this? I tried to use na.roughfix but then i get an error message ""na.roughfix only works for numeric or factor"". I also tried rfImpute but this does not work either because I have some NA in my response variable. Does anyone have som tips?
",r,regression,random-forest,,,04/07/2012 16:20:10,too localized,1,80,3,"NA in randomForest I have a question regarding NA in randomForest (in R). I have a dataset which include both numerical and non-numerical variables, and the data includes some NA. Do anyone have some tips how to deal with this? I tried to use na.roughfix but then i get an error message ""na.roughfix only works for numeric or factor"". I also tried rfImpute but this does not work either because I have some NA in my response variable. Does anyone have som tips?
",3
8697796,01/02/2012 05:48:03,914308,08/26/2011 14:38:22,860,10,R Cran can't connect to package pages,"I'm having a problem accessing R information.  Normally when I google something like:

    r randomforest

I'll get results like these:

    CRAN - Package randomForest

    [PDF] Package 'randomForest'

that link to package pages or pdf user manuals.  For the last week or so I get the following error whenever I try to search for package manuals or information and click on the returned links:

    Object not found!
    
    The requested URL was not found on this server. The link on the referring page seems to be wrong or outdated. Please inform the author of that page about the error.
    
    If you think this is a server error, please contact the webmaster.
    Error 404
    cran.r-project.org
    Mon Jan 2 06:44:23 2012
    Apache/2.2.21 (Debian) 

I'm not sure what's wrong.  I'm using R 2.14 on a windows 7 machine.  I use firefox but did recently install IE 9 but don't use it.  Any suggestions?
",r,,,,,01/05/2012 04:35:03,too localized,1,189,7,"R Cran can't connect to package pages I'm having a problem accessing R information.  Normally when I google something like:

    r randomforest

I'll get results like these:

    CRAN - Package randomForest

    [PDF] Package 'randomForest'

that link to package pages or pdf user manuals.  For the last week or so I get the following error whenever I try to search for package manuals or information and click on the returned links:

    Object not found!
    
    The requested URL was not found on this server. The link on the referring page seems to be wrong or outdated. Please inform the author of that page about the error.
    
    If you think this is a server error, please contact the webmaster.
    Error 404
    cran.r-project.org
    Mon Jan 2 06:44:23 2012
    Apache/2.2.21 (Debian) 

I'm not sure what's wrong.  I'm using R 2.14 on a windows 7 machine.  I use firefox but did recently install IE 9 but don't use it.  Any suggestions?
",1
8255841,11/24/2011 10:51:17,917551,08/29/2011 09:43:47,1,0,cubic spline interpolation in R,"I am trying an implementation of a cubic spline in R. I have already used the spline, smooth.spline and smooth.Pspline functions that are available in the R libraries but I am not that happy with the results so I want to convince myself about the consistency of the results by a ""homemade"" spline function. I have already computed the coefficients for the 3rd degree polynomials, but I am not sure how to plot the results..they seem random points. You can find the source code below. Any help would be appreciated.

    x = c(35,36,39,42,45,48)
    y = c(2.87671519825595, 4.04868309245341,	3.95202175000174,	
      3.87683188946186,	4.07739945984612,	2.16064840967985)


    n = length(x)

    #determine width of intervals
    for (i in 1:(n-1)){
       h[i] = (x[i+1] - x[i])
    }

    A = 0
    B = 0
    C = 0
    D = 0
    #determine the matrix influence coefficients for the natural spline
    for (i in 2:(n-1)){
      j = i-1
      D[j] = 2*(h[i-1] + h[i])
      A[j] = h[i]
      B[j] = h[i-1] 
  
    }

    #determine the constant matrix C
    for (i in 2:(n-1)){
      j = i-1
      C[j] = 6*((y[i+1] - y[i]) / h[i] - (y[i] - y[i-1]) / h[i-1])
    }

    #maximum TDMA length
    ntdma = n - 2

    #tridiagonal matrix algorithm

    #upper triangularization
    R = 0
    for (i in 2:ntdma){
      R = B[i]/D[i-1]
      D[i] = D[i] - R * A[i-1]
      C[i] = C[i] - R * C[i-1] 
    }

    #set the last C
    C[ntdma] = C[ntdma] / D[ntdma]

    #back substitute
    for (i in (ntdma-1):1){
      C[i] = (C[i] - A[i] * C[i+1]) / D[i]
    }

    #end of tdma

    #switch from C to S
    S = 0
    for (i in 2:(n-1)){
      j = i - 1
      S[i] = C[j]
    }
    #end conditions
    S[1] <- 0 -> S[n]

    #Calculate cubic ai,bi,ci and di from S and h
    for (i in 1:(n-1)){
     A[i] = (S[i+ 1] - S[i]) / (6 * h[i])
     B[i] = S[i] / 2
     C[i] = (y[i+1] - y[i]) / h[i] - (2 * h[i] * S[i] + h[i] * S[i + 1]) / 6
     D[i] = y[i]
    }


    #control points
    xx = c(x[2],x[4])

    #spline evaluation
    for (j in 1:length(xx)){
      for (i in 1:n){
        if (xx[j]<=x[i]){
          break
        }
        yy[i] = A[i]*(xx[j] - x[i])^3 + B[i]*(xx[j] - x[i])^2 + C[i]*(xx[j] - x[i]) + D[i]
    
     }
    points(x,yy ,col=""blue"")
    }

Thank you",r,plot,spline,cubic,,11/24/2011 15:18:11,too localized,1,633,5,"cubic spline interpolation in R I am trying an implementation of a cubic spline in R. I have already used the spline, smooth.spline and smooth.Pspline functions that are available in the R libraries but I am not that happy with the results so I want to convince myself about the consistency of the results by a ""homemade"" spline function. I have already computed the coefficients for the 3rd degree polynomials, but I am not sure how to plot the results..they seem random points. You can find the source code below. Any help would be appreciated.

    x = c(35,36,39,42,45,48)
    y = c(2.87671519825595, 4.04868309245341,	3.95202175000174,	
      3.87683188946186,	4.07739945984612,	2.16064840967985)


    n = length(x)

    #determine width of intervals
    for (i in 1:(n-1)){
       h[i] = (x[i+1] - x[i])
    }

    A = 0
    B = 0
    C = 0
    D = 0
    #determine the matrix influence coefficients for the natural spline
    for (i in 2:(n-1)){
      j = i-1
      D[j] = 2*(h[i-1] + h[i])
      A[j] = h[i]
      B[j] = h[i-1] 
  
    }

    #determine the constant matrix C
    for (i in 2:(n-1)){
      j = i-1
      C[j] = 6*((y[i+1] - y[i]) / h[i] - (y[i] - y[i-1]) / h[i-1])
    }

    #maximum TDMA length
    ntdma = n - 2

    #tridiagonal matrix algorithm

    #upper triangularization
    R = 0
    for (i in 2:ntdma){
      R = B[i]/D[i-1]
      D[i] = D[i] - R * A[i-1]
      C[i] = C[i] - R * C[i-1] 
    }

    #set the last C
    C[ntdma] = C[ntdma] / D[ntdma]

    #back substitute
    for (i in (ntdma-1):1){
      C[i] = (C[i] - A[i] * C[i+1]) / D[i]
    }

    #end of tdma

    #switch from C to S
    S = 0
    for (i in 2:(n-1)){
      j = i - 1
      S[i] = C[j]
    }
    #end conditions
    S[1] <- 0 -> S[n]

    #Calculate cubic ai,bi,ci and di from S and h
    for (i in 1:(n-1)){
     A[i] = (S[i+ 1] - S[i]) / (6 * h[i])
     B[i] = S[i] / 2
     C[i] = (y[i+1] - y[i]) / h[i] - (2 * h[i] * S[i] + h[i] * S[i + 1]) / 6
     D[i] = y[i]
    }


    #control points
    xx = c(x[2],x[4])

    #spline evaluation
    for (j in 1:length(xx)){
      for (i in 1:n){
        if (xx[j]<=x[i]){
          break
        }
        yy[i] = A[i]*(xx[j] - x[i])^3 + B[i]*(xx[j] - x[i])^2 + C[i]*(xx[j] - x[i]) + D[i]
    
     }
    points(x,yy ,col=""blue"")
    }

Thank you",4
6061305,05/19/2011 15:45:14,761386,05/19/2011 15:45:14,1,0,Using R for Covariate Adjusted Logistic Regression,"I would like to run a 'Covariable Adjusted Logistic Regression' test, and I would like to do this in R. So I am having some difficulties with this.

I am uncertain if I am correct in my understanding of Covariable Adjusted Logistic Regression. I have to translate code from a software suite and I've been having a very hard finding articles to help me understand what it exactly is. This type of thing is commonly done in clinical trials but I am not a biostatitician and have been having trouble contextualizing what I am reading. 

Does anyone know if there is an R package out there that could help me with this? I can understand reading code much better than those clinical trial papers. The software suite code is unavailable to me, but I do have final values to compare against.

It would be very helpful. I tried the R forum, but was told that I should take more statistics courses instead of trying to program... so not really useful advice.",r,,,,,05/19/2011 16:27:29,off topic,1,169,7,"Using R for Covariate Adjusted Logistic Regression I would like to run a 'Covariable Adjusted Logistic Regression' test, and I would like to do this in R. So I am having some difficulties with this.

I am uncertain if I am correct in my understanding of Covariable Adjusted Logistic Regression. I have to translate code from a software suite and I've been having a very hard finding articles to help me understand what it exactly is. This type of thing is commonly done in clinical trials but I am not a biostatitician and have been having trouble contextualizing what I am reading. 

Does anyone know if there is an R package out there that could help me with this? I can understand reading code much better than those clinical trial papers. The software suite code is unavailable to me, but I do have final values to compare against.

It would be very helpful. I tried the R forum, but was told that I should take more statistics courses instead of trying to program... so not really useful advice.",1
11004260,06/12/2012 20:22:43,1445183,06/08/2012 18:19:26,13,0,Mean hour-of-day and imputation...would this be easier with time calculations?,"I'm working with a data set and am imputing NAs for times. I have a simplified example below where I am creating a new column that includes the original data and imputed values for NAs (i.e., the mean of the time of day). The code works fine, but I am so weak with dates I was wondering if there was an easier way to calculate the mean time of day date/time values?

Thanks in advance,
--JT

        arrivals <- data.frame(ships = c(""Glory"", ""Discover"", ""Intrepid"", ""Enchantment"", 
                                ""Summit""), times = c(""8:00"", ""10:00"", ""11:42"",NA,""9:20""),                        
                                stringsAsFactors = FALSE)
        sumtime <- sapply(strsplit(as.character(arrivals$times),"":""),
                  function(x){as.numeric(x[1])*60 + as.numeric(x[2])})
        avgtime <- paste(trunc((mean(sumtime, na.rm=TRUE)/60)),"":"",
                          trunc(mean(sumtime, na.rm=TRUE)%%60), sep="""")
        arrivals$times2 <- arrivals$times
        arrivals$times2[is.na(arrivals$times)] <- avgtime",r,date,,,,,open,0,274,10,"Mean hour-of-day and imputation...would this be easier with time calculations? I'm working with a data set and am imputing NAs for times. I have a simplified example below where I am creating a new column that includes the original data and imputed values for NAs (i.e., the mean of the time of day). The code works fine, but I am so weak with dates I was wondering if there was an easier way to calculate the mean time of day date/time values?

Thanks in advance,
--JT

        arrivals <- data.frame(ships = c(""Glory"", ""Discover"", ""Intrepid"", ""Enchantment"", 
                                ""Summit""), times = c(""8:00"", ""10:00"", ""11:42"",NA,""9:20""),                        
                                stringsAsFactors = FALSE)
        sumtime <- sapply(strsplit(as.character(arrivals$times),"":""),
                  function(x){as.numeric(x[1])*60 + as.numeric(x[2])})
        avgtime <- paste(trunc((mean(sumtime, na.rm=TRUE)/60)),"":"",
                          trunc(mean(sumtime, na.rm=TRUE)%%60), sep="""")
        arrivals$times2 <- arrivals$times
        arrivals$times2[is.na(arrivals$times)] <- avgtime",2
8046428,11/08/2011 05:43:54,1034993,11/08/2011 05:13:55,1,0,R - randomly choosing vectors in a matrix?,"I'll preface this by saying I'm an R noob and that I think this may have an easy solution, but I'm struggling to find it. 

I've got a matrix with 2 columns and 1,000 rows. Keeping the rows fixed, I'd like to create a new variable that randomly chooses one of the elements from the 2 columns. For example making a simple matrix:

            matrix(c(1,1,4,6,1,3,2,1,1,7), ncol=2)

            [,1] [,2] [,3]
      [1,]    1    3    3
      [2,]    1    2    1  
      [3,]    4    1    4
      [4,]    6    1    1
      [5,]    1    7    7
      
In the simplified matrix above, the 3rd column (which I just added by hand) just contains a random element from either of the prior columns in the corresponding row. My question is, how would I create such a variable in R? I don't necessarily need it to be created within the matrix itself either. 

Many thanks in advance.
",r,random,,,,,open,0,245,8,"R - randomly choosing vectors in a matrix? I'll preface this by saying I'm an R noob and that I think this may have an easy solution, but I'm struggling to find it. 

I've got a matrix with 2 columns and 1,000 rows. Keeping the rows fixed, I'd like to create a new variable that randomly chooses one of the elements from the 2 columns. For example making a simple matrix:

            matrix(c(1,1,4,6,1,3,2,1,1,7), ncol=2)

            [,1] [,2] [,3]
      [1,]    1    3    3
      [2,]    1    2    1  
      [3,]    4    1    4
      [4,]    6    1    1
      [5,]    1    7    7
      
In the simplified matrix above, the 3rd column (which I just added by hand) just contains a random element from either of the prior columns in the corresponding row. My question is, how would I create such a variable in R? I don't necessarily need it to be created within the matrix itself either. 

Many thanks in advance.
",2
4750701,01/20/2011 17:55:37,429846,08/24/2010 17:25:03,6013,161,How does one use `Recall()` to write a recursive function to recursively list directories in a given directory?,"This [Question][1] asked about listing directories (not files) in the current directory. I noted in a comment to one of the answers that we can't use the `recursive` argument of functions like `dir` and `list.files` to recursively list directories in the current directory.

The obvious solution to this is to write a recursive function that lists the directories in the current directory that calls itself on each of those directories in turn, and so on, adding to the overall list of directories that gets returned at the end of the recursion.

The `Recall()` function would seem the ideal candidate for this, but I've never really got my head round how one writes a recursive function that adds to the final output each time it is called.

How would one modify this function:

    list.dirs <- function(path) {
        x <- dir(path, full.names = TRUE)
        dnames <- x[file_test(""-d"", x)]
        dnames
    }

To have it descend recursively through the directories in `dnames` adding any directories it finds to a list of all directories found within the `dnames` directories, and so on...?

  [1]: http://stackoverflow.com/questions/4749783/how-to-obtain-a-list-of-directories-within-a-directory-like-list-files-but-in",r,recursion,,,,,open,0,203,18,"How does one use `Recall()` to write a recursive function to recursively list directories in a given directory? This [Question][1] asked about listing directories (not files) in the current directory. I noted in a comment to one of the answers that we can't use the `recursive` argument of functions like `dir` and `list.files` to recursively list directories in the current directory.

The obvious solution to this is to write a recursive function that lists the directories in the current directory that calls itself on each of those directories in turn, and so on, adding to the overall list of directories that gets returned at the end of the recursion.

The `Recall()` function would seem the ideal candidate for this, but I've never really got my head round how one writes a recursive function that adds to the final output each time it is called.

How would one modify this function:

    list.dirs <- function(path) {
        x <- dir(path, full.names = TRUE)
        dnames <- x[file_test(""-d"", x)]
        dnames
    }

To have it descend recursively through the directories in `dnames` adding any directories it finds to a list of all directories found within the `dnames` directories, and so on...?

  [1]: http://stackoverflow.com/questions/4749783/how-to-obtain-a-list-of-directories-within-a-directory-like-list-files-but-in",2
6334054,06/13/2011 17:41:18,631237,02/23/2011 22:29:23,20,1,Reshape data from long to semi-wide in R,"I have data in which each participant made 3 judgments on each of 9 objects (27 judgments).  The 9 objects varied in a 3x3 design (within subjects) so there are 2 factors.

I'm starting with ID + 27 data columns, and I need to have

 - ID
 - 2 factor columns: Performance, Situation
 - 3 value columns: Success, ProbAdmit, Admit

I have read the manuals on reshape() and melt() and cast() but haven't yet been able to figure out what I need to do to make it happen.  Here is my current progress from which you can see my actual data.

    scsc3 <- read.csv(""http://swift.cbdr.cmu.edu/data/SCSC3-2006-10-10.csv"")
    library(reshape)
    scsc3.long <- melt(scsc3,id=""Participant"")
    scsc3.long <- cbind(scsc3.long,colsplit(scsc3.long$variable,split=""[.]"",names=c(""Item"",""Candidate"",""Performance"",""Situation"")))
    scsc3.long$variable <- NULL
    scsc3.long$Candidate <- NULL

The above code leaves me with this:

    Participant  value  Item      Performance  Situation
    4001         5.0    Success   GL           IL
    4001         60     ProbAdmit GL           IL
    4001         1      Admit     GL           IL
    4002         ....

What I need is a dataframe like this

    Participant Performance  Situation SuccessValue ProbAdmitValue AdmitValue
    4001        GL           IL        5.0          60             1
    ...

Thanks!",r,,,,,,open,0,341,8,"Reshape data from long to semi-wide in R I have data in which each participant made 3 judgments on each of 9 objects (27 judgments).  The 9 objects varied in a 3x3 design (within subjects) so there are 2 factors.

I'm starting with ID + 27 data columns, and I need to have

 - ID
 - 2 factor columns: Performance, Situation
 - 3 value columns: Success, ProbAdmit, Admit

I have read the manuals on reshape() and melt() and cast() but haven't yet been able to figure out what I need to do to make it happen.  Here is my current progress from which you can see my actual data.

    scsc3 <- read.csv(""http://swift.cbdr.cmu.edu/data/SCSC3-2006-10-10.csv"")
    library(reshape)
    scsc3.long <- melt(scsc3,id=""Participant"")
    scsc3.long <- cbind(scsc3.long,colsplit(scsc3.long$variable,split=""[.]"",names=c(""Item"",""Candidate"",""Performance"",""Situation"")))
    scsc3.long$variable <- NULL
    scsc3.long$Candidate <- NULL

The above code leaves me with this:

    Participant  value  Item      Performance  Situation
    4001         5.0    Success   GL           IL
    4001         60     ProbAdmit GL           IL
    4001         1      Admit     GL           IL
    4002         ....

What I need is a dataframe like this

    Participant Performance  Situation SuccessValue ProbAdmitValue AdmitValue
    4001        GL           IL        5.0          60             1
    ...

Thanks!",1
11105728,06/19/2012 16:57:17,1467068,06/19/2012 16:47:34,1,0,Pivoting a DataFrame in Pandas for output to CSV,"This is a simple question for which answer are surprisingly difficult to find online. Here's the situation:

    >>> A
    [('hey', 'you', 4), ('hey', 'not you', 5), ('not hey', 'you', 2), ('not hey', 'not you', 6)]
    >>> A_p = pandas.DataFrame(A)
    >>> A_p
             0        1  2
    0      hey      you  4
    1      hey  not you  5
    2  not hey      you  2
    3  not hey  not you  6
    >>> B_p = A_p.pivot(0,1,2)
    >>> B_p
    1        not you  you
    0                    
    hey            5    4
    not hey        6    2

This isn't quite what's suggested in the documentation for pivot -- there, it shows results without the 1 and 0 in the upper-left-hand corner. And that's what I'm looking for, a DataFrame object that prints as

             not you  you
    hey            5    4
    not hey        6    2

The problem is that the normal behavior results in a csv file whose first line is

    0,not you,you

when I really want

    not you, you

When the normal csv file (with the preceding ""0,"") reads into R, it doesn't properly set the column and row names from the frame object, resulting in painful manual manipulation to get it in the right format. Is there a way to get pivot to give me a DataFrame object without that additional information in the upper-left corner?",r,data.frame,pandas,,,,open,0,398,9,"Pivoting a DataFrame in Pandas for output to CSV This is a simple question for which answer are surprisingly difficult to find online. Here's the situation:

    >>> A
    [('hey', 'you', 4), ('hey', 'not you', 5), ('not hey', 'you', 2), ('not hey', 'not you', 6)]
    >>> A_p = pandas.DataFrame(A)
    >>> A_p
             0        1  2
    0      hey      you  4
    1      hey  not you  5
    2  not hey      you  2
    3  not hey  not you  6
    >>> B_p = A_p.pivot(0,1,2)
    >>> B_p
    1        not you  you
    0                    
    hey            5    4
    not hey        6    2

This isn't quite what's suggested in the documentation for pivot -- there, it shows results without the 1 and 0 in the upper-left-hand corner. And that's what I'm looking for, a DataFrame object that prints as

             not you  you
    hey            5    4
    not hey        6    2

The problem is that the normal behavior results in a csv file whose first line is

    0,not you,you

when I really want

    not you, you

When the normal csv file (with the preceding ""0,"") reads into R, it doesn't properly set the column and row names from the frame object, resulting in painful manual manipulation to get it in the right format. Is there a way to get pivot to give me a DataFrame object without that additional information in the upper-left corner?",3
9813320,03/21/2012 21:43:34,1261288,03/10/2012 17:10:34,1,0,Package ‘GeneR’ is not available,"I'm trying to install GeneR library (http://www.bioconductor.org/packages/release/bioc/html/GeneR.html):
I'm using win7 and the newest R 2.14.2.

Error during installation:

    > source(""http://bioconductor.org/biocLite.R"")
    trying URL 'http://www.bioconductor.org/packages/2.9/bioc/bin/windows/contrib/2.14/BiocInstaller_1.2.1.zip'
    Content type 'application/zip' length 32947 bytes (32 Kb)
    opened URL
    downloaded 32 Kb
    
    package ‘BiocInstaller’ successfully unpacked and MD5 sums checked
    
    The downloaded packages are in
            C:\Users\Bluev\AppData\Local\Temp\RtmpQJYYaS\downloaded_packages
    BiocInstaller version 1.2.1, ?biocLite for help
    > biocLite(""GeneR"")
    BioC_mirror: 'http://www.bioconductor.org'
    Using R version 2.14, BiocInstaller version 1.2.1.
    Installing package(s) 'GeneR'
    Old packages: 'lattice', 'Matrix', 'rpart'
    Update all/some/none? [a/s/n]: a
    trying URL 'http://cran.fhcrc.org/bin/windows/contrib/2.14/lattice_0.20-6.zip'
    Content type 'application/zip' length 704272 bytes (687 Kb)
    opened URL
    downloaded 687 Kb
    
    trying URL 'http://cran.fhcrc.org/bin/windows/contrib/2.14/Matrix_1.0-5.zip'
    Content type 'application/zip' length 3278758 bytes (3.1 Mb)
    opened URL
    downloaded 3.1 Mb
    
    trying URL 'http://cran.fhcrc.org/bin/windows/contrib/2.14/rpart_3.1-52.zip'
    Content type 'application/zip' length 200827 bytes (196 Kb)
    opened URL
    downloaded 196 Kb
    
    package ‘lattice’ successfully unpacked and MD5 sums checked
    package ‘Matrix’ successfully unpacked and MD5 sums checked
    package ‘rpart’ successfully unpacked and MD5 sums checked
    
    The downloaded packages are in
            C:\Users\Bluev\AppData\Local\Temp\RtmpQJYYaS\downloaded_packages
    Warning message:
    In getDependencies(pkgs, dependencies, available, lib) :
      package ‘GeneR’ is not available (for R version 2.14.2)

How to install this library? ",r,bioconductor,genetics,,,,open,0,324,5,"Package ‘GeneR’ is not available I'm trying to install GeneR library (http://www.bioconductor.org/packages/release/bioc/html/GeneR.html):
I'm using win7 and the newest R 2.14.2.

Error during installation:

    > source(""http://bioconductor.org/biocLite.R"")
    trying URL 'http://www.bioconductor.org/packages/2.9/bioc/bin/windows/contrib/2.14/BiocInstaller_1.2.1.zip'
    Content type 'application/zip' length 32947 bytes (32 Kb)
    opened URL
    downloaded 32 Kb
    
    package ‘BiocInstaller’ successfully unpacked and MD5 sums checked
    
    The downloaded packages are in
            C:\Users\Bluev\AppData\Local\Temp\RtmpQJYYaS\downloaded_packages
    BiocInstaller version 1.2.1, ?biocLite for help
    > biocLite(""GeneR"")
    BioC_mirror: 'http://www.bioconductor.org'
    Using R version 2.14, BiocInstaller version 1.2.1.
    Installing package(s) 'GeneR'
    Old packages: 'lattice', 'Matrix', 'rpart'
    Update all/some/none? [a/s/n]: a
    trying URL 'http://cran.fhcrc.org/bin/windows/contrib/2.14/lattice_0.20-6.zip'
    Content type 'application/zip' length 704272 bytes (687 Kb)
    opened URL
    downloaded 687 Kb
    
    trying URL 'http://cran.fhcrc.org/bin/windows/contrib/2.14/Matrix_1.0-5.zip'
    Content type 'application/zip' length 3278758 bytes (3.1 Mb)
    opened URL
    downloaded 3.1 Mb
    
    trying URL 'http://cran.fhcrc.org/bin/windows/contrib/2.14/rpart_3.1-52.zip'
    Content type 'application/zip' length 200827 bytes (196 Kb)
    opened URL
    downloaded 196 Kb
    
    package ‘lattice’ successfully unpacked and MD5 sums checked
    package ‘Matrix’ successfully unpacked and MD5 sums checked
    package ‘rpart’ successfully unpacked and MD5 sums checked
    
    The downloaded packages are in
            C:\Users\Bluev\AppData\Local\Temp\RtmpQJYYaS\downloaded_packages
    Warning message:
    In getDependencies(pkgs, dependencies, available, lib) :
      package ‘GeneR’ is not available (for R version 2.14.2)

How to install this library? ",3
9551555,03/04/2012 01:38:24,120898,06/10/2009 21:21:57,1187,13,Combine a series of data frames and create new columns for data in each,"I have an Excel file with a sheet for each week in my data set. Each sheet has the same number of rows, and each row is identical across the sheets (with the exception of the time period… sheet 1 represents week 1, sheet 2 week 2, etc.). I'm trying to import all the Excel sheets as one data frame in R.

For example, my data is essentially structured like this (with several more columns and sheets):

	Week 1 sheet
	ID    Gender    DOB    Absences    Lates
	1     M         1997   5           14
	2     F         1998   4           3

	Week 2 sheet
	ID    Gender    DOB    Absences    Lates
	1     M         1997   2           10
	2     F         1998   8           2

I'm trying to build a script that will take x numbers of sheets and combine them into one data frame like this:

	Combined (ideal)
	ID    Gender    DOB    Absences.1    Lates.1    Absences.2    Lates.2
	1     M         1997   5             14         2             10
	2     F         1998   4             3          8             2

I'm using gdata to import the Excel files. 

I've tried creating a loop (normally bad for R, I know...) that will go through all the sheets in the Excel file and add each to a list as a data frame:

    library(gdata)

    number_sheets <- 3
    all.sheets <- vector(mode=""list"", length=number_sheets)

    for (i in 1:number_sheets) {
      all.sheets[[i]] <- read.xls(""/path/to/file.xlsx"", sheet=i)
    }

This gives me a nice list, `all.sheets`, that I can access, but I'm unsure about the best way to create a new data frame from specific columns in the list of data frames.

I've tried the code below, which creates a brand new data frame by looping through the list of data frames. On the first data frame, it saves the columns that are consistent in all the sheets, and then adds the week-specific columns.

    Cleaned <- data.frame()
    number_sheets <- 3
    
    for (i in 1:number_sheets) {
      if (i == 1) {
        Cleaned <- all.sheets[[i]][,c(""ID"", ""Gender"", ""DOB"")]
      }
      Cleaned$Absences.i <- all.sheets[[i]][,c(""Asences"")]  # wrong... obviously doesn't work... but essentially what I want
      # Other week-specific columns go here... somehow...
    }

This code doesn't work though, since `Cleaned$Absences.i` is obviously not how you create dynamic columns in a data frame.

What's the best way to combine a set of data frames and create new columns for each of the variables I'm trying to track?

*Extra hurdle:* I'm also trying to combine two columns, ""Absences"" and ""Absences_excused"" into a single ""Absences"" column in the final data frame, so I'm trying to make my solution let me perform transformations to the new columns, like so (again, this isn't right):

	Cleaned$Absences.i <- all.sheets[[i]][,c(""Asences"")] + all.sheets[[i]][,c(""Asences_excused"")]  ",r,data.frame,,,,,open,0,701,14,"Combine a series of data frames and create new columns for data in each I have an Excel file with a sheet for each week in my data set. Each sheet has the same number of rows, and each row is identical across the sheets (with the exception of the time period… sheet 1 represents week 1, sheet 2 week 2, etc.). I'm trying to import all the Excel sheets as one data frame in R.

For example, my data is essentially structured like this (with several more columns and sheets):

	Week 1 sheet
	ID    Gender    DOB    Absences    Lates
	1     M         1997   5           14
	2     F         1998   4           3

	Week 2 sheet
	ID    Gender    DOB    Absences    Lates
	1     M         1997   2           10
	2     F         1998   8           2

I'm trying to build a script that will take x numbers of sheets and combine them into one data frame like this:

	Combined (ideal)
	ID    Gender    DOB    Absences.1    Lates.1    Absences.2    Lates.2
	1     M         1997   5             14         2             10
	2     F         1998   4             3          8             2

I'm using gdata to import the Excel files. 

I've tried creating a loop (normally bad for R, I know...) that will go through all the sheets in the Excel file and add each to a list as a data frame:

    library(gdata)

    number_sheets <- 3
    all.sheets <- vector(mode=""list"", length=number_sheets)

    for (i in 1:number_sheets) {
      all.sheets[[i]] <- read.xls(""/path/to/file.xlsx"", sheet=i)
    }

This gives me a nice list, `all.sheets`, that I can access, but I'm unsure about the best way to create a new data frame from specific columns in the list of data frames.

I've tried the code below, which creates a brand new data frame by looping through the list of data frames. On the first data frame, it saves the columns that are consistent in all the sheets, and then adds the week-specific columns.

    Cleaned <- data.frame()
    number_sheets <- 3
    
    for (i in 1:number_sheets) {
      if (i == 1) {
        Cleaned <- all.sheets[[i]][,c(""ID"", ""Gender"", ""DOB"")]
      }
      Cleaned$Absences.i <- all.sheets[[i]][,c(""Asences"")]  # wrong... obviously doesn't work... but essentially what I want
      # Other week-specific columns go here... somehow...
    }

This code doesn't work though, since `Cleaned$Absences.i` is obviously not how you create dynamic columns in a data frame.

What's the best way to combine a set of data frames and create new columns for each of the variables I'm trying to track?

*Extra hurdle:* I'm also trying to combine two columns, ""Absences"" and ""Absences_excused"" into a single ""Absences"" column in the final data frame, so I'm trying to make my solution let me perform transformations to the new columns, like so (again, this isn't right):

	Cleaned$Absences.i <- all.sheets[[i]][,c(""Asences"")] + all.sheets[[i]][,c(""Asences_excused"")]  ",2
8278966,11/26/2011 14:10:46,744240,05/08/2011 21:11:07,11,0,R: how can i get cluster number correspond to data using k-means clustering techniques in R?," i clustered data by k-means clustering method,  how can i get cluster number correspond to data using k-means clustering techniques in R? In order to get each record belongs to which cluster. 

example
12  32  13   =>  1. 12,13   2. 32",r,cluster-analysis,k-means,,,,open,0,49,16,"R: how can i get cluster number correspond to data using k-means clustering techniques in R?  i clustered data by k-means clustering method,  how can i get cluster number correspond to data using k-means clustering techniques in R? In order to get each record belongs to which cluster. 

example
12  32  13   =>  1. 12,13   2. 32",3
11397274,07/09/2012 14:36:21,1445183,06/08/2012 18:19:26,66,1,Different aggregation methods using french_fries,"I'm trying to further my understanding of data aggregation in R. Here is the data that I'm working with:

    library(plyr)
    library(data.table)
    library(reshape)
    ffm <- melt(french_fries, id.vars = 1:4, na.rm = T)
    dt <- data.table(ffm)

The following statements all produce nearly identical results.

    #using ddply
    ddply(ffm,.(variable),summarise,min = min(value), max=max(value), median=median(value))
    #using data.table
    dt[,list(min=min(value),max=max(value),mean=mean(value)), by=variable]
    #using tapply
    do.call(rbind, tapply(ffm$value, ffm$variable, function(x) c(min=min(x),max=max(x),median=median(x))))
    #using aggregate
    aggregate(value~variable, data=ffm, function(x) c(min=min(x),max=max(x),median=median(x)))
    #using cast
    cast(ffm, variable ~ ., c(min,median,max), value=""value"")
    #doesn't appear possible using dcast from reshape2

My question is, what other statements using **different methods** will accomplish the same thing? I realize this seems like an odd question but I'm trying to get a hang of the syntax and functionality of various packages in R. If there are a ton of ways to do this, I apologize in advance but I'm guessing I'm only missing a few of the aggregation methods that can produce summaries on multiple aggregation formulae.

Thanks in advance,
--JT",r,,,,,07/09/2012 14:55:07,not a real question,1,202,5,"Different aggregation methods using french_fries I'm trying to further my understanding of data aggregation in R. Here is the data that I'm working with:

    library(plyr)
    library(data.table)
    library(reshape)
    ffm <- melt(french_fries, id.vars = 1:4, na.rm = T)
    dt <- data.table(ffm)

The following statements all produce nearly identical results.

    #using ddply
    ddply(ffm,.(variable),summarise,min = min(value), max=max(value), median=median(value))
    #using data.table
    dt[,list(min=min(value),max=max(value),mean=mean(value)), by=variable]
    #using tapply
    do.call(rbind, tapply(ffm$value, ffm$variable, function(x) c(min=min(x),max=max(x),median=median(x))))
    #using aggregate
    aggregate(value~variable, data=ffm, function(x) c(min=min(x),max=max(x),median=median(x)))
    #using cast
    cast(ffm, variable ~ ., c(min,median,max), value=""value"")
    #doesn't appear possible using dcast from reshape2

My question is, what other statements using **different methods** will accomplish the same thing? I realize this seems like an odd question but I'm trying to get a hang of the syntax and functionality of various packages in R. If there are a ton of ways to do this, I apologize in advance but I'm guessing I'm only missing a few of the aggregation methods that can produce summaries on multiple aggregation formulae.

Thanks in advance,
--JT",1
9711291,03/14/2012 22:30:30,1270236,03/14/2012 22:05:38,1,0,Improving help in R,"I always wander what other people say about the R help. Finally after some years of using, I decided that it is probably time to try to do something about it, because the feeling of gritting teeth does not go away with years of usage. :) Moreover, I think it is one of the few things where R does not kick asses to the other statistical softwares. So, to the point:

I get the feeling (by some experience with learning programming languages when I am not primarily a programmer but economist/statistician) that structure of help really helps and I would like to have it to go in the way

 - basic syntax (by basic I really mean only the necessary arguments)
 - example for ""dummies"", to see what it does
 - click-here-if-you-want-to-know-more menu/button

The best documentation that I ever used is probably one of Mathematica, look for example here -- ok, because I am new it prevents me from posting more than two links, so look for reference wolfram for Fit command (it is somehow related to the stuff below).

So what I did is that I took R help file for Fitting Linear Models ( http://stat.ethz.ch/R-manual/R-patched/library/stats/html/lm.html ) and put it into some more readable shape for me.
So I built some other version (actually two) look at http://pinda.sifruje.cz/ . The ""original"" is just the same text taken and only some basic stuff solved (on my 27 inch monitor, if I make the browser full screen the original help is almost unreadable, because it stretches from one end to the other. Next we already have some pretty good fonts so why not to use them, here Linux Biolinum, and I did some to me aesthetic changes...). The ""custom"" goes a bit deeper and tries to implement a bit more of my thoughts. The buttons are really made quickly and only as an example. Also I do not claim that the code is anywhere good, I spent like hour or two just sawing some pieces that already lay around web. Just take it as some demonstration.

What I would like to receive from you is your opinions about this topic. The stuff that I did is pretty easy to do even algorithmically (some parser could probably parse the existing help files). The only added value here is making important stuff more visible. I also added the ""dummies"" example.

So please tell me what you think. I am a bit busy and if I do anything with it I would like to have it thought through carefully beforehand. Also if anybody would be interested in helping, or if he is running similar project, tell me.

Thanks,
Tomas.",r,documentation,,,,03/15/2012 13:27:58,not a real question,1,437,4,"Improving help in R I always wander what other people say about the R help. Finally after some years of using, I decided that it is probably time to try to do something about it, because the feeling of gritting teeth does not go away with years of usage. :) Moreover, I think it is one of the few things where R does not kick asses to the other statistical softwares. So, to the point:

I get the feeling (by some experience with learning programming languages when I am not primarily a programmer but economist/statistician) that structure of help really helps and I would like to have it to go in the way

 - basic syntax (by basic I really mean only the necessary arguments)
 - example for ""dummies"", to see what it does
 - click-here-if-you-want-to-know-more menu/button

The best documentation that I ever used is probably one of Mathematica, look for example here -- ok, because I am new it prevents me from posting more than two links, so look for reference wolfram for Fit command (it is somehow related to the stuff below).

So what I did is that I took R help file for Fitting Linear Models ( http://stat.ethz.ch/R-manual/R-patched/library/stats/html/lm.html ) and put it into some more readable shape for me.
So I built some other version (actually two) look at http://pinda.sifruje.cz/ . The ""original"" is just the same text taken and only some basic stuff solved (on my 27 inch monitor, if I make the browser full screen the original help is almost unreadable, because it stretches from one end to the other. Next we already have some pretty good fonts so why not to use them, here Linux Biolinum, and I did some to me aesthetic changes...). The ""custom"" goes a bit deeper and tries to implement a bit more of my thoughts. The buttons are really made quickly and only as an example. Also I do not claim that the code is anywhere good, I spent like hour or two just sawing some pieces that already lay around web. Just take it as some demonstration.

What I would like to receive from you is your opinions about this topic. The stuff that I did is pretty easy to do even algorithmically (some parser could probably parse the existing help files). The only added value here is making important stuff more visible. I also added the ""dummies"" example.

So please tell me what you think. I am a bit busy and if I do anything with it I would like to have it thought through carefully beforehand. Also if anybody would be interested in helping, or if he is running similar project, tell me.

Thanks,
Tomas.",2
11062540,06/16/2012 10:09:52,1322296,04/09/2012 16:41:24,37,0,mapply over two lists,"I recentely asked a question about using an `apply` function over two lists.  Each list is a list of data frames. For each time the function runs I want to take vectors from the first element (a dataframe) in `mylist1` and some vectors from the first element (a dataframe) in `mylist2` and regress them against each other. Then move onto the next `mylist1` element and `mylist2` element.  Effectively the function takes two lists with the same number of elements and takes a pair (one from each list) and plays about with them.

I tried the following, but the results I get are bot what I want: 



    a1<-c(1:5,rep(0,5))
    a2<-c(1:5,10:6)
    b2<-c(rep(100,5),rep(50,5))
    z<-c(rep(""part1"",5),rep(""part2"",5))
    df1<-data.frame(a1,z)
    df2<-data.frame(a2,b2,z)

    mylist1<-split(df1,z)
    mylist2<-split(df2,z)


    myfunction<-function(x,y) 
    {
    
    meana <- mean(x$a)
    meanb <- mean(y$b)
    model<-lm((x$a)~(y$a))
    return(c(model$coefficients[2],meana=meana,meanb=meanb))
    }

    result <- mapply(myfunction,x=mylist,y=mylist2)

     #result
    #        x   y
    #y$a     1  -1
    #meana   3   8
    #meanb 100  50

What I want is:

    #y$a     1   0   
    #meana   3   0
    #meanb   100 50


    #e.g. the results in the first row are lm((mylist1[[1]][,1])~(mylist2[[1]][,1]))  and lm((x[[2]][,1])~(y[[2]][,1]))",r,list,apply,,,06/16/2012 16:56:02,too localized,1,282,4,"mapply over two lists I recentely asked a question about using an `apply` function over two lists.  Each list is a list of data frames. For each time the function runs I want to take vectors from the first element (a dataframe) in `mylist1` and some vectors from the first element (a dataframe) in `mylist2` and regress them against each other. Then move onto the next `mylist1` element and `mylist2` element.  Effectively the function takes two lists with the same number of elements and takes a pair (one from each list) and plays about with them.

I tried the following, but the results I get are bot what I want: 



    a1<-c(1:5,rep(0,5))
    a2<-c(1:5,10:6)
    b2<-c(rep(100,5),rep(50,5))
    z<-c(rep(""part1"",5),rep(""part2"",5))
    df1<-data.frame(a1,z)
    df2<-data.frame(a2,b2,z)

    mylist1<-split(df1,z)
    mylist2<-split(df2,z)


    myfunction<-function(x,y) 
    {
    
    meana <- mean(x$a)
    meanb <- mean(y$b)
    model<-lm((x$a)~(y$a))
    return(c(model$coefficients[2],meana=meana,meanb=meanb))
    }

    result <- mapply(myfunction,x=mylist,y=mylist2)

     #result
    #        x   y
    #y$a     1  -1
    #meana   3   8
    #meanb 100  50

What I want is:

    #y$a     1   0   
    #meana   3   0
    #meanb   100 50


    #e.g. the results in the first row are lm((mylist1[[1]][,1])~(mylist2[[1]][,1]))  and lm((x[[2]][,1])~(y[[2]][,1]))",3
2476946,03/19/2010 11:53:44,203420,11/05/2009 12:38:28,87,3,Creating large XML Trees in R,"I'm trying to create a large XML tree in R. Here's a simplified version of the code:

    library(XML)
    N = 100000#In practice is larger  10^8/ 10^9
    seq = newXMLNode(""sequence"")
    pars = as.character(1:N)
    for(i in 1:N)
        newXMLNode(""Parameter"", parent=seq, attrs=c(id=pars[i]))


When N is about N^6 this takes about a minute, N^7 takes about forty minutes. Is there anyway to speed this up?

Using the paste command:

    par_tmp = paste('<Parameter id=""', pars, '""/>', sep="""")

takes less than a second.

",r,xml,performance,,,,open,0,99,6,"Creating large XML Trees in R I'm trying to create a large XML tree in R. Here's a simplified version of the code:

    library(XML)
    N = 100000#In practice is larger  10^8/ 10^9
    seq = newXMLNode(""sequence"")
    pars = as.character(1:N)
    for(i in 1:N)
        newXMLNode(""Parameter"", parent=seq, attrs=c(id=pars[i]))


When N is about N^6 this takes about a minute, N^7 takes about forty minutes. Is there anyway to speed this up?

Using the paste command:

    par_tmp = paste('<Parameter id=""', pars, '""/>', sep="""")

takes less than a second.

",3
5360521,03/19/2011 06:15:18,180892,09/29/2009 04:54:20,897,29,"Proofreading the PDF of a book, thesis, or report derived from a large multi-file Sweave project","I'm a big fan of reproducible research.
I often use make, Sweave, LaTeX, and R to produce large research reports (i.e., lots of `Sexpr()` commands and heaps of graphs and tables).

Obviously, `R CMD Sweave` identifies certain errors in the R code chunks at compilation.
But the resulting PDF can still contain undesirable results.
I have a few strategies for proofreading such documents, but I was interested in learning from  others on SO.

### Questions:
1. Does anyone have any tips or tricks related to proofreading and quality control when it comes to producing PDFs based on large multi-file Sweave projects?
2. What are the most common errors that you encounter in resulting PDFs?
3. How do you efficiently identify errors in the resulting PDF? 
4. How do you efficiently move between PDF and Rnw source?",r,sweave,,,,,open,0,129,16,"Proofreading the PDF of a book, thesis, or report derived from a large multi-file Sweave project I'm a big fan of reproducible research.
I often use make, Sweave, LaTeX, and R to produce large research reports (i.e., lots of `Sexpr()` commands and heaps of graphs and tables).

Obviously, `R CMD Sweave` identifies certain errors in the R code chunks at compilation.
But the resulting PDF can still contain undesirable results.
I have a few strategies for proofreading such documents, but I was interested in learning from  others on SO.

### Questions:
1. Does anyone have any tips or tricks related to proofreading and quality control when it comes to producing PDFs based on large multi-file Sweave projects?
2. What are the most common errors that you encounter in resulting PDFs?
3. How do you efficiently identify errors in the resulting PDF? 
4. How do you efficiently move between PDF and Rnw source?",2
10091319,04/10/2012 15:07:51,1215379,02/17/2012 04:20:56,56,0,read.csv appends/modifies column headdings with date values,"Im trying to read a csv file into R that has date values in some of the colum headdings.

As an example, the data file looks something like this:  

    ID  Type   1/1/2001  2/1/2001  3/1/2001  4/1/2011
    A   Supply       25        35        45        55  
    B   Demand       26        35        41        22
    C   Supply       25        35        44        85  
    D   Supply       24        39        45        75  
    D   Demand       26        35        41        22

...and my read.csv logic looks like this

    dat10<-read.csv(""c:\data.csv"",header=TRUE, sep="","",as.is=TRUE)

The read.csv works fine except it modifies the name of the colums with dates as follows:

    x1.1.2001  x2.1.2001  x3.1.2001  x4.1.2001

Is there a way to prevent this, or a easy way to correct afterwards?







 


",r,csv,data.frame,,,,open,0,291,7,"read.csv appends/modifies column headdings with date values Im trying to read a csv file into R that has date values in some of the colum headdings.

As an example, the data file looks something like this:  

    ID  Type   1/1/2001  2/1/2001  3/1/2001  4/1/2011
    A   Supply       25        35        45        55  
    B   Demand       26        35        41        22
    C   Supply       25        35        44        85  
    D   Supply       24        39        45        75  
    D   Demand       26        35        41        22

...and my read.csv logic looks like this

    dat10<-read.csv(""c:\data.csv"",header=TRUE, sep="","",as.is=TRUE)

The read.csv works fine except it modifies the name of the colums with dates as follows:

    x1.1.2001  x2.1.2001  x3.1.2001  x4.1.2001

Is there a way to prevent this, or a easy way to correct afterwards?







 


",3
7031935,08/11/2011 19:49:52,875345,08/02/2011 19:31:12,3,0,R: using previously determined variable as part of png file name,"I am still new to R and I have searched around for a solution to my simple question, but I haven't found an answer that I've been able to get to work. I am looking to use a previously identified variable per data set, here variable=SNPname to include in script for automated generation of graph output in png format.

I am using this to generate a kmeans plot and have:

    (cl <- kmeans(FilteredData[,6:7], 5, nstart=25))
    png(""C:/temp/$SNPnamegraph1.png"")                 #SNPname to include in filename
    plot(FilteredData[,6:7], col=cl$cluster)
    points(cl$centers, col=1:5, pch=8)
    dev.off()

where I want to include that variable in line 2 at the beginning of the file name. Is there a simple way to do this that I am just missing? ",r,variables,png,,,,open,0,147,11,"R: using previously determined variable as part of png file name I am still new to R and I have searched around for a solution to my simple question, but I haven't found an answer that I've been able to get to work. I am looking to use a previously identified variable per data set, here variable=SNPname to include in script for automated generation of graph output in png format.

I am using this to generate a kmeans plot and have:

    (cl <- kmeans(FilteredData[,6:7], 5, nstart=25))
    png(""C:/temp/$SNPnamegraph1.png"")                 #SNPname to include in filename
    plot(FilteredData[,6:7], col=cl$cluster)
    points(cl$centers, col=1:5, pch=8)
    dev.off()

where I want to include that variable in line 2 at the beginning of the file name. Is there a simple way to do this that I am just missing? ",3
1384403,09/05/2009 22:30:08,142879,07/22/2009 14:12:09,26,2,animated gifs in R,anyone have any tips as to making animated GIFs in R. I'm trying to make some time lapse map GIFs.,r,,,,,,open,0,20,4,animated gifs in R anyone have any tips as to making animated GIFs in R. I'm trying to make some time lapse map GIFs.,1
9936269,03/30/2012 02:37:15,258755,01/25/2010 20:20:50,945,14,Storing a histogram from ggplot2 for retrieval later,"Is it possible to store the information generated from `geom_histogram()` in such a way that it can be retrieved at a latter stage? I want to calculated histograms of a large dataset, and to store it so that I can added another layer of information at a later stage.

I did previously think about using a `pdf` or `jpeg` to do this (and asked a [question][1] recently on the topic) but I think that it would be cleaner if I managed to use the actual data.


  [1]: http://stackoverflow.com/questions/9917049/inserting-an-image-to-ggplot2",r,ggplot2,histogram,,,,open,0,88,8,"Storing a histogram from ggplot2 for retrieval later Is it possible to store the information generated from `geom_histogram()` in such a way that it can be retrieved at a latter stage? I want to calculated histograms of a large dataset, and to store it so that I can added another layer of information at a later stage.

I did previously think about using a `pdf` or `jpeg` to do this (and asked a [question][1] recently on the topic) but I think that it would be cleaner if I managed to use the actual data.


  [1]: http://stackoverflow.com/questions/9917049/inserting-an-image-to-ggplot2",3
5470983,03/29/2011 10:18:37,680324,03/28/2011 13:46:15,1,0,How to apply spline() to a large dataframe,"I am a newbie to R and I am trying to apply smooth.spline() to a large dataframe. I've looked at the related threads (""Apply a list of n functions to each row of a dataframe,"", ""How to apply a spline basis matrix"",...). Here is my dataframe and what I've tried so far:

> dim(mUnique)  
[1] 4565    9  
> str(mUnique)  
'data.frame':	4565 obs. of  9 variables:  
 $ Group.1: Factor w/ 4565 levels ""mal_mito_1"",""mal_mito_2"",..: 1 2 3 4 5 6 7 8 9 10 ...  
 $ h0     : num  0.18 -0.025 0.212 0.015 0.12 ...  
 $ h6     : num  -0.04 -0.305 -0.188 -0.185 -0.09 ...  
 $ h12    : num  -0.86 -1.1 -1.01 -1.04 -0.91 ...  
 $ h18    : num  -0.73 -1.215 -1.222 -0.355 -0.65 ...  
 $ h24    : num  0.04 0.025 -0.143 0.295 0.09 ...  
 $ h30    : num  -0.14 1.275 0.732 -0.015 -0.27 ...  
 $ h36    : num  1.44 1.795 1.627 0.385 0.91 ...  
 $ h42    : num  1.49 1.385 1.397 0.305 1.12 ...  

> head(mUnique)  
          ID      h0      h6     h12     h18     h24     h30    h36    h42  
1      mal_mito_1  0.1800 -0.0400 -0.8600 -0.7300  0.0400 -0.1400 1.4400 1.4900  
2      mal_mito_2 -0.0250 -0.3050 -1.1050 -1.2150  0.0250  1.2750 1.7950 1.3850  
3      mal_mito_3  0.2125 -0.1875 -1.0075 -1.2225 -0.1425  0.7325 1.6275 1.3975  
4 mal_rna_10_rRNA  0.0150 -0.1850 -1.0450 -0.3550  0.2950 -0.0150 0.3850 0.3050  
5 mal_rna_11_rRNA  0.1200 -0.0900 -0.9100 -0.6500  0.0900 -0.2700 0.9100 1.1200  
6 mal_rna_14_rRNA  0.0200 -0.0200 -0.8400 -0.6600  0.1700 -0.0900 0.6200 0.0800  
>  

I can apply smooth.spline on each row independently and it looks good with spline() so far (I want 48 points. I'll figure out later how to do it with smoooth.spline spar):
>time <- c(0,6,12,18,24,30,36,42)  
> plot(time, mUnique[1, 2:9])  
> smooth <- smooth.spline(time, mUnique[1, 2:9])  
> lines(smooth, col=""blue"")  
>splin <-spline(time, mUnique[1, 2:9], n=48)  
>lines(splin, col=""blue"")  

My question is I suppose basic, but how to I apply smooth.spline() or spline() to the whole dataframe, and get back a matrix 4565 * 49 where I have the coordinates for each knots of the smoothed spline?  I don't really care about plotting that data.

I tried:
> smooth <- smooth.spline(time, mUnique[, 2:9]|factor(ID))

Now, don't know what to do. Is that a matter of making loops?

Thank you in advance",r,modeling,spline,,,,open,0,498,8,"How to apply spline() to a large dataframe I am a newbie to R and I am trying to apply smooth.spline() to a large dataframe. I've looked at the related threads (""Apply a list of n functions to each row of a dataframe,"", ""How to apply a spline basis matrix"",...). Here is my dataframe and what I've tried so far:

> dim(mUnique)  
[1] 4565    9  
> str(mUnique)  
'data.frame':	4565 obs. of  9 variables:  
 $ Group.1: Factor w/ 4565 levels ""mal_mito_1"",""mal_mito_2"",..: 1 2 3 4 5 6 7 8 9 10 ...  
 $ h0     : num  0.18 -0.025 0.212 0.015 0.12 ...  
 $ h6     : num  -0.04 -0.305 -0.188 -0.185 -0.09 ...  
 $ h12    : num  -0.86 -1.1 -1.01 -1.04 -0.91 ...  
 $ h18    : num  -0.73 -1.215 -1.222 -0.355 -0.65 ...  
 $ h24    : num  0.04 0.025 -0.143 0.295 0.09 ...  
 $ h30    : num  -0.14 1.275 0.732 -0.015 -0.27 ...  
 $ h36    : num  1.44 1.795 1.627 0.385 0.91 ...  
 $ h42    : num  1.49 1.385 1.397 0.305 1.12 ...  

> head(mUnique)  
          ID      h0      h6     h12     h18     h24     h30    h36    h42  
1      mal_mito_1  0.1800 -0.0400 -0.8600 -0.7300  0.0400 -0.1400 1.4400 1.4900  
2      mal_mito_2 -0.0250 -0.3050 -1.1050 -1.2150  0.0250  1.2750 1.7950 1.3850  
3      mal_mito_3  0.2125 -0.1875 -1.0075 -1.2225 -0.1425  0.7325 1.6275 1.3975  
4 mal_rna_10_rRNA  0.0150 -0.1850 -1.0450 -0.3550  0.2950 -0.0150 0.3850 0.3050  
5 mal_rna_11_rRNA  0.1200 -0.0900 -0.9100 -0.6500  0.0900 -0.2700 0.9100 1.1200  
6 mal_rna_14_rRNA  0.0200 -0.0200 -0.8400 -0.6600  0.1700 -0.0900 0.6200 0.0800  
>  

I can apply smooth.spline on each row independently and it looks good with spline() so far (I want 48 points. I'll figure out later how to do it with smoooth.spline spar):
>time <- c(0,6,12,18,24,30,36,42)  
> plot(time, mUnique[1, 2:9])  
> smooth <- smooth.spline(time, mUnique[1, 2:9])  
> lines(smooth, col=""blue"")  
>splin <-spline(time, mUnique[1, 2:9], n=48)  
>lines(splin, col=""blue"")  

My question is I suppose basic, but how to I apply smooth.spline() or spline() to the whole dataframe, and get back a matrix 4565 * 49 where I have the coordinates for each knots of the smoothed spline?  I don't really care about plotting that data.

I tried:
> smooth <- smooth.spline(time, mUnique[, 2:9]|factor(ID))

Now, don't know what to do. Is that a matter of making loops?

Thank you in advance",3
6009222,05/15/2011 15:08:04,670186,03/21/2011 21:12:39,39,0,R Venn Diagram package Venerable unavailable - alternative package?,"I need to plot area proportional Venn Diagrams with at least 5 variables.

I tried to install Vennerable package but its just not available anymore.

https://r-forge.r-project.org/bin/windows/contrib/latest/Vennerable_2.0.zip

LINK is dead!!!

Is there an alternative package?",r,area,venn-diagram,,,,open,0,30,9,"R Venn Diagram package Venerable unavailable - alternative package? I need to plot area proportional Venn Diagrams with at least 5 variables.

I tried to install Vennerable package but its just not available anymore.

https://r-forge.r-project.org/bin/windows/contrib/latest/Vennerable_2.0.zip

LINK is dead!!!

Is there an alternative package?",3
7963747,11/01/2011 07:51:31,969051,09/28/2011 12:24:16,19,0,How to aggregate data in timeSequence?,"I have a problem and would really need your help? My data (let's name it ""date"") looks like this:

location       date  value
       1 2010-01-01    6.4
       1 2010-01-02    5.7
       .
       .  
       2 2010-01-01    0.8
       2 2010-01-02    2.5
       2 2010-01-03    5.5

I would like to aggregate data (value) on location and on 3 weeks period? I have already try to use package timeSeries but it is not working?

    by1 <- timeSequence(from = ""2009-12-30"", to = ""2010-12-29"", by= ""4 week"")
    by1
    aggregate(date, by=list(by1, date$location), sum)

Thank you for your help!
",r,aggregate,time-series,,,,open,0,159,6,"How to aggregate data in timeSequence? I have a problem and would really need your help? My data (let's name it ""date"") looks like this:

location       date  value
       1 2010-01-01    6.4
       1 2010-01-02    5.7
       .
       .  
       2 2010-01-01    0.8
       2 2010-01-02    2.5
       2 2010-01-03    5.5

I would like to aggregate data (value) on location and on 3 weeks period? I have already try to use package timeSeries but it is not working?

    by1 <- timeSequence(from = ""2009-12-30"", to = ""2010-12-29"", by= ""4 week"")
    by1
    aggregate(date, by=list(by1, date$location), sum)

Thank you for your help!
",3
4874867,02/02/2011 13:13:48,217595,11/24/2009 06:50:17,1251,54,Learning functional programming with R,"I have recently studied functional programming with Haskell and
Clojure and found it has also improved my R coding practices. For
example I've better grasped the possibilities of usign lists and apply
family of functions instead of loops. As a side effect I also
discovered that a lot of my problems are parallel (and I can use
`mclapply` for significant speed up), but I've been
thinking sequentially in terms of loops.


I've read quite a bit of R material over the past couple of years and 
although the benefits of using the apply functions is usually
demonstrated, I've found that still most of the examples
use loops instead of lapply, mapply etc. 

Now my question is: Can you recommend me some good R tutorials or
books that have special focus on functional programming with R? If
such resources exists I'd like to recommend those for my students when
learning R. 
",r,books,functional-programming,tutorials,,10/04/2011 21:31:55,not constructive,1,139,5,"Learning functional programming with R I have recently studied functional programming with Haskell and
Clojure and found it has also improved my R coding practices. For
example I've better grasped the possibilities of usign lists and apply
family of functions instead of loops. As a side effect I also
discovered that a lot of my problems are parallel (and I can use
`mclapply` for significant speed up), but I've been
thinking sequentially in terms of loops.


I've read quite a bit of R material over the past couple of years and 
although the benefits of using the apply functions is usually
demonstrated, I've found that still most of the examples
use loops instead of lapply, mapply etc. 

Now my question is: Can you recommend me some good R tutorials or
books that have special focus on functional programming with R? If
such resources exists I'd like to recommend those for my students when
learning R. 
",4
5648383,04/13/2011 11:24:53,705919,04/13/2011 11:24:53,1,0,How to do clustering using r language??....,"I'm very new to R programming language....Now i have got to do clustering on our dataset.......I have spent the whole day in knowing about the procedure but didnt get succeed.....plz someone can help me so that i could get clusters for my dataset...
                  I wil wait for reply...Thanking you guys...",r,cluster-analysis,,,,04/13/2011 12:52:02,not a real question,1,67,7,"How to do clustering using r language??.... I'm very new to R programming language....Now i have got to do clustering on our dataset.......I have spent the whole day in knowing about the procedure but didnt get succeed.....plz someone can help me so that i could get clusters for my dataset...
                  I wil wait for reply...Thanking you guys...",2
7464095,09/18/2011 20:02:23,951610,09/18/2011 20:02:23,1,0,R random skewers,"I am trying to make a function for random skewers method for matrix comparison sensu Cheverud, 1986 in R but I ran into immediate problems. How to scale vector to a unit length of one after generating it with rnorm(n) function? And afterwards how to apply generated skewers(vectors) to var/cov matrices and finally to determine vector correlation between the response vectors from two matrices? 

I know this is basically the whole thing but I really need help... ",r,vector,matrix,,,09/19/2011 23:00:07,not a real question,1,78,3,"R random skewers I am trying to make a function for random skewers method for matrix comparison sensu Cheverud, 1986 in R but I ran into immediate problems. How to scale vector to a unit length of one after generating it with rnorm(n) function? And afterwards how to apply generated skewers(vectors) to var/cov matrices and finally to determine vector correlation between the response vectors from two matrices? 

I know this is basically the whole thing but I really need help... ",3
6590920,07/06/2011 02:24:12,826450,07/02/2011 23:04:34,4,0,Help me correct my loop function on optim calculation: its giving indentical results for all the runs,"May you help me correct my loop function. It is giving me indentical results for all the runs.

My code and results and data I am getting are below(in order).

Thank you in advance

Edward
a=read.table(""D:/hope.txt"",header=T) 
attach(a) 
a 

#code

 #likilihood function 
llik = function(x) 
    { 
     al_j=x[1]; au_j=x[2]; sigma_j=x[3];  b_j=x[4] 
     sum(na.rm=T, 
         ifelse(a$R_j< 0, -log(1/(2*pi*(sigma_j^2)))- 
                            (1/(2*(sigma_j^2))*(a$R_j+al_j-b_j*a$R_m))^2, 
  
          ifelse(a$R_j>0 , -log(1/(2*pi*(sigma_j^2)))- 
                            (1/(2*(sigma_j^2))*(a$R_j+au_j-b_j*a$R_m))^2, 
  
  -log(ifelse (( pnorm (au_j, mean=b_j * a$R_m, 
                                                sd= sqrt(sigma_j^2))- 
                          pnorm(al_j, mean=b_j * a$R_m, sd=sqrt (sigma_j^2) )) > 0, 
  (pnorm (au_j,mean=b_j * a$R_m, sd= sqrt(sigma_j^2))- 
                            pnorm(al_j, mean=b_j * a$R_m, sd= sqrt(sigma_j^2) )), 
  1)) )) 
       ) 
    } 
start.par = c(-0.01,0.01,0.1,1) 

#looping now 

runs=133/20+1 #total data points divided by number od days in each quater+1 
out <- matrix(NA, nrow = runs, ncol = 4, 
 dimnames = list(paste(""Quater:"", 1:runs, sep = ''), 
 c(""al_j"", ""au_j"", ""sigma_j"", ""b_j""))) 

 for (i in 1:runs) { 
   a[seq(20 * (i - 1) +1, 20 * i), ] 
   out[i, ] <- optim(llik, par = start.par, method = ""Nelder-Mead"")[[1]] 
 } 
out 

#results I am getting 
> out 
               al_j       au_j       sigma_j      b_j 
Quater:1 0.04001525 0.06006251 -7.171336e-25 1.049982 
Quater:2 0.04001525 0.06006251 -7.171336e-25 1.049982 
Quater:3 0.04001525 0.06006251 -7.171336e-25 1.049982 
Quater:4 0.04001525 0.06006251 -7.171336e-25 1.049982 
Quater:5 0.04001525 0.06006251 -7.171336e-25 1.049982 
Quater:6 0.04001525 0.06006251 -7.171336e-25 1.049982 
Quater:7 0.04001525 0.06006251 -7.171336e-25 1.049982 
> 

#data

> a=read.table(""D:/hope.txt"",header=T)
> attach(a)
> a
             R_j          R_m
1    0.004522613  0.005969628
2    0.000500250 -0.003733120
3    0.000000000 -0.014917236
4    0.000000000 -0.014645022
5    0.020000000  0.017211165
6   -0.007352941  0.011623616
7    0.034567901  0.007440959
8   -0.000954654  0.008891883
9   -0.003822265  0.003425583
10  -0.011990408  0.000173465
11   0.017961165 -0.004791331
12  -0.018121125 -0.004075254
13  -0.003885381  0.002425306
14  -0.004388103  0.003214301
15   0.003917728  0.001167475
16            NA  0.000000000
17            NA -0.002288068
18   0.016582915  0.003207290
19   0.005931784  0.010440079
20   0.000000000 -0.000150949
21  -0.002457002  0.002131697
22  -0.002463054 -0.002809137
23            NA  0.000000000
24            NA -0.000772543
25   0.004926108  0.003470228
26   0.000000000  0.002697099
27   0.009803922 -0.002829863
28  -0.004854369  0.005891237
29  -0.016585366  0.003692262
30   0.004464286 -0.012855486
31   0.012345679  0.003291105
32  -0.015609756 -0.005796509
33  -0.013875124 -0.007241842
34   0.000000000  0.014627195
35   0.005025126  0.010005239
36  -0.001500000  0.004562562
37  -0.004506760  0.000885936
38   0.002515091 -0.010558751
39  -0.017561465  0.007535539
40   0.006128703 -0.006018786
41  -0.030456853 -0.016271600
42   0.010471204  0.009513103
43   0.002590674 -0.008430302
44  -0.010335917 -0.006665713
45  -0.002088773  0.009748669
46  -0.002093145  0.005421514
47  -0.006292606 -0.018528844
48  -0.034300792 -0.004574700
49   0.002732240  0.013994506
50   0.008174387  0.020398478
51  -0.010270270  0.009456836
52   0.006553796  0.003283443
53  -0.014107434 -0.003108880
54   0.028068244 -0.000346644
55   0.017130621 -0.003394446
56  -0.013157895 -0.008203795
57   0.013333333  0.016811439
58   0.005263158  0.004240110
59   0.002617801 -0.001020727
60   0.007832898 -0.008534713
61   0.018134715 -0.001684355
62   0.012722646 -0.006383295
63   0.017587940  0.009000422
64  -0.010370370 -0.011048668
65  -0.024950100 -0.015784948
66  -0.010747185 -0.007064998
67   0.000517331  0.008172464
68   0.037745605  0.009588436
69  -0.033881415 -0.006859178
70   0.010314595 -0.002422410
71   0.007146503  0.011792104
72   0.013684744 -0.000403708
73  -0.010000000 -0.001201408
74   0.000000000 -0.014576350
75  -0.006060606 -0.000221507
76  -0.004573171 -0.028957237
77   0.005615110  0.008897393
78  -0.024873096 -0.009472038
79  -0.013014055 -0.022171680
80  -0.006329114  0.013859068
81   0.007961783  0.003467221
82   0.000526593 -0.001349999
83            NA  0.000000000
84            NA  0.006233644
85   0.026315789  0.011249077
86   0.007692308  0.013843741
87  -0.007633588 -0.001636770
88   0.005128205 -0.006924002
89   0.010204082  0.010864286
90  -0.002020202  0.017548390
91  -0.030364372 -0.002185929
92  -0.003653445  0.005628483
93  -0.004714510  0.001619561
94   0.000000000  0.005952629
95   0.015789474  0.007864028
96  -0.036269430 -0.007729996
97   0.002688172  0.005301954
98   0.000000000  0.000248733
99  -0.008042895 -0.017509370
100  0.000000000  0.003533513
101  0.013513514  0.001994820
102 -0.013333333 -0.005881549
103 -0.035135135 -0.026034469
104 -0.047619048  0.014286383
105  0.026470588  0.021648984
106  0.013753582  0.004075471
107           NA  0.000000000
108           NA  0.000000000
109           NA  0.000284456
110           NA  0.000000000
111           NA -0.000251009
112  0.011797753  0.005397436
113           NA  0.000000000
114           NA -0.015543806
115 -0.002238388 -0.014514399
116 -0.006730230 -0.008616728
117  0.005081875  0.009394234
118 -0.002808989 -0.008439770
119 -0.004507042  0.015051671
120  0.010186757 -0.001066212
121  0.000000000 -0.011529000
122 -0.007843137  0.008777902
123 -0.003387916  0.006135182
124 -0.002832861 -0.009705806
125           NA  0.000000000
126           NA  0.002355866
127 -0.004524887 -0.003909906
128 -0.008522727 -0.014140975
129  0.009169054  0.012331338
130  0.013628620  0.001970940
131  0.005602241  0.005866591
132 -0.011142061  0.012642233
133  0.012957746  0.000169835







",r,optimization,loops,statistics,finance,07/06/2011 13:44:32,too localized,1,1143,17,"Help me correct my loop function on optim calculation: its giving indentical results for all the runs May you help me correct my loop function. It is giving me indentical results for all the runs.

My code and results and data I am getting are below(in order).

Thank you in advance

Edward
a=read.table(""D:/hope.txt"",header=T) 
attach(a) 
a 

#code

 #likilihood function 
llik = function(x) 
    { 
     al_j=x[1]; au_j=x[2]; sigma_j=x[3];  b_j=x[4] 
     sum(na.rm=T, 
         ifelse(a$R_j< 0, -log(1/(2*pi*(sigma_j^2)))- 
                            (1/(2*(sigma_j^2))*(a$R_j+al_j-b_j*a$R_m))^2, 
  
          ifelse(a$R_j>0 , -log(1/(2*pi*(sigma_j^2)))- 
                            (1/(2*(sigma_j^2))*(a$R_j+au_j-b_j*a$R_m))^2, 
  
  -log(ifelse (( pnorm (au_j, mean=b_j * a$R_m, 
                                                sd= sqrt(sigma_j^2))- 
                          pnorm(al_j, mean=b_j * a$R_m, sd=sqrt (sigma_j^2) )) > 0, 
  (pnorm (au_j,mean=b_j * a$R_m, sd= sqrt(sigma_j^2))- 
                            pnorm(al_j, mean=b_j * a$R_m, sd= sqrt(sigma_j^2) )), 
  1)) )) 
       ) 
    } 
start.par = c(-0.01,0.01,0.1,1) 

#looping now 

runs=133/20+1 #total data points divided by number od days in each quater+1 
out <- matrix(NA, nrow = runs, ncol = 4, 
 dimnames = list(paste(""Quater:"", 1:runs, sep = ''), 
 c(""al_j"", ""au_j"", ""sigma_j"", ""b_j""))) 

 for (i in 1:runs) { 
   a[seq(20 * (i - 1) +1, 20 * i), ] 
   out[i, ] <- optim(llik, par = start.par, method = ""Nelder-Mead"")[[1]] 
 } 
out 

#results I am getting 
> out 
               al_j       au_j       sigma_j      b_j 
Quater:1 0.04001525 0.06006251 -7.171336e-25 1.049982 
Quater:2 0.04001525 0.06006251 -7.171336e-25 1.049982 
Quater:3 0.04001525 0.06006251 -7.171336e-25 1.049982 
Quater:4 0.04001525 0.06006251 -7.171336e-25 1.049982 
Quater:5 0.04001525 0.06006251 -7.171336e-25 1.049982 
Quater:6 0.04001525 0.06006251 -7.171336e-25 1.049982 
Quater:7 0.04001525 0.06006251 -7.171336e-25 1.049982 
> 

#data

> a=read.table(""D:/hope.txt"",header=T)
> attach(a)
> a
             R_j          R_m
1    0.004522613  0.005969628
2    0.000500250 -0.003733120
3    0.000000000 -0.014917236
4    0.000000000 -0.014645022
5    0.020000000  0.017211165
6   -0.007352941  0.011623616
7    0.034567901  0.007440959
8   -0.000954654  0.008891883
9   -0.003822265  0.003425583
10  -0.011990408  0.000173465
11   0.017961165 -0.004791331
12  -0.018121125 -0.004075254
13  -0.003885381  0.002425306
14  -0.004388103  0.003214301
15   0.003917728  0.001167475
16            NA  0.000000000
17            NA -0.002288068
18   0.016582915  0.003207290
19   0.005931784  0.010440079
20   0.000000000 -0.000150949
21  -0.002457002  0.002131697
22  -0.002463054 -0.002809137
23            NA  0.000000000
24            NA -0.000772543
25   0.004926108  0.003470228
26   0.000000000  0.002697099
27   0.009803922 -0.002829863
28  -0.004854369  0.005891237
29  -0.016585366  0.003692262
30   0.004464286 -0.012855486
31   0.012345679  0.003291105
32  -0.015609756 -0.005796509
33  -0.013875124 -0.007241842
34   0.000000000  0.014627195
35   0.005025126  0.010005239
36  -0.001500000  0.004562562
37  -0.004506760  0.000885936
38   0.002515091 -0.010558751
39  -0.017561465  0.007535539
40   0.006128703 -0.006018786
41  -0.030456853 -0.016271600
42   0.010471204  0.009513103
43   0.002590674 -0.008430302
44  -0.010335917 -0.006665713
45  -0.002088773  0.009748669
46  -0.002093145  0.005421514
47  -0.006292606 -0.018528844
48  -0.034300792 -0.004574700
49   0.002732240  0.013994506
50   0.008174387  0.020398478
51  -0.010270270  0.009456836
52   0.006553796  0.003283443
53  -0.014107434 -0.003108880
54   0.028068244 -0.000346644
55   0.017130621 -0.003394446
56  -0.013157895 -0.008203795
57   0.013333333  0.016811439
58   0.005263158  0.004240110
59   0.002617801 -0.001020727
60   0.007832898 -0.008534713
61   0.018134715 -0.001684355
62   0.012722646 -0.006383295
63   0.017587940  0.009000422
64  -0.010370370 -0.011048668
65  -0.024950100 -0.015784948
66  -0.010747185 -0.007064998
67   0.000517331  0.008172464
68   0.037745605  0.009588436
69  -0.033881415 -0.006859178
70   0.010314595 -0.002422410
71   0.007146503  0.011792104
72   0.013684744 -0.000403708
73  -0.010000000 -0.001201408
74   0.000000000 -0.014576350
75  -0.006060606 -0.000221507
76  -0.004573171 -0.028957237
77   0.005615110  0.008897393
78  -0.024873096 -0.009472038
79  -0.013014055 -0.022171680
80  -0.006329114  0.013859068
81   0.007961783  0.003467221
82   0.000526593 -0.001349999
83            NA  0.000000000
84            NA  0.006233644
85   0.026315789  0.011249077
86   0.007692308  0.013843741
87  -0.007633588 -0.001636770
88   0.005128205 -0.006924002
89   0.010204082  0.010864286
90  -0.002020202  0.017548390
91  -0.030364372 -0.002185929
92  -0.003653445  0.005628483
93  -0.004714510  0.001619561
94   0.000000000  0.005952629
95   0.015789474  0.007864028
96  -0.036269430 -0.007729996
97   0.002688172  0.005301954
98   0.000000000  0.000248733
99  -0.008042895 -0.017509370
100  0.000000000  0.003533513
101  0.013513514  0.001994820
102 -0.013333333 -0.005881549
103 -0.035135135 -0.026034469
104 -0.047619048  0.014286383
105  0.026470588  0.021648984
106  0.013753582  0.004075471
107           NA  0.000000000
108           NA  0.000000000
109           NA  0.000284456
110           NA  0.000000000
111           NA -0.000251009
112  0.011797753  0.005397436
113           NA  0.000000000
114           NA -0.015543806
115 -0.002238388 -0.014514399
116 -0.006730230 -0.008616728
117  0.005081875  0.009394234
118 -0.002808989 -0.008439770
119 -0.004507042  0.015051671
120  0.010186757 -0.001066212
121  0.000000000 -0.011529000
122 -0.007843137  0.008777902
123 -0.003387916  0.006135182
124 -0.002832861 -0.009705806
125           NA  0.000000000
126           NA  0.002355866
127 -0.004524887 -0.003909906
128 -0.008522727 -0.014140975
129  0.009169054  0.012331338
130  0.013628620  0.001970940
131  0.005602241  0.005866591
132 -0.011142061  0.012642233
133  0.012957746  0.000169835







",5
