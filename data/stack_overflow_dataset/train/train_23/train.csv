PostId,PostCreationDate,OwnerUserId,OwnerCreationDate,ReputationAtPostCreation,OwnerUndeletedAnswerCountAtPostTime,Title,BodyMarkdown,Tag1,Tag2,Tag3,Tag4,Tag5,PostClosedDate,OpenStatus,OpenStatusInt,BodyLength,TitleLength,TitleConcatWithBody,NumberOfTags
4383477,12/08/2010 01:47:19,534445,12/08/2010 01:47:19,1,0,Extracting logical records from a physical file in the fastest time. ,"This is a generic question not specific to any particular language. Java can be chosen as a placeholder. If I have a flat file that has the following structure  
start:this is record 1:end  
start:this is record 2, with more chars than record 1:end  
start:this is record 3, with more chars than record 1 and record 2:end  
....  
start:this is record n, with more chars than record n-1 ... 1:end  
How can I efficiently process this physical file where every logical record is demarcated by a 'start' and 'end'. I'm not looking for the fastest way to do this ?",performance,,,,,,open,0,105,12,"Extracting logical records from a physical file in the fastest time.  This is a generic question not specific to any particular language. Java can be chosen as a placeholder. If I have a flat file that has the following structure  
start:this is record 1:end  
start:this is record 2, with more chars than record 1:end  
start:this is record 3, with more chars than record 1 and record 2:end  
....  
start:this is record n, with more chars than record n-1 ... 1:end  
How can I efficiently process this physical file where every logical record is demarcated by a 'start' and 'end'. I'm not looking for the fastest way to do this ?",1
8460378,12/10/2011 22:21:25,357035,06/03/2010 01:52:36,14,1,How to Test Site Performance in Asia,I'm located in the US and need to test site performance for Asian audiences. Any provide help on how I accomplish this?,performance,,,,,,open,0,22,7,How to Test Site Performance in Asia I'm located in the US and need to test site performance for Asian audiences. Any provide help on how I accomplish this?,1
6033080,05/17/2011 15:18:26,521799,11/26/2010 21:16:49,4300,168,Speed of paged queries in Oracle,"This is a never-ending topic for me and I'm wondering if I might be overlooking something. Essentially I use two types of SQL statements in an application:

 1. Regular queries with a ""fallback"" limit
 2. Sorted and paged queries

Now, we're talking about some queries against tables with several million records, joined to 5 more tables with several million records. Clearly, we hardly want to fetch all of them, that's why we have the above two methods to limit user queries.

**Case 1** is really simple. We just add an additional `ROWNUM` filter:

    WHERE ...
      AND ROWNUM < ?


That's quite fast, as Oracle's CBO will take this filter into consideration for its execution plan and probably apply a `FIRST_ROWS` operation (similar to the one enforced by the `/*+FIRST_ROWS*/` hint.

**Case 2**, however is a bit more tricky with Oracle, as there is no `LIMIT ... OFFSET` clause as in other RDBMS. So we nest our ""business"" query in a technical wrapper as such:

    SELECT outer.* FROM (
      SELECT * FROM (
        SELECT inner.*, ROWNUM as RNUM, MAX(ROWNUM) OVER(PARTITION BY 1) as TOTAL_ROWS
        FROM (
          [""business query""]
        ) inner
      ) 
      WHERE ROWNUM < ?
    ) outer
    WHERE outer.RNUM > ?

Note that the `TOTAL_ROWS` field is calculated to know how many pages we will have even without fetching all data. Now this paging query is usually quite satisfying. But every now and then (as I said, when querying 5M+ records, possibly including non-indexed searches), this runs for 2-3minutes.

I'm wondering, is that state-of-the-art simulation of `LIMIT ... OFFSET`, including `TOTAL_ROWS` in Oracle, or is there a better solution that will be faster by design, e.g. by using the `ROW_NUMBER()` window function instead of the `ROWNUM` pseudo-column?",performance,oracle,oracle11g,rownum,window-functions,,open,0,343,6,"Speed of paged queries in Oracle This is a never-ending topic for me and I'm wondering if I might be overlooking something. Essentially I use two types of SQL statements in an application:

 1. Regular queries with a ""fallback"" limit
 2. Sorted and paged queries

Now, we're talking about some queries against tables with several million records, joined to 5 more tables with several million records. Clearly, we hardly want to fetch all of them, that's why we have the above two methods to limit user queries.

**Case 1** is really simple. We just add an additional `ROWNUM` filter:

    WHERE ...
      AND ROWNUM < ?


That's quite fast, as Oracle's CBO will take this filter into consideration for its execution plan and probably apply a `FIRST_ROWS` operation (similar to the one enforced by the `/*+FIRST_ROWS*/` hint.

**Case 2**, however is a bit more tricky with Oracle, as there is no `LIMIT ... OFFSET` clause as in other RDBMS. So we nest our ""business"" query in a technical wrapper as such:

    SELECT outer.* FROM (
      SELECT * FROM (
        SELECT inner.*, ROWNUM as RNUM, MAX(ROWNUM) OVER(PARTITION BY 1) as TOTAL_ROWS
        FROM (
          [""business query""]
        ) inner
      ) 
      WHERE ROWNUM < ?
    ) outer
    WHERE outer.RNUM > ?

Note that the `TOTAL_ROWS` field is calculated to know how many pages we will have even without fetching all data. Now this paging query is usually quite satisfying. But every now and then (as I said, when querying 5M+ records, possibly including non-indexed searches), this runs for 2-3minutes.

I'm wondering, is that state-of-the-art simulation of `LIMIT ... OFFSET`, including `TOTAL_ROWS` in Oracle, or is there a better solution that will be faster by design, e.g. by using the `ROW_NUMBER()` window function instead of the `ROWNUM` pseudo-column?",5
10502280,05/08/2012 16:07:59,1365990,04/30/2012 14:15:51,6,0,Improve loading speed of a dynatree (300 nodes) which takes 20 to 25 seconds,I am building the entire JSON reading contents of each node and its details from the database. Then use initAjax method to load the dynatree. It takes about 20 to 25 seconds to load.Is there any way to speed up the loading time. I understand I could use lazy loading instead but I need the search ability on the tree for which I am assuming I need all the node present and not load when they are activated. The tree is going to grow considerbly more in the future months and years. Please suggest/advise what is the best path to take in my scenario. Thanks in advance.,performance,dynatree,,,,,open,0,107,14,Improve loading speed of a dynatree (300 nodes) which takes 20 to 25 seconds I am building the entire JSON reading contents of each node and its details from the database. Then use initAjax method to load the dynatree. It takes about 20 to 25 seconds to load.Is there any way to speed up the loading time. I understand I could use lazy loading instead but I need the search ability on the tree for which I am assuming I need all the node present and not load when they are activated. The tree is going to grow considerbly more in the future months and years. Please suggest/advise what is the best path to take in my scenario. Thanks in advance.,2
7657896,10/05/2011 07:16:23,504357,11/11/2010 10:17:49,441,14,Apache Benchmark - concurrency and number of requests,"The benchmark documentation says concurrency is how many requests are done simultaneously, while number of requests is total number of requests. What I'm wondering is, if I put a 100 requests at a concurrency level of 20, does that mean 5 tests of 20 requests at the same time, or 100 tests of 20 requests at the same time each? I'm assuming the second option, because of the example numbers quoted below..

I'm wondering because I frequently see results such as this one on some testing blogs:

    Complete requests: 1000000
    Failed requests: 2617614

This seems implausible, since the number of failed requests is higher than the number of total requests.",performance,apache,concurrency,request,benchmarking,,open,0,114,8,"Apache Benchmark - concurrency and number of requests The benchmark documentation says concurrency is how many requests are done simultaneously, while number of requests is total number of requests. What I'm wondering is, if I put a 100 requests at a concurrency level of 20, does that mean 5 tests of 20 requests at the same time, or 100 tests of 20 requests at the same time each? I'm assuming the second option, because of the example numbers quoted below..

I'm wondering because I frequently see results such as this one on some testing blogs:

    Complete requests: 1000000
    Failed requests: 2617614

This seems implausible, since the number of failed requests is higher than the number of total requests.",5
8075362,11/10/2011 05:30:34,944738,09/14/2011 13:36:55,106,5,programming approach for better performance,"Which approach is better?

Initialize everything at the start of application and free it at the end of application(exit).

Or Initialize when you need it and free it after using it.

And does it effect on the performance?

Thanks in advance",performance,development-approach,,,,11/10/2011 08:55:14,not a real question,1,37,5,"programming approach for better performance Which approach is better?

Initialize everything at the start of application and free it at the end of application(exit).

Or Initialize when you need it and free it after using it.

And does it effect on the performance?

Thanks in advance",2
10010890,04/04/2012 12:15:58,419425,08/13/2010 09:59:53,11,0,indexedDB vs WebSQL,"While I was studying IndexedDB, I materialized my findings in a performance test between IndexeDB and Web SQL. From my test it seems that WebSQL is faster, although I find other results on the web. I have added this project to [github](https://github.com/scaljeri/indexeddb-vs-websql)
If anybody would like to review my code and provide me feedback, I would really appreciate this!

Cheers
Luca",performance,web-sql,indexeddb,,,04/05/2012 12:33:31,not constructive,1,58,3,"indexedDB vs WebSQL While I was studying IndexedDB, I materialized my findings in a performance test between IndexeDB and Web SQL. From my test it seems that WebSQL is faster, although I find other results on the web. I have added this project to [github](https://github.com/scaljeri/indexeddb-vs-websql)
If anybody would like to review my code and provide me feedback, I would really appreciate this!

Cheers
Luca",3
7655060,10/04/2011 23:08:16,979447,10/04/2011 22:57:54,1,0,Apple Keyboard Efficiency and Big Oh,"I don't like Apple keyboards because there is no backspace key - I have to traverse to the end of a text segment and then delete everything before it (the delete key does what the backspace key usually would). 

It takes twice as long!

I wanted to make a case that the algorithm efficiency is different somehow, but I know that O (2n) and O(n) are equivalent in growth. 

Is there a better way to express this than saying O(2n), where the ""2"" matters for such small values of n?

Thanks, 
any help is appreciated! ",performance,algorithm,big-o,,,10/05/2011 03:25:59,not a real question,1,94,6,"Apple Keyboard Efficiency and Big Oh I don't like Apple keyboards because there is no backspace key - I have to traverse to the end of a text segment and then delete everything before it (the delete key does what the backspace key usually would). 

It takes twice as long!

I wanted to make a case that the algorithm efficiency is different somehow, but I know that O (2n) and O(n) are equivalent in growth. 

Is there a better way to express this than saying O(2n), where the ""2"" matters for such small values of n?

Thanks, 
any help is appreciated! ",3
7839049,10/20/2011 16:20:39,895174,08/15/2011 15:24:25,443,7,Mongo db any trick for speed up query whit OFFSET and LIMIT?,"does anyone knows if exist any good trick to speed up queries on mongo db when using limit with offset too?
i'm just using indexes, i would like to know if are there no-standard tricks. 
thx",performance,query,mongodb,tricks,,11/27/2011 17:33:01,not constructive,1,35,12,"Mongo db any trick for speed up query whit OFFSET and LIMIT? does anyone knows if exist any good trick to speed up queries on mongo db when using limit with offset too?
i'm just using indexes, i would like to know if are there no-standard tricks. 
thx",4
3971455,10/19/2010 18:29:42,361850,06/08/2010 22:11:44,6,0,Element host first time creation performance.,"We have WPF app and sometimes we display windows panel with windows controls. And one of those controls can be again a WPF control, which is hosted in element host.


    WPF (MainWindow)

      WindowsFormsHost

        Windows(Panel)

          Elementhost

                WPF(usercontrol)

 

 

This windows Panel on an average has 2-3 wpf controls. Everytime panel is displayed the initialization of first element host is taking around 250 ms and subsequent element hosts very less around 10 ms. We are disposing panel once it is closed. Per this article if we create an empty element host and keep it alive/visible till the life of app, any subsequent creation of elementhosts takes very little time, which is true and i tested with a sample project. But when i created an empty element host and added to our Main wpf window, it does not help. Which means, when a wndow panel displayed element host inside it still takes around 250 ms. Any ideas whats going on here?

 

Thanks

Santosh",performance,element,host,,,,open,0,198,6,"Element host first time creation performance. We have WPF app and sometimes we display windows panel with windows controls. And one of those controls can be again a WPF control, which is hosted in element host.


    WPF (MainWindow)

      WindowsFormsHost

        Windows(Panel)

          Elementhost

                WPF(usercontrol)

 

 

This windows Panel on an average has 2-3 wpf controls. Everytime panel is displayed the initialization of first element host is taking around 250 ms and subsequent element hosts very less around 10 ms. We are disposing panel once it is closed. Per this article if we create an empty element host and keep it alive/visible till the life of app, any subsequent creation of elementhosts takes very little time, which is true and i tested with a sample project. But when i created an empty element host and added to our Main wpf window, it does not help. Which means, when a wndow panel displayed element host inside it still takes around 250 ms. Any ideas whats going on here?

 

Thanks

Santosh",3
8155546,11/16/2011 16:56:45,1050112,11/16/2011 16:36:50,1,0,Slow facebook API via Python on Google App Engine (GAE),"i am fetching data from my news stream to filter them and this takes facebook sometimes more than 5 seconds and i hit the url_fetch() timeout of Google's App Engine.

Now is there any way to work around this timeout or to improve the speed with which facebook replies to my request?
This is the part where i get my Exceptions:

    params[u'access_token'] = self.access_token

    result = json.loads(
      urlfetch.fetch(
        url=u'https://graph.facebook.com/me/home?limit=1000,
        payload=urllib.urlencode(params),
        method=urlfetch.POST,
        headers={u'Content-Type': u'application/x-www-form-urlencoded'}
      ).content)

Thanks for your time :)",performance,google-app-engine,facebook-graph-api,timeout,url-fetch,,open,0,120,10,"Slow facebook API via Python on Google App Engine (GAE) i am fetching data from my news stream to filter them and this takes facebook sometimes more than 5 seconds and i hit the url_fetch() timeout of Google's App Engine.

Now is there any way to work around this timeout or to improve the speed with which facebook replies to my request?
This is the part where i get my Exceptions:

    params[u'access_token'] = self.access_token

    result = json.loads(
      urlfetch.fetch(
        url=u'https://graph.facebook.com/me/home?limit=1000,
        payload=urllib.urlencode(params),
        method=urlfetch.POST,
        headers={u'Content-Type': u'application/x-www-form-urlencoded'}
      ).content)

Thanks for your time :)",5
10712324,05/23/2012 01:25:29,1411449,05/23/2012 00:52:07,1,0,JavaMail SMTPTransport.isConnected() very slow with Exchange server,"I have a mail daemon to send email notification using JavaMail (1.4.5) with SMTP. It is suggested to use the instance method `sendMessage()` instead of the static `Transport.send()`. So I am testing the connection with `isConnected()` before calling `sendMessage()`. It works fine on my home computer with my ISP's SMTP server. However when I test the same code on my work computer, it is much slower. (My work computer is much faster than my home computer.) So I think the only difference is it talks to company Exchange server. Anyone experienced similar issues?

-Home computer with ISP: `isConnected()` takes 10-100ms

-Work computer with Exchange: `isConnected()` takes 5s.

-If I use the static `Transport.send()` to send message (no need to test connection), it takes about 300ms on my work computer.

Here is the sample code:


	public class TestMail {
		static Session session;
		static InternetAddress fromAddr;
		static InternetAddress[] toAddr;
		static int n = 1;
	
		public static void main(String[] args) throws MessagingException {
			String to = ""yourname@gmail.com"";	//System.getProperty(""user.name"") + ""@aaaa.com""
			String from = ""yourname@gmail.com"";	//System.getProperty(""user.name"") + ""@aaaa.com""
	
			Properties props = new Properties();
			props.put(""mail.smtp.host"", ""mail.optonline.net"");
			// props.put(""mail.smtp.host"", ""mail.aaaa.com"");
	
			session = Session.getInstance(props);
			fromAddr = new InternetAddress(from);
			toAddr = new InternetAddress[] { new InternetAddress(to) };
	
			Transport bus = session.getTransport(""smtp"");
			bus.connect();
	
			String body = ""This is the body of the email.\n"";
	
			for (int i = 1; i <= n; ++i) {
	
				Message msg = createMessage(i + ""th email"", body);
	
				long stime = System.currentTimeMillis();
	
				if (!bus.isConnected()) {
					System.out.println(""Connecting ...."");
					bus.connect();
				}
				long etime = System.currentTimeMillis();
				long isConnectedTime = etime - stime;
	
				stime = etime;
				bus.sendMessage(msg, toAddr);
				// Transport.send(msg);
				etime = System.currentTimeMillis();
				long sendTime = etime - stime;
	
				System.out.printf(""IsConnected: %d,  Sending: %d\n"", isConnectedTime, sendTime);
			}
	
			bus.close();
		}
	
		public static Message createMessage(String subject, String body)
				throws MessagingException {
			Message msg = new MimeMessage(session);
			msg.setFrom(fromAddr);
			msg.setRecipients(Message.RecipientType.TO, toAddr);
			msg.setSubject(subject);
			msg.setSentDate(new Date());
			msg.setText(body);
			msg.saveChanges();
			return msg;
		}
	
	}
",performance,javamail,,,,,open,0,253,7,"JavaMail SMTPTransport.isConnected() very slow with Exchange server I have a mail daemon to send email notification using JavaMail (1.4.5) with SMTP. It is suggested to use the instance method `sendMessage()` instead of the static `Transport.send()`. So I am testing the connection with `isConnected()` before calling `sendMessage()`. It works fine on my home computer with my ISP's SMTP server. However when I test the same code on my work computer, it is much slower. (My work computer is much faster than my home computer.) So I think the only difference is it talks to company Exchange server. Anyone experienced similar issues?

-Home computer with ISP: `isConnected()` takes 10-100ms

-Work computer with Exchange: `isConnected()` takes 5s.

-If I use the static `Transport.send()` to send message (no need to test connection), it takes about 300ms on my work computer.

Here is the sample code:


	public class TestMail {
		static Session session;
		static InternetAddress fromAddr;
		static InternetAddress[] toAddr;
		static int n = 1;
	
		public static void main(String[] args) throws MessagingException {
			String to = ""yourname@gmail.com"";	//System.getProperty(""user.name"") + ""@aaaa.com""
			String from = ""yourname@gmail.com"";	//System.getProperty(""user.name"") + ""@aaaa.com""
	
			Properties props = new Properties();
			props.put(""mail.smtp.host"", ""mail.optonline.net"");
			// props.put(""mail.smtp.host"", ""mail.aaaa.com"");
	
			session = Session.getInstance(props);
			fromAddr = new InternetAddress(from);
			toAddr = new InternetAddress[] { new InternetAddress(to) };
	
			Transport bus = session.getTransport(""smtp"");
			bus.connect();
	
			String body = ""This is the body of the email.\n"";
	
			for (int i = 1; i <= n; ++i) {
	
				Message msg = createMessage(i + ""th email"", body);
	
				long stime = System.currentTimeMillis();
	
				if (!bus.isConnected()) {
					System.out.println(""Connecting ...."");
					bus.connect();
				}
				long etime = System.currentTimeMillis();
				long isConnectedTime = etime - stime;
	
				stime = etime;
				bus.sendMessage(msg, toAddr);
				// Transport.send(msg);
				etime = System.currentTimeMillis();
				long sendTime = etime - stime;
	
				System.out.printf(""IsConnected: %d,  Sending: %d\n"", isConnectedTime, sendTime);
			}
	
			bus.close();
		}
	
		public static Message createMessage(String subject, String body)
				throws MessagingException {
			Message msg = new MimeMessage(session);
			msg.setFrom(fromAddr);
			msg.setRecipients(Message.RecipientType.TO, toAddr);
			msg.setSubject(subject);
			msg.setSentDate(new Date());
			msg.setText(body);
			msg.saveChanges();
			return msg;
		}
	
	}
",2
6941347,08/04/2011 12:24:52,619968,02/16/2011 15:54:49,189,0,Does running your servers at 100% CPU usage cause any issues or is it just good CPU utilisation?,"Does running your servers at 100% CPU usage cause any issues or is it just good CPU utilisation?

My servers have 8 physical cores constantly running at near 100% for “open hours”/10 hours per pay. 

The program is architectured to running on 8 threads – and it fully uses them. Performance is good but the infrastructure guys are worrying about the “maxed out servers”

I think it's just good use of available resources. What's the point of having lots of core if there not all fully utilisated.",performance,hardware,cpu,cpu-usage,,08/04/2011 22:16:31,off topic,1,85,18,"Does running your servers at 100% CPU usage cause any issues or is it just good CPU utilisation? Does running your servers at 100% CPU usage cause any issues or is it just good CPU utilisation?

My servers have 8 physical cores constantly running at near 100% for “open hours”/10 hours per pay. 

The program is architectured to running on 8 threads – and it fully uses them. Performance is good but the infrastructure guys are worrying about the “maxed out servers”

I think it's just good use of available resources. What's the point of having lots of core if there not all fully utilisated.",4
3097421,06/22/2010 21:50:52,92692,04/19/2009 06:55:33,1696,24,Performance tips for making Visual Studio 2010 faster?,"I don't know if anybody else has had an issue with the performance of Visual Studio 2010, but I close it daily and reopen it, and with an hour it starts to really bog down, and can't even keep up with my typing. Is there some obvious setting I am missing that would help to speed it up?

I am also using ReSharper, but even if I remove that, it only marginally increases the speed.",performance,visual-studio-2010,not-programming-related,visual,,09/12/2011 16:40:34,not constructive,1,74,8,"Performance tips for making Visual Studio 2010 faster? I don't know if anybody else has had an issue with the performance of Visual Studio 2010, but I close it daily and reopen it, and with an hour it starts to really bog down, and can't even keep up with my typing. Is there some obvious setting I am missing that would help to speed it up?

I am also using ReSharper, but even if I remove that, it only marginally increases the speed.",4
8127862,11/14/2011 20:44:47,613617,02/11/2011 20:38:42,29,0,How do you calculate how much faster time X is from time Y in terms of %?,"We're having a bit of a dispute at my office as to how this question should be interpreted. 

**Time 1 = 0.6053 seconds
Time 2 = 1.3477 seconds
 
What percentage faster is time1 to time 2?**

I am of the believe that if you have a time of X seconds. X/2 (half as long) is 100% faster. 

My solution to this problem is calculated as 

(T2/T1)-1 

1.3477/.6053 - 1 = 1.2265

Other people are saying that you should just look at these as numbers and calculate it like 

1- (T1/T2)

1- .6053/1.3477 = .5508 

(the answers above are rounded). 

",performance,math,time,,,11/15/2011 01:27:41,off topic,1,96,17,"How do you calculate how much faster time X is from time Y in terms of %? We're having a bit of a dispute at my office as to how this question should be interpreted. 

**Time 1 = 0.6053 seconds
Time 2 = 1.3477 seconds
 
What percentage faster is time1 to time 2?**

I am of the believe that if you have a time of X seconds. X/2 (half as long) is 100% faster. 

My solution to this problem is calculated as 

(T2/T1)-1 

1.3477/.6053 - 1 = 1.2265

Other people are saying that you should just look at these as numbers and calculate it like 

1- (T1/T2)

1- .6053/1.3477 = .5508 

(the answers above are rounded). 

",3
8616413,12/23/2011 13:00:23,1113437,12/23/2011 12:51:09,1,0,Suddenly Website page loading slow,"I am using Magento and my website. Suddenly my website loading slow. Sometimes it shows connection failed error.

We have not changed anything on query. We doing all caching etc but not able to figure out the issue.

here is the link: http://lootspot.com/id-america-cushi-stripe-soft-foam-pad-for-iphone4-4s-beach-red",performance,magento,loading,,,12/23/2011 22:08:34,off topic,1,41,5,"Suddenly Website page loading slow I am using Magento and my website. Suddenly my website loading slow. Sometimes it shows connection failed error.

We have not changed anything on query. We doing all caching etc but not able to figure out the issue.

here is the link: http://lootspot.com/id-america-cushi-stripe-soft-foam-pad-for-iphone4-4s-beach-red",3
10383248,04/30/2012 11:51:31,1326314,04/11/2012 10:25:34,1,0,best architecture for low latency,"What is the best model for  low-latency?
1) apache-prefork + mod_php - more memory + more cpu
2) apache-worker + mod-php - less memory + more cpu but will memcached, apc and all standard extensions be usable here as I read that PHP is mostly not thread safe.
3) apache-prefork + PHP-FPM -  more memory + less cpu
4) apache-worker + PHP-FPM  - less memory + less cpu



",performance,apache,,,,04/30/2012 18:45:16,not constructive,1,67,5,"best architecture for low latency What is the best model for  low-latency?
1) apache-prefork + mod_php - more memory + more cpu
2) apache-worker + mod-php - less memory + more cpu but will memcached, apc and all standard extensions be usable here as I read that PHP is mostly not thread safe.
3) apache-prefork + PHP-FPM -  more memory + less cpu
4) apache-worker + PHP-FPM  - less memory + less cpu



",2
10050391,04/06/2012 23:33:28,1318401,04/06/2012 23:31:07,1,0,Why does my website wait until everything finish loading before it renders a page,"Everytime I got to my website, I have to wait around 10 sec for the whole page to load before it displays anything.  I could it make it display at least something while it loads to finish the page.",performance,website,render,,,04/07/2012 13:05:56,not a real question,1,40,14,"Why does my website wait until everything finish loading before it renders a page Everytime I got to my website, I have to wait around 10 sec for the whole page to load before it displays anything.  I could it make it display at least something while it loads to finish the page.",3
7350224,09/08/2011 15:03:43,816721,06/27/2011 03:24:50,30,3,Which is better for performance?,"Which is better for performance?

1. through the data (collection of objects) and go once processed in the act. or
2. through the data, process (multiple operations and replacements) and cross it again to view it

I'm taking data from MySQL's business by applying logic to finally generate reports in PDF.",performance,php5,symfony,symfony-1.4,,09/08/2011 22:49:57,not a real question,1,48,5,"Which is better for performance? Which is better for performance?

1. through the data (collection of objects) and go once processed in the act. or
2. through the data, process (multiple operations and replacements) and cross it again to view it

I'm taking data from MySQL's business by applying logic to finally generate reports in PDF.",4
1889026,12/11/2009 16:01:57,1246613,03/03/2012 09:25:03,452,3,Are there applications that can crunch numbers without an OS,"Applications that would boot straight from boot media without needing to be run from an OS. Something like memtest86 but for crunching numbers.

If there are not, why not?

WOuld this lead to better performance, leaving the OS behind?

Or would there be no difference, basically replacing system calls with function calls?",performance,operating-system,system,,,12/12/2009 10:55:25,off topic,1,49,10,"Are there applications that can crunch numbers without an OS Applications that would boot straight from boot media without needing to be run from an OS. Something like memtest86 but for crunching numbers.

If there are not, why not?

WOuld this lead to better performance, leaving the OS behind?

Or would there be no difference, basically replacing system calls with function calls?",3
1997095,01/04/2010 01:33:40,15955,09/17/2008 13:43:34,14294,344,Where does performance still matters?,"Just wondering:

In which fields of programming does performance still matter? I know about some of the classic fields like multimedia/games, embedded-systems and so on, but that can't be all. 

Do you know about any niches where performance is still relevant?
",performance,language-agnostic,,,,01/04/2010 06:05:14,not a real question,1,40,5,"Where does performance still matters? Just wondering:

In which fields of programming does performance still matter? I know about some of the classic fields like multimedia/games, embedded-systems and so on, but that can't be all. 

Do you know about any niches where performance is still relevant?
",2
10355646,04/27/2012 18:03:27,1361739,04/27/2012 17:54:55,1,0,Oracle OWB Cube load SQL tuning,"I have a OWB mapping which takes input from a staging table and add those row to the Cube. The underlying table behind cube is a relational fact table joined with the dimensions using foreign keys. Explain plan behind the query has a rather high cost and the mapping runs for 30 minutes. If you see below, in step 17, the cost goes up to 1,396,573 which is also where nested loops start to appear. Can somebody provide general pointers to tune this query?


Plan
SELECT STATEMENT  ALL_ROWSCost: 1,746,526,275  Bytes: 386,835,904  Cardinality: 464,947  															
	46 NESTED LOOPS OUTER  Cost: 1,746,526,275  Bytes: 386,835,904  Cardinality: 464,947  														
		41 NESTED LOOPS OUTER  Cost: 1,744,200,663  Bytes: 380,791,593  Cardinality: 464,947  													
			37 NESTED LOOPS OUTER  Cost: 1,743,270,415  Bytes: 374,747,282  Cardinality: 464,947  												
				34 NESTED LOOPS OUTER  Cost: 1,740,476,128  Bytes: 368,702,971  Cardinality: 464,947  											
					29 NESTED LOOPS OUTER  Cost: 1,739,545,862  Bytes: 362,658,660  Cardinality: 464,947  										
						25 NESTED LOOPS OUTER  Cost: 1,710,193,475  Bytes: 356,614,349  Cardinality: 464,947  									
							20 NESTED LOOPS OUTER  Cost: 49,230,267  Bytes: 350,570,038  Cardinality: 464,947  								
								17 NESTED LOOPS OUTER  Cost: 1,402,837  Bytes: 344,525,727  Cardinality: 464,947  							
									13 HASH JOIN RIGHT OUTER  Cost: 7,481  Bytes: 338,481,416  Cardinality: 464,947  						
										1 TABLE ACCESS STORAGE FULL TABLE ORION.DIM_HR_SALARY Cost: 6  Bytes: 31  Cardinality: 1  					
										12 HASH JOIN RIGHT OUTER  Cost: 7,472  Bytes: 324,068,059  Cardinality: 464,947  					
											2 TABLE ACCESS STORAGE FULL TABLE ORION.DIM_ADDRESS Cost: 2,050  Bytes: 65  Cardinality: 1  				
											11 HASH JOIN RIGHT OUTER  Cost: 5,420  Bytes: 293,846,504  Cardinality: 464,947  				
												3 TABLE ACCESS STORAGE FULL TABLE ORION.DIM_SESSION Cost: 12  Bytes: 70  Cardinality: 1  			
												10 HASH JOIN RIGHT OUTER  Cost: 5,405  Bytes: 261,300,214  Cardinality: 464,947  			
													4 TABLE ACCESS STORAGE FULL TABLE ORION.DIM_LOCATION Cost: 9  Bytes: 21  Cardinality: 1  		
													9 HASH JOIN RIGHT OUTER  Cost: 5,393  Bytes: 251,536,327  Cardinality: 464,947  		
														5 TABLE ACCESS STORAGE FULL TABLE ORION.DIM_HR_EMPLOYEE Cost: 135  Bytes: 75  Cardinality: 1  	
														8 HASH JOIN RIGHT OUTER  Cost: 5,256  Bytes: 216,665,302  Cardinality: 464,947  	
															6 TABLE ACCESS STORAGE FULL TABLE ORION.DIM_CLASS_INSTRUCTOR Cost: 12  Bytes: 48  Cardinality: 1  
															7 TABLE ACCESS STORAGE FULL TABLE O_STG.FACT_CLASS_INSTRUCTOR_STG2 Cost: 5,241  Bytes: 194,347,846  Cardinality: 464,947  
									16 VIEW SYS. Cost: 3  Bytes: 13  Cardinality: 1  						
										15 TABLE ACCESS BY INDEX ROWID TABLE ORION.DIM_CLASS_ATTRIBUTES Cost: 3  Bytes: 153  Cardinality: 1  					
											14 INDEX RANGE SCAN INDEX ORION.ALL_ATTRIBUTES_IDX_12 Cost: 2  Cardinality: 1  				
								19 VIEW SYS. Cost: 103  Bytes: 13  Cardinality: 1  							
									18 TABLE ACCESS STORAGE FULL TABLE ORION.DIM_COURSE Cost: 103  Bytes: 30  Cardinality: 1  						
							24 PARTITION HASH ALL  Cost: 3,572  Bytes: 13  Cardinality: 1  Partition #: 27  Partitions accessed #1 - #8								
								23 VIEW SYS. Cost: 3,572  Bytes: 13  Cardinality: 1  							
									22 TABLE ACCESS BY LOCAL INDEX ROWID TABLE ORION.DIM_PERSON Cost: 3,572  Bytes: 31  Cardinality: 1  Partition #: 27  Partitions accessed #1 - #8						
										21 INDEX RANGE SCAN INDEX ORION.ALL_ATTRIBUTES_IDX_2_P Cost: 8  Cardinality: 3,661  Partition #: 27  Partitions accessed #1 - #8					
						28 PARTITION RANGE ALL  Cost: 63  Bytes: 13  Cardinality: 1  Partition #: 31  Partitions accessed #1 - #7									
							27 VIEW SYS. Cost: 63  Bytes: 13  Cardinality: 1  								
								26 TABLE ACCESS STORAGE FULL TABLE ORION.DIM_TIME_TERM Cost: 63  Bytes: 27  Cardinality: 1  Partition #: 31  Partitions accessed #1 - #7							
					33 VIEW SYS. Cost: 2  Bytes: 13  Cardinality: 1  										
						32 FILTER  									
							31 TABLE ACCESS BY INDEX ROWID TABLE ORION.DIM_HR_JOB Cost: 2  Bytes: 38  Cardinality: 1  								
								30 INDEX RANGE SCAN INDEX ORION.ALL_ATTRIBUTES_IDX_19 Cost: 1  Cardinality: 1  							
				36 VIEW SYS. Cost: 6  Bytes: 13  Cardinality: 1  											
					35 TABLE ACCESS STORAGE FULL TABLE ORION.DIM_ORG_STRUCTURE Cost: 6  Bytes: 37  Cardinality: 1  										
			40 VIEW SYS. Cost: 2  Bytes: 13  Cardinality: 1  												
				39 TABLE ACCESS BY INDEX ROWID TABLE ORION.DIM_DEMOGRAPHICS Cost: 2  Bytes: 43  Cardinality: 1  											
					38 INDEX RANGE SCAN INDEX ORION.ALL_ATTRIBUTES_IDX Cost: 1  Cardinality: 1  										
		45 VIEW SYS. Cost: 5  Bytes: 13  Cardinality: 1  													
			44 FILTER  												
				43 TABLE ACCESS BY INDEX ROWID TABLE ORION.DIM_DEPARTMENT Cost: 5  Bytes: 31  Cardinality: 1  											
					42 INDEX RANGE SCAN INDEX ORION.ALL_DEPT_ATTRIBUTES_IDX Cost: 1  Cardinality: 3  										",performance,oracle,tuning,sql-tuning,,,open,0,774,6,"Oracle OWB Cube load SQL tuning I have a OWB mapping which takes input from a staging table and add those row to the Cube. The underlying table behind cube is a relational fact table joined with the dimensions using foreign keys. Explain plan behind the query has a rather high cost and the mapping runs for 30 minutes. If you see below, in step 17, the cost goes up to 1,396,573 which is also where nested loops start to appear. Can somebody provide general pointers to tune this query?


Plan
SELECT STATEMENT  ALL_ROWSCost: 1,746,526,275  Bytes: 386,835,904  Cardinality: 464,947  															
	46 NESTED LOOPS OUTER  Cost: 1,746,526,275  Bytes: 386,835,904  Cardinality: 464,947  														
		41 NESTED LOOPS OUTER  Cost: 1,744,200,663  Bytes: 380,791,593  Cardinality: 464,947  													
			37 NESTED LOOPS OUTER  Cost: 1,743,270,415  Bytes: 374,747,282  Cardinality: 464,947  												
				34 NESTED LOOPS OUTER  Cost: 1,740,476,128  Bytes: 368,702,971  Cardinality: 464,947  											
					29 NESTED LOOPS OUTER  Cost: 1,739,545,862  Bytes: 362,658,660  Cardinality: 464,947  										
						25 NESTED LOOPS OUTER  Cost: 1,710,193,475  Bytes: 356,614,349  Cardinality: 464,947  									
							20 NESTED LOOPS OUTER  Cost: 49,230,267  Bytes: 350,570,038  Cardinality: 464,947  								
								17 NESTED LOOPS OUTER  Cost: 1,402,837  Bytes: 344,525,727  Cardinality: 464,947  							
									13 HASH JOIN RIGHT OUTER  Cost: 7,481  Bytes: 338,481,416  Cardinality: 464,947  						
										1 TABLE ACCESS STORAGE FULL TABLE ORION.DIM_HR_SALARY Cost: 6  Bytes: 31  Cardinality: 1  					
										12 HASH JOIN RIGHT OUTER  Cost: 7,472  Bytes: 324,068,059  Cardinality: 464,947  					
											2 TABLE ACCESS STORAGE FULL TABLE ORION.DIM_ADDRESS Cost: 2,050  Bytes: 65  Cardinality: 1  				
											11 HASH JOIN RIGHT OUTER  Cost: 5,420  Bytes: 293,846,504  Cardinality: 464,947  				
												3 TABLE ACCESS STORAGE FULL TABLE ORION.DIM_SESSION Cost: 12  Bytes: 70  Cardinality: 1  			
												10 HASH JOIN RIGHT OUTER  Cost: 5,405  Bytes: 261,300,214  Cardinality: 464,947  			
													4 TABLE ACCESS STORAGE FULL TABLE ORION.DIM_LOCATION Cost: 9  Bytes: 21  Cardinality: 1  		
													9 HASH JOIN RIGHT OUTER  Cost: 5,393  Bytes: 251,536,327  Cardinality: 464,947  		
														5 TABLE ACCESS STORAGE FULL TABLE ORION.DIM_HR_EMPLOYEE Cost: 135  Bytes: 75  Cardinality: 1  	
														8 HASH JOIN RIGHT OUTER  Cost: 5,256  Bytes: 216,665,302  Cardinality: 464,947  	
															6 TABLE ACCESS STORAGE FULL TABLE ORION.DIM_CLASS_INSTRUCTOR Cost: 12  Bytes: 48  Cardinality: 1  
															7 TABLE ACCESS STORAGE FULL TABLE O_STG.FACT_CLASS_INSTRUCTOR_STG2 Cost: 5,241  Bytes: 194,347,846  Cardinality: 464,947  
									16 VIEW SYS. Cost: 3  Bytes: 13  Cardinality: 1  						
										15 TABLE ACCESS BY INDEX ROWID TABLE ORION.DIM_CLASS_ATTRIBUTES Cost: 3  Bytes: 153  Cardinality: 1  					
											14 INDEX RANGE SCAN INDEX ORION.ALL_ATTRIBUTES_IDX_12 Cost: 2  Cardinality: 1  				
								19 VIEW SYS. Cost: 103  Bytes: 13  Cardinality: 1  							
									18 TABLE ACCESS STORAGE FULL TABLE ORION.DIM_COURSE Cost: 103  Bytes: 30  Cardinality: 1  						
							24 PARTITION HASH ALL  Cost: 3,572  Bytes: 13  Cardinality: 1  Partition #: 27  Partitions accessed #1 - #8								
								23 VIEW SYS. Cost: 3,572  Bytes: 13  Cardinality: 1  							
									22 TABLE ACCESS BY LOCAL INDEX ROWID TABLE ORION.DIM_PERSON Cost: 3,572  Bytes: 31  Cardinality: 1  Partition #: 27  Partitions accessed #1 - #8						
										21 INDEX RANGE SCAN INDEX ORION.ALL_ATTRIBUTES_IDX_2_P Cost: 8  Cardinality: 3,661  Partition #: 27  Partitions accessed #1 - #8					
						28 PARTITION RANGE ALL  Cost: 63  Bytes: 13  Cardinality: 1  Partition #: 31  Partitions accessed #1 - #7									
							27 VIEW SYS. Cost: 63  Bytes: 13  Cardinality: 1  								
								26 TABLE ACCESS STORAGE FULL TABLE ORION.DIM_TIME_TERM Cost: 63  Bytes: 27  Cardinality: 1  Partition #: 31  Partitions accessed #1 - #7							
					33 VIEW SYS. Cost: 2  Bytes: 13  Cardinality: 1  										
						32 FILTER  									
							31 TABLE ACCESS BY INDEX ROWID TABLE ORION.DIM_HR_JOB Cost: 2  Bytes: 38  Cardinality: 1  								
								30 INDEX RANGE SCAN INDEX ORION.ALL_ATTRIBUTES_IDX_19 Cost: 1  Cardinality: 1  							
				36 VIEW SYS. Cost: 6  Bytes: 13  Cardinality: 1  											
					35 TABLE ACCESS STORAGE FULL TABLE ORION.DIM_ORG_STRUCTURE Cost: 6  Bytes: 37  Cardinality: 1  										
			40 VIEW SYS. Cost: 2  Bytes: 13  Cardinality: 1  												
				39 TABLE ACCESS BY INDEX ROWID TABLE ORION.DIM_DEMOGRAPHICS Cost: 2  Bytes: 43  Cardinality: 1  											
					38 INDEX RANGE SCAN INDEX ORION.ALL_ATTRIBUTES_IDX Cost: 1  Cardinality: 1  										
		45 VIEW SYS. Cost: 5  Bytes: 13  Cardinality: 1  													
			44 FILTER  												
				43 TABLE ACCESS BY INDEX ROWID TABLE ORION.DIM_DEPARTMENT Cost: 5  Bytes: 31  Cardinality: 1  											
					42 INDEX RANGE SCAN INDEX ORION.ALL_DEPT_ATTRIBUTES_IDX Cost: 1  Cardinality: 3  										",4
9500457,02/29/2012 13:58:36,1145837,01/12/2012 15:28:26,25,0,IIS 7.5 MVC3 Speed Issues,"I am seeing speed issues with my MVC3 application. For testing purposes I created a blank MVC3 application and loaded it into IIS 7.5. The first time I go to the site it takes anywhere between 6 -15 seconds to load the page. I have tried the following link and it does not help. 

http://weblogs.asp.net/scottgu/archive/2009/09/15/auto-start-asp-net-applications-vs-2010-and-net-4-0-series.aspx

I enabled everything as that post says, IIS comes up just fine. But still I have the same issues. I go to the site the first time and it's very slow to respond. After that it's very quick. Then if I leave it alone for about 30 minutes and go back - it's slow again. 

The idle timeout does not seem to do anything either if I set it to 0. ",performance,asp.net-mvc-3,iis-7.5,,,02/29/2012 19:27:38,not a real question,1,126,5,"IIS 7.5 MVC3 Speed Issues I am seeing speed issues with my MVC3 application. For testing purposes I created a blank MVC3 application and loaded it into IIS 7.5. The first time I go to the site it takes anywhere between 6 -15 seconds to load the page. I have tried the following link and it does not help. 

http://weblogs.asp.net/scottgu/archive/2009/09/15/auto-start-asp-net-applications-vs-2010-and-net-4-0-series.aspx

I enabled everything as that post says, IIS comes up just fine. But still I have the same issues. I go to the site the first time and it's very slow to respond. After that it's very quick. Then if I leave it alone for about 30 minutes and go back - it's slow again. 

The idle timeout does not seem to do anything either if I set it to 0. ",3
6306141,06/10/2011 12:08:23,792685,06/10/2011 11:42:44,1,0,Batch I/O performance problem,"I have a c#.net application that has to process a big number of files. The processing of this files is done through command line calls to external tools. Every one of this files goes through 4 processes which create a new file that feeds the next process in a pipeline fashion. As a result, we have 5 files (the original + the result of the 4 processes) per every original file.

I am processing now through a c# generated batch file that I execute through command line. The I/O takes too long. I have been sugested to use a RAM Disk to speed the intermediate result I/O processes. But I have read (in this page) that this is stale technology. 

What solution is there to speed up the processing in this case?

What good freeware solution is there if I want to give the RAM Disk a try? I am using Windows 7.

Thanks
Oscar",performance,command-line,c#-3.0,batch,io,06/10/2011 13:27:56,off topic,1,151,4,"Batch I/O performance problem I have a c#.net application that has to process a big number of files. The processing of this files is done through command line calls to external tools. Every one of this files goes through 4 processes which create a new file that feeds the next process in a pipeline fashion. As a result, we have 5 files (the original + the result of the 4 processes) per every original file.

I am processing now through a c# generated batch file that I execute through command line. The I/O takes too long. I have been sugested to use a RAM Disk to speed the intermediate result I/O processes. But I have read (in this page) that this is stale technology. 

What solution is there to speed up the processing in this case?

What good freeware solution is there if I want to give the RAM Disk a try? I am using Windows 7.

Thanks
Oscar",5
4325630,12/01/2010 14:57:58,41021,11/26/2008 13:16:39,3470,99,Ways to speedup Visual Studio 2010,"There's been a lot of [noise][1] about Visual Studio 2010 being slower than its predecessor, VS 2008.

- [Visual Studio 2010 is a pig][2]
- [Visual Studio 2010 – 10 is the new 6, but not the way they wanted][3]
- [Visual Studio 2010 is very slow on even very fast PC][4]
- [Visual Studio 2010 Ultimate is really slow][5]
- [Visual Studio 2010 Slow to Load][6]
- [Visual Studio 2010 Slow Startup Issues][7]

I was facing the same issue for the last couple weeks, and after an hour of searching around I found a couple of solutions that have made my IDE about **70% faster** on most operations, including code editing.


> **30% speedup**
> 
> Tools > Options -- CHECK ""Show all options""
> 
>  -  IntelliTrace -- DISABLE
>  -  HTML Designer -- DISABLE
> 
> **50% startup speedup**
> 
> Tools > Options
> 
> -  Environment > Add-in/Macros Security -- UNCHECK ""Allow Add-in components to load""
> 
> Tools > Extension Manager 
> 
> -    Uninstall all you don't need.



Restart your IDE after these and you should observe a noticeable speed increase. 

Do you have any more tricks? feel free to add them as answers.


  [1]: http://www.google.com/search?q=Visual+Studio+2010+slow
  [2]: http://www.johndcook.com/blog/2010/04/22/visual-studio-2010-is-a-pig/
  [3]: http://blogs.ijw.co.nz/chris/index.php/2009/07/visual-studio-2010-10-is-the-new-6-but-not-the-way-they-wanted/
  [4]: http://stridersoft.com/blog/archive/2010/10/26/visual-studio-2010-is-very-slow-on-even-very-fast.aspx
  [5]: http://www.devtopics.com/visual-studio-2010-slow-startup/
  [6]: http://stackoverflow.com/questions/3807703/visual-studio-2010-slow-to-load
  [7]: http://jonsblogat.blogspot.com/2010/06/developer-tools-visual-studio-2010-slow.html
",performance,visual-studio-2010,ide,development,,09/12/2011 16:52:00,not constructive,1,214,6,"Ways to speedup Visual Studio 2010 There's been a lot of [noise][1] about Visual Studio 2010 being slower than its predecessor, VS 2008.

- [Visual Studio 2010 is a pig][2]
- [Visual Studio 2010 – 10 is the new 6, but not the way they wanted][3]
- [Visual Studio 2010 is very slow on even very fast PC][4]
- [Visual Studio 2010 Ultimate is really slow][5]
- [Visual Studio 2010 Slow to Load][6]
- [Visual Studio 2010 Slow Startup Issues][7]

I was facing the same issue for the last couple weeks, and after an hour of searching around I found a couple of solutions that have made my IDE about **70% faster** on most operations, including code editing.


> **30% speedup**
> 
> Tools > Options -- CHECK ""Show all options""
> 
>  -  IntelliTrace -- DISABLE
>  -  HTML Designer -- DISABLE
> 
> **50% startup speedup**
> 
> Tools > Options
> 
> -  Environment > Add-in/Macros Security -- UNCHECK ""Allow Add-in components to load""
> 
> Tools > Extension Manager 
> 
> -    Uninstall all you don't need.



Restart your IDE after these and you should observe a noticeable speed increase. 

Do you have any more tricks? feel free to add them as answers.


  [1]: http://www.google.com/search?q=Visual+Studio+2010+slow
  [2]: http://www.johndcook.com/blog/2010/04/22/visual-studio-2010-is-a-pig/
  [3]: http://blogs.ijw.co.nz/chris/index.php/2009/07/visual-studio-2010-10-is-the-new-6-but-not-the-way-they-wanted/
  [4]: http://stridersoft.com/blog/archive/2010/10/26/visual-studio-2010-is-very-slow-on-even-very-fast.aspx
  [5]: http://www.devtopics.com/visual-studio-2010-slow-startup/
  [6]: http://stackoverflow.com/questions/3807703/visual-studio-2010-slow-to-load
  [7]: http://jonsblogat.blogspot.com/2010/06/developer-tools-visual-studio-2010-slow.html
",4
3322072,07/23/2010 20:21:48,114391,05/29/2009 16:18:10,13,0,How most popular media rich websites implement their media library?,"I'm wondering how those popular media rich websites implement their media library. Do they store all the media files in the database? What kind of database do they use? Do they employ other mechanism to boost the performance?

Thanks for any response.",performance,web-applications,multimedia,,,,open,0,41,10,"How most popular media rich websites implement their media library? I'm wondering how those popular media rich websites implement their media library. Do they store all the media files in the database? What kind of database do they use? Do they employ other mechanism to boost the performance?

Thanks for any response.",3
10607750,05/15/2012 19:46:15,1250713,03/05/2012 20:20:40,8,1,Tools to measure MPI communication costs,"I'm using MPI and I want to measure the communication costs, so that I can then compare them to the 'processing' costs, e.g., how much time do I need to scatter a list through n processes and then compare it to how much time I need to sort it. 

Does anyone know any tools to measure this communication costs? (the scatter for example)

Is there anything to make measurements on MPI communication costs like there is, for example, PAPI to analyse code performance?

Thanks in advance!",performance,communication,mpi,,,06/19/2012 11:51:02,not constructive,1,84,6,"Tools to measure MPI communication costs I'm using MPI and I want to measure the communication costs, so that I can then compare them to the 'processing' costs, e.g., how much time do I need to scatter a list through n processes and then compare it to how much time I need to sort it. 

Does anyone know any tools to measure this communication costs? (the scatter for example)

Is there anything to make measurements on MPI communication costs like there is, for example, PAPI to analyse code performance?

Thanks in advance!",3
3572909,08/26/2010 07:29:54,64160,02/09/2009 12:39:36,257,5,IIS performance improvements,"What can be done to improve performance in IIS? When i deploy my webapplication to my local IIS machine it goes much slower than when i run the solution in visual studio without debugging. The difference is remarkable, like double as fast.",performance,iis,,,,,open,0,42,3,"IIS performance improvements What can be done to improve performance in IIS? When i deploy my webapplication to my local IIS machine it goes much slower than when i run the solution in visual studio without debugging. The difference is remarkable, like double as fast.",2
10866561,06/02/2012 22:58:24,892584,08/12/2011 23:22:34,127,7,Cocoa OpenGL unlocked framerate,"Quick question.

I want to be able to run my game without a locked framerate (currently 60 fps). The only way that I have found to run the animation is with a NSTimer. Is there a way to have an unrestricted framerate in Cocoa. If so, a link or a code snippet would help greatly.

Thanks",performance,osx,cocoa,opengl,fps,,open,0,54,4,"Cocoa OpenGL unlocked framerate Quick question.

I want to be able to run my game without a locked framerate (currently 60 fps). The only way that I have found to run the animation is with a NSTimer. Is there a way to have an unrestricted framerate in Cocoa. If so, a link or a code snippet would help greatly.

Thanks",5
4677298,01/13/2011 06:08:08,313421,04/10/2010 10:36:10,1211,106,Performance analysis of a HttpHandler,"`Visual Studio >> Debug menu >> Start performance analysis` doesn't work with a `HttpHandler` because it cannot be run by itself. The error is something like this:

getting asp.net information failed. 'http://localhost/myprojrct/VSEnterpriseHelper.axd' returned error (500 internal server error)

Also attaching to `w3wp.exe` via `profiler` only profiles CPU.

How is it possible to run performance analysis on a `HttpHandler` asp.net project (I don't use `ashx` file and there's no asp.net page, just a class which implements `IHttpHandler`)",performance,visual-studio-2010,httphandler,analysis,,,open,0,73,5,"Performance analysis of a HttpHandler `Visual Studio >> Debug menu >> Start performance analysis` doesn't work with a `HttpHandler` because it cannot be run by itself. The error is something like this:

getting asp.net information failed. 'http://localhost/myprojrct/VSEnterpriseHelper.axd' returned error (500 internal server error)

Also attaching to `w3wp.exe` via `profiler` only profiles CPU.

How is it possible to run performance analysis on a `HttpHandler` asp.net project (I don't use `ashx` file and there's no asp.net page, just a class which implements `IHttpHandler`)",4
6938452,08/04/2011 08:38:45,871202,07/31/2011 02:04:31,119,0,"Speed of ""disk"" access on virtual machines?","I've been playing with the free micro instance that amazon is offering, and it is quite cool how they abstract away the fact that you don't _actually_ have a disk; for all intents and purposes the local storage and EBS behave as if you really had one.

However, i have no idea how fast they are, even in rough order of magnitude. The list i'm familiar with looks something like:

- Direct memory accesses (nanosecond latency?) 
- Database accesses (???)
- SSD accesses (???)
- Hard Disk accesses are really slow (~10ms?), 
- Network requests (>500ms!!!)

Where does the sort of Virtual disk that Amazon is providing fall on this line? They probably vary wildly, but even a rough order-of-magnitude number would be helpful.",performance,memory,amazon-ec2,virtualization,,11/13/2011 11:33:09,off topic,1,119,7,"Speed of ""disk"" access on virtual machines? I've been playing with the free micro instance that amazon is offering, and it is quite cool how they abstract away the fact that you don't _actually_ have a disk; for all intents and purposes the local storage and EBS behave as if you really had one.

However, i have no idea how fast they are, even in rough order of magnitude. The list i'm familiar with looks something like:

- Direct memory accesses (nanosecond latency?) 
- Database accesses (???)
- SSD accesses (???)
- Hard Disk accesses are really slow (~10ms?), 
- Network requests (>500ms!!!)

Where does the sort of Virtual disk that Amazon is providing fall on this line? They probably vary wildly, but even a rough order-of-magnitude number would be helpful.",4
9859904,03/25/2012 11:52:17,445114,09/11/2010 14:05:11,304,17,NVIDIA GTX 570 vs. 590 for Matlab,"I'm considering buying a GPU to accelerate my Matlab code performance.

My final choices are GTX 570 or GTX 590.

Is the GTX 590 performance (for computing, not games) worth the extra cost?

My main use will be for optimization problems (gradient descent, etc.).

Thanks.",performance,matlab,gpu,,,03/25/2012 12:27:37,not constructive,1,41,7,"NVIDIA GTX 570 vs. 590 for Matlab I'm considering buying a GPU to accelerate my Matlab code performance.

My final choices are GTX 570 or GTX 590.

Is the GTX 590 performance (for computing, not games) worth the extra cost?

My main use will be for optimization problems (gradient descent, etc.).

Thanks.",3
7991120,11/03/2011 06:42:06,295541,03/17/2010 09:59:46,117,9,Visual Studio 2010 with SSD Performance,"I have a Core i5 laptop with 4Gbyte RAM and I use Windows 7 32 bit operation system.

I would like to improve my laptop performance, but unfortunately I can't upgrade Win 7 to 64 bit.

My question is, if I change the HDD to an SSD drive (Sata III, read, and write speed is over than 500Mbyte/sec), this changes will improve the Visual Studio 2010 performance?

Anybody has any experience about it?

I read some article about it, and some people say it would not be a big improvement, I should change the CPU speed, but some people say the compiling of Visual Studio is 2 times faster on SSD than HDD.

Thanks advanced!

l.",performance,visual-studio-2010,ssd,,,11/03/2011 09:56:06,off topic,1,110,6,"Visual Studio 2010 with SSD Performance I have a Core i5 laptop with 4Gbyte RAM and I use Windows 7 32 bit operation system.

I would like to improve my laptop performance, but unfortunately I can't upgrade Win 7 to 64 bit.

My question is, if I change the HDD to an SSD drive (Sata III, read, and write speed is over than 500Mbyte/sec), this changes will improve the Visual Studio 2010 performance?

Anybody has any experience about it?

I read some article about it, and some people say it would not be a big improvement, I should change the CPU speed, but some people say the compiling of Visual Studio is 2 times faster on SSD than HDD.

Thanks advanced!

l.",3
4169650,11/12/2010 22:29:47,506333,11/12/2010 22:29:47,1,0,How to performance test an asp.net application,Performance testing best practises for an ap .net pplication,performance,,,,,11/13/2010 17:14:06,not a real question,1,9,7,How to performance test an asp.net application Performance testing best practises for an ap .net pplication,1
7748324,10/13/2011 01:04:48,40078,11/23/2008 14:44:08,2367,63,Xen and KVM kernel compile slowdown,"In [this slide deck on Xen vs KVM](http://www.xen.org/files/xensummitboston08/Deshane-XenSummit08-Slides.pdf), the benchmarks indicate that CPU and disk is nearly as fast under virtualization (~10% slowdown). Yet virtualization slows down a kernel compile by more than a factor of 2. What causes this?",performance,virtualization,,,,10/14/2011 11:08:28,off topic,1,40,6,"Xen and KVM kernel compile slowdown In [this slide deck on Xen vs KVM](http://www.xen.org/files/xensummitboston08/Deshane-XenSummit08-Slides.pdf), the benchmarks indicate that CPU and disk is nearly as fast under virtualization (~10% slowdown). Yet virtualization slows down a kernel compile by more than a factor of 2. What causes this?",2
8183176,11/18/2011 13:31:22,1053885,11/18/2011 13:26:48,1,0,Need help to speed up my website,"I'm developing a website in PHP.Seems it contains more images and the running time of the site is very slow, 
     Is any other way to speed up my website?


Thanks in advance,
Vinoth.",performance,,,,,11/18/2011 17:00:38,not a real question,1,36,7,"Need help to speed up my website I'm developing a website in PHP.Seems it contains more images and the running time of the site is very slow, 
     Is any other way to speed up my website?


Thanks in advance,
Vinoth.",1
5929668,05/08/2011 19:13:36,744145,05/08/2011 19:13:36,1,0,What tanenbaum rules refer to the OSI layers?,"What tanenbaum performance rules can help the data link, network and transport layers of the osi model? 

Im trying to revise for an exam i have in 2 weeks but dont know the answer to this",performance,osi,,,,05/09/2011 03:23:45,off topic,1,36,8,"What tanenbaum rules refer to the OSI layers? What tanenbaum performance rules can help the data link, network and transport layers of the osi model? 

Im trying to revise for an exam i have in 2 weeks but dont know the answer to this",2
11348654,07/05/2012 16:31:07,1261203,03/10/2012 15:59:05,110,11,choose hosting based on website statistics,"I have Google Analytics data of a Website that is currently running on a small server that is also hosting several other sites, and the Website is pretty slow so we want to host it somewhere else, but i need some way of calculating which hosting method is the best and cheapest (vServer/dedicated Server/cloudhosting).
I need something like KPI's for Webhosting to choose the appropriate method based on pageviews/user average, pageviews/user peak, traffic etc.
I did some research myself, also got a book on ""Computer Systems Performance Evaluation and Prediction"" but i still cant find anything helpful...
I was wondering if any of you guys did this before and could help me, i dont want the entire solution but just point me in the right direction ^^

Thanks in advance,
and sorry for my bad english
 ",performance,webserver,hosting,,,07/07/2012 11:13:45,off topic,1,132,6,"choose hosting based on website statistics I have Google Analytics data of a Website that is currently running on a small server that is also hosting several other sites, and the Website is pretty slow so we want to host it somewhere else, but i need some way of calculating which hosting method is the best and cheapest (vServer/dedicated Server/cloudhosting).
I need something like KPI's for Webhosting to choose the appropriate method based on pageviews/user average, pageviews/user peak, traffic etc.
I did some research myself, also got a book on ""Computer Systems Performance Evaluation and Prediction"" but i still cant find anything helpful...
I was wondering if any of you guys did this before and could help me, i dont want the entire solution but just point me in the right direction ^^

Thanks in advance,
and sorry for my bad english
 ",3
10692890,05/21/2012 21:29:02,1408841,05/21/2012 21:25:42,1,0,Does SSD improve performance regardless of buffering,Would there ever be an instance where removing the file buffering aspect of a program result in higher performance when using an SSD (solid state drive)?,performance,ssd,,,,05/23/2012 03:44:53,off topic,1,26,7,Does SSD improve performance regardless of buffering Would there ever be an instance where removing the file buffering aspect of a program result in higher performance when using an SSD (solid state drive)?,2
880826,05/19/2009 03:27:39,21957,09/25/2008 01:33:13,179,23,How often is the performance of a programming language a significant issue?,"It seems that I often hear people criticize certain programming languages because they ""have poor performance"", or because some other language is ""faster"" in general (not necessarily for a specific application). However, my experience and education have taught me that anytime you have a performance problem, at least one of the following is probably happening:

 1. The bottleneck isn't in the CPU, it's in some other device, such as the network or the hard drive.
 2. The poor performance is caused by your algorithms, not by the language you're using.

My general impression is that the speed of a programming language itself is all but irrelevant in the vast majority of cases, with exceptions for serious data processing problems. Even in those cases, I believe you could use a hybrid approach and use a lower-level language only for the CPU-intensive pieces so that you wouldn't lose the benefits of the more abstract language altogether.

Do you agree? Is programming language speed insignificant most of the time, or do the critics have a right to  point out language performance issues?

I hope this question isn't too subjective, but it seems to me that there should be a relatively objective answer to this.",performance,programming-languages,,,,05/19/2009 23:09:07,not constructive,1,199,12,"How often is the performance of a programming language a significant issue? It seems that I often hear people criticize certain programming languages because they ""have poor performance"", or because some other language is ""faster"" in general (not necessarily for a specific application). However, my experience and education have taught me that anytime you have a performance problem, at least one of the following is probably happening:

 1. The bottleneck isn't in the CPU, it's in some other device, such as the network or the hard drive.
 2. The poor performance is caused by your algorithms, not by the language you're using.

My general impression is that the speed of a programming language itself is all but irrelevant in the vast majority of cases, with exceptions for serious data processing problems. Even in those cases, I believe you could use a hybrid approach and use a lower-level language only for the CPU-intensive pieces so that you wouldn't lose the benefits of the more abstract language altogether.

Do you agree? Is programming language speed insignificant most of the time, or do the critics have a right to  point out language performance issues?

I hope this question isn't too subjective, but it seems to me that there should be a relatively objective answer to this.",2
11155390,06/22/2012 11:33:15,549240,12/20/2010 22:17:18,57,2,Is there sense to separate one table to many,"This is performance question.
Is split data to many data tables give you performance advantage  upon just using single data table indexed by property, on the base of which data could be separated to many data tables?

Thanks.",performance,mongodb,,,,,open,0,37,9,"Is there sense to separate one table to many This is performance question.
Is split data to many data tables give you performance advantage  upon just using single data table indexed by property, on the base of which data could be separated to many data tables?

Thanks.",2
182600,10/08/2008 12:59:56,1533,08/16/2008 12:20:13,510,37,Should one use < or <= in a for loop,"If you had to iterate through a loop 7 times, would you use:

    for (int i = 0; i < 7; i++)

or:

    for (int i = 0; i <= 6; i++)

There are two considerations:

  - performance
  - readability 

For performance I'm assuming Java or C#.  Does it matter if ""less than"" or ""less than or equal to"" is used?  If you have insight for a different language, please indicate which.

For readability I'm assuming 0-based arrays.",performance,readability,conventions,,,,open,0,85,10,"Should one use < or <= in a for loop If you had to iterate through a loop 7 times, would you use:

    for (int i = 0; i < 7; i++)

or:

    for (int i = 0; i <= 6; i++)

There are two considerations:

  - performance
  - readability 

For performance I'm assuming Java or C#.  Does it matter if ""less than"" or ""less than or equal to"" is used?  If you have insight for a different language, please indicate which.

For readability I'm assuming 0-based arrays.",3
7664941,10/05/2011 17:03:11,980661,10/05/2011 15:16:34,8,0,how to schedule a task (in unix) to start running when the cpu is free?,I have a program to run on a machine that is shared with many other people. I want to time my job and get performance summaries etc so it's important to run the job when the machine is not shared with other jobs. Is there a way to schedule my jobs to start running once the cpu is not occupied with another job? Are there any unix commands for something like this? Currently I'm having to constantly check to see if the machine is free before running my job so something like this would save me a lot of time! ^^,performance,unix,scheduled-tasks,,,10/06/2011 01:40:21,off topic,1,101,15,how to schedule a task (in unix) to start running when the cpu is free? I have a program to run on a machine that is shared with many other people. I want to time my job and get performance summaries etc so it's important to run the job when the machine is not shared with other jobs. Is there a way to schedule my jobs to start running once the cpu is not occupied with another job? Are there any unix commands for something like this? Currently I'm having to constantly check to see if the machine is free before running my job so something like this would save me a lot of time! ^^,3
8026250,11/06/2011 09:47:27,1032021,11/06/2011 09:40:46,1,0,COM performance vs. DLL?,I use an application which has provides DLL and com interfaces. Cloud anyone explain Which of them is faster in visual studio? ,performance,dll,com,,,11/06/2011 10:59:11,not constructive,1,23,4,COM performance vs. DLL? I use an application which has provides DLL and com interfaces. Cloud anyone explain Which of them is faster in visual studio? ,3
6255197,06/06/2011 16:37:21,783280,06/03/2011 19:44:11,138,6,Is there a difference in speed between programming languages?,I mean is there such a huge difference and if so what are the causes for it and which language is the fastest?,performance,programming-languages,language,,,06/06/2011 18:48:30,not a real question,1,23,9,Is there a difference in speed between programming languages? I mean is there such a huge difference and if so what are the causes for it and which language is the fastest?,3
11066148,06/16/2012 18:49:13,1331402,04/13/2012 11:05:23,33,1,Which is more efficient coding?,"I have a bible project and I need some inputs from experts to determine which is faster and more efficient in displaying results in the page.

My Bible table contains only 3 fields at the moment? id, verse_code, and verse_content.

For me to be able to display appropriate Bible Book and verse in word (ex. Genesis 1:1), I am doing switch-case statement on verse_code that includes 66 cases something like...

    select (verse_code)
    {
    case 1: echo ""Genesis"";
    case 2: echo ""Exodus"";
    ...
    case 66: echo ""Revelation"";
    }

I can keep it this way or I can add new column **book_name** and input book name of each corresponding verse. This will be equivalent to 31,103 rows.

Performance wise, which is faster between these 2 approach. Shall I keep my case statement of shall I input the book name for 31,103 rows?

Thanks",performance,,,,,,open,0,157,5,"Which is more efficient coding? I have a bible project and I need some inputs from experts to determine which is faster and more efficient in displaying results in the page.

My Bible table contains only 3 fields at the moment? id, verse_code, and verse_content.

For me to be able to display appropriate Bible Book and verse in word (ex. Genesis 1:1), I am doing switch-case statement on verse_code that includes 66 cases something like...

    select (verse_code)
    {
    case 1: echo ""Genesis"";
    case 2: echo ""Exodus"";
    ...
    case 66: echo ""Revelation"";
    }

I can keep it this way or I can add new column **book_name** and input book name of each corresponding verse. This will be equivalent to 31,103 rows.

Performance wise, which is faster between these 2 approach. Shall I keep my case statement of shall I input the book name for 31,103 rows?

Thanks",1
7779701,10/15/2011 18:15:38,827297,07/03/2011 21:58:40,375,14,Tips and tricks on improving Fortran code performance,"As part of my Ph.D. research, I am working on development of numerical models of atmosphere and ocean circulation. These involve numerically solving systems of PDE's on the order of ~10^6 grid points, over ~10^4 time steps. Thus, a typical model simulation takes hours to a few days to complete when run in MPI on dozens of CPUs. Naturally, improving model efficiency as much as possible is important, while making sure the results are byte-to-byte identical.

While I feel quite comfortable with my Fortran programming, and am aware of quite some tricks to make code more efficient, I feel like there is still space to improve, and tricks that I am not aware of.

Currently, I make sure I use as few divisions as possible, and try not to use literal constants (I was taught to do this from very early on, e.g. use half=0.5 instead of 0.5 in actual computations), use as few transcendental functions as possible etc.

What other performance sensitive factors are there? At the moment, I am wondering about a few:

1) Does the order of mathematical operations matter? For example if I have:

    a=1E-7 ; b=2E4 ; c=3E13
    d=a*b*c

would d evaluate with different efficiency based on the order of multiplication? Nowadays, this must be compiler specific, but is there a straight answer? I notice d getting (slightly) different value based on the order (precision limit), but will this impact the efficiency or not?

2) Passing lots (e.g. dozens) of arrays as arguments to a subroutine versus accessing these arrays from a module within the subroutine? 

3) Fortran 95 constructs (FORALL and WHERE) versus DO and IF? I know that these mattered back in the 90's when code vectorization was a big thing, but is there any difference now with modern compilers being able to vectorize explicit DO loops? (I am using PGI, Intel, and IBM compilers in my work)

4) Raising a number to an integer power versus multiplication? E.g.:

    b=a**4

or

    b=a*a*a*a

I have been taught to always use the latter where possible. Does this affect efficiency and/or precision? (probably compiler dependent as well)

Please discuss and/or add any tricks and tips that you know about improving Fortran code efficiency. What else is out there? If you know anything specific to what each of the compilers above do related to this question, please include that as well.

Thanks!    ",performance,fortran,hpc,,,10/16/2011 08:56:14,not constructive,1,400,8,"Tips and tricks on improving Fortran code performance As part of my Ph.D. research, I am working on development of numerical models of atmosphere and ocean circulation. These involve numerically solving systems of PDE's on the order of ~10^6 grid points, over ~10^4 time steps. Thus, a typical model simulation takes hours to a few days to complete when run in MPI on dozens of CPUs. Naturally, improving model efficiency as much as possible is important, while making sure the results are byte-to-byte identical.

While I feel quite comfortable with my Fortran programming, and am aware of quite some tricks to make code more efficient, I feel like there is still space to improve, and tricks that I am not aware of.

Currently, I make sure I use as few divisions as possible, and try not to use literal constants (I was taught to do this from very early on, e.g. use half=0.5 instead of 0.5 in actual computations), use as few transcendental functions as possible etc.

What other performance sensitive factors are there? At the moment, I am wondering about a few:

1) Does the order of mathematical operations matter? For example if I have:

    a=1E-7 ; b=2E4 ; c=3E13
    d=a*b*c

would d evaluate with different efficiency based on the order of multiplication? Nowadays, this must be compiler specific, but is there a straight answer? I notice d getting (slightly) different value based on the order (precision limit), but will this impact the efficiency or not?

2) Passing lots (e.g. dozens) of arrays as arguments to a subroutine versus accessing these arrays from a module within the subroutine? 

3) Fortran 95 constructs (FORALL and WHERE) versus DO and IF? I know that these mattered back in the 90's when code vectorization was a big thing, but is there any difference now with modern compilers being able to vectorize explicit DO loops? (I am using PGI, Intel, and IBM compilers in my work)

4) Raising a number to an integer power versus multiplication? E.g.:

    b=a**4

or

    b=a*a*a*a

I have been taught to always use the latter where possible. Does this affect efficiency and/or precision? (probably compiler dependent as well)

Please discuss and/or add any tricks and tips that you know about improving Fortran code efficiency. What else is out there? If you know anything specific to what each of the compilers above do related to this question, please include that as well.

Thanks!    ",3
3082379,06/21/2010 05:48:15,322866,04/22/2010 02:37:49,240,14,Is triple buffering really a free performance boost?,"So I've been reading a lot about openGL and gpus and graphics in general, and triple buffering is semi-frequently mentioned as giving a free performance boost. I know why it helps; in effect, you can do v-sync without being limited to a framerate choice of 60 or 30 or 20 or 15 or etc, so you can actually achieve, say, 55fps. But is there actually a cost to this? Intuitively, I'd expect triple buffering to delay output by one frame, adding a very small lag to everything.

Also, what should I tag this?",performance,,,,,,open,0,92,8,"Is triple buffering really a free performance boost? So I've been reading a lot about openGL and gpus and graphics in general, and triple buffering is semi-frequently mentioned as giving a free performance boost. I know why it helps; in effect, you can do v-sync without being limited to a framerate choice of 60 or 30 or 20 or 15 or etc, so you can actually achieve, say, 55fps. But is there actually a cost to this? Intuitively, I'd expect triple buffering to delay output by one frame, adding a very small lag to everything.

Also, what should I tag this?",1
8559233,12/19/2011 09:21:25,1052057,11/17/2011 14:59:18,11,0,How do i find Disk IO rate and IOPS for Ubuntu machines?,"How do i find disk IO rate and IOPS of my disk ?
Are there any benchmarks or tools that will be of some use to me ?


Arun",performance,ubuntu,io,benchmarking,,12/19/2011 12:51:54,off topic,1,27,12,"How do i find Disk IO rate and IOPS for Ubuntu machines? How do i find disk IO rate and IOPS of my disk ?
Are there any benchmarks or tools that will be of some use to me ?


Arun",4
9071828,01/30/2012 22:33:27,455087,09/22/2010 13:29:15,11,0,Mac Mini slows down without any obvious reason,"I have a problem where I can't find any reason..

At home I have a MacBook 13"" with an 2 GHz Intel Core 2 Duo and 2 GB 1067 MHz DDR3 and
a MacMini 2,26 GHz Intel Core 2 Duo and 2 GB 1067 MHz DDR3. They are connected over a n-WLAN. On my MacMini I have several HD-Movies (mkv) which I want to watch on my MacBook. I use the Finder and the afp protocol to connect to my MacMini and start the movies with the VLC player (tried Plex too). Most of time there is no problem. The movies are played without any lags. 
But some days the movie hangs and I have no chance to watch it normally. When I connect to the MacMini via Screensharing there are performance problems too. I takes a lot of time before I receive feedback for my entered data. 

I checked the client (MacBook) and server (MacMini) network traffic but there are no noticeable problems. If I connect to a website from my client it builds the page as fast as usual. So I don't think it's an network problem.

One time I saw that the available memory on the server was just 10MB of 2GB. But another day it was about 500MB. 

The free hard disk space of the server is about 23GB.
The cpu is just about 3%

I rebooted the server but there was no improvement. 

I know I didn't provide much informations but perhaps you can give me some hints what to check next?

Ben",performance,osx,networking,macbook,movies,01/30/2012 22:37:20,off topic,1,252,8,"Mac Mini slows down without any obvious reason I have a problem where I can't find any reason..

At home I have a MacBook 13"" with an 2 GHz Intel Core 2 Duo and 2 GB 1067 MHz DDR3 and
a MacMini 2,26 GHz Intel Core 2 Duo and 2 GB 1067 MHz DDR3. They are connected over a n-WLAN. On my MacMini I have several HD-Movies (mkv) which I want to watch on my MacBook. I use the Finder and the afp protocol to connect to my MacMini and start the movies with the VLC player (tried Plex too). Most of time there is no problem. The movies are played without any lags. 
But some days the movie hangs and I have no chance to watch it normally. When I connect to the MacMini via Screensharing there are performance problems too. I takes a lot of time before I receive feedback for my entered data. 

I checked the client (MacBook) and server (MacMini) network traffic but there are no noticeable problems. If I connect to a website from my client it builds the page as fast as usual. So I don't think it's an network problem.

One time I saw that the available memory on the server was just 10MB of 2GB. But another day it was about 500MB. 

The free hard disk space of the server is about 23GB.
The cpu is just about 3%

I rebooted the server but there was no improvement. 

I know I didn't provide much informations but perhaps you can give me some hints what to check next?

Ben",5
8188064,11/18/2011 19:32:34,1054487,11/18/2011 19:27:12,1,0,Pregunta con relación a la identificación,"quiero saber como lograr identificar qué página de fans.

El ejemplo es sencillo: Yo instalé a la aplicación en en una página de fans cualquiera, en el momento de que el usuario inicio la aplicación mediante la fanspage, esta me mostrará una información según sea la página. hay alguna manera de indentificarlos por id?

Gracias.",performance,facebook,,,,11/18/2011 20:32:46,not a real question,1,53,6,"Pregunta con relación a la identificación quiero saber como lograr identificar qué página de fans.

El ejemplo es sencillo: Yo instalé a la aplicación en en una página de fans cualquiera, en el momento de que el usuario inicio la aplicación mediante la fanspage, esta me mostrará una información según sea la página. hay alguna manera de indentificarlos por id?

Gracias.",2
4180580,11/15/2010 00:30:55,143373,07/23/2009 03:21:02,631,7,Using Windows Performance Tools for tracing an application running on a remote computer,"How can I trace a system responsiveness and application resource usage issue on a remote computer? I deployed my application on my client's computer running Windows XP sp3 pro, but its running very slowly and I want to diagnose it since it runs well on my Windows 7 machine. How can i fo it remotely and if thats too difficult how can i do it onsite?",performance,trace,,,,,open,0,66,13,"Using Windows Performance Tools for tracing an application running on a remote computer How can I trace a system responsiveness and application resource usage issue on a remote computer? I deployed my application on my client's computer running Windows XP sp3 pro, but its running very slowly and I want to diagnose it since it runs well on my Windows 7 machine. How can i fo it remotely and if thats too difficult how can i do it onsite?",2
295065,11/17/2008 07:37:05,38174,11/17/2008 07:37:05,1,0,"Is there a ""result server"" that store performance test result, with good analysis tool?","I need a ""result server"" that can store my raw test data (server cpu, memory usage, etc), create report template (e.g. user number vs response time), generate live-reports base on templates, and perform complicated analysis (such as 70/80/90 percentile charts, data distribution comparison between test, etc.)",performance,results,report,analysis,testing,,open,0,46,14,"Is there a ""result server"" that store performance test result, with good analysis tool? I need a ""result server"" that can store my raw test data (server cpu, memory usage, etc), create report template (e.g. user number vs response time), generate live-reports base on templates, and perform complicated analysis (such as 70/80/90 percentile charts, data distribution comparison between test, etc.)",5
8320658,11/30/2011 03:08:17,496067,11/03/2010 14:50:47,526,13,Yammer's Experience w/ Scala,"I have been advocating using Scala at my company.  One of my co-workers forwarded me this link tonight

http://blog.joda.org/2011/11/real-life-scala-feedback-from-yammer.html

I was hoping to get some constructive feedback from the SO community about this.  I don't want this to turn into a flaming thread, but if there are legitimate concerns floating around out there I think it would be beneficial to discuss possible reasons and best practices that can avoid others falling into such traps.

I will say that I have been loving Scala and have not run into any of the problems that are mentioned.  My application is also not very hashmap intensive, which appears to be where a fair number of their problems came from.
",performance,scala,build-process,scalability,usability,11/30/2011 04:14:34,not constructive,1,116,4,"Yammer's Experience w/ Scala I have been advocating using Scala at my company.  One of my co-workers forwarded me this link tonight

http://blog.joda.org/2011/11/real-life-scala-feedback-from-yammer.html

I was hoping to get some constructive feedback from the SO community about this.  I don't want this to turn into a flaming thread, but if there are legitimate concerns floating around out there I think it would be beneficial to discuss possible reasons and best practices that can avoid others falling into such traps.

I will say that I have been loving Scala and have not run into any of the problems that are mentioned.  My application is also not very hashmap intensive, which appears to be where a fair number of their problems came from.
",5
11278173,06/30/2012 22:32:31,1084353,12/06/2011 20:49:29,64,0,Get the ith bit - is % or & faster?,"I'd like to know which of the following is faster for getting the i'th rightmost bit of integer x, where i starts with 0:

    x & (1 << i)
    x >> i % 2

Also curious about why one is faster.

Thanks!",performance,math,assembly,bit-manipulation,bit,07/01/2012 17:57:13,not a real question,1,46,10,"Get the ith bit - is % or & faster? I'd like to know which of the following is faster for getting the i'th rightmost bit of integer x, where i starts with 0:

    x & (1 << i)
    x >> i % 2

Also curious about why one is faster.

Thanks!",5
4783527,01/24/2011 15:06:58,587673,01/24/2011 15:06:58,1,0,how to Improve performance of application,"I am working on web based application. in this application, the web server calling one .vbs file in batch server which is in same network using Queueuing(QCP). that .vbs file calling so many files which have some busness logic. 

The whole process taking log time to complete.

please give an tip to improve performance of this task.

Your help is appreciated.

Thanks",performance,application,,,,01/24/2011 20:33:50,not a real question,1,59,6,"how to Improve performance of application I am working on web based application. in this application, the web server calling one .vbs file in batch server which is in same network using Queueuing(QCP). that .vbs file calling so many files which have some busness logic. 

The whole process taking log time to complete.

please give an tip to improve performance of this task.

Your help is appreciated.

Thanks",2
7649000,10/04/2011 13:53:33,351982,05/27/2010 13:02:29,491,26,tablediff takes a very long time to execute,"I'm using tablediff.exe tool to compare tables and generate a change script on a remote SQL Server but it takes a very very long time to execute(after more than 1.5hour I gave up). 

I've profiled the SQL Server and it seems to execute the queries strangely slower then I expected.

The tables have about 2 million records, but when I executed this on my local machine it look a lot less(about 2-3 minutes).

I'm trying to find out what the reasons for this might be and if there's any way to speed this up?",performance,sql-server-2008,compare,,,12/27/2011 16:05:58,too localized,1,92,8,"tablediff takes a very long time to execute I'm using tablediff.exe tool to compare tables and generate a change script on a remote SQL Server but it takes a very very long time to execute(after more than 1.5hour I gave up). 

I've profiled the SQL Server and it seems to execute the queries strangely slower then I expected.

The tables have about 2 million records, but when I executed this on my local machine it look a lot less(about 2-3 minutes).

I'm trying to find out what the reasons for this might be and if there's any way to speed this up?",3
10880418,06/04/2012 11:21:57,944430,09/14/2011 10:56:19,76,0,Comparing Lua with Mono,"I wanted to do some research but i could not find any information about this topic.

The only information that i found -> http://shootout.alioth.debian.org/u32/benchmark.php?test=all&lang=csharp&lang2=lua

And I'm not sure how i can interpret this chart.
If I'm right it basicly says that Lua is 10times faster than Mono.
But Mono's memory usage is  ~8 times better than Lua.

What does this say about perfomance ?



Comparing **Lua** with **Mono** -> *Advantages/Disadvantages*

",performance,mono,lua,,,06/04/2012 11:43:02,not constructive,1,65,4,"Comparing Lua with Mono I wanted to do some research but i could not find any information about this topic.

The only information that i found -> http://shootout.alioth.debian.org/u32/benchmark.php?test=all&lang=csharp&lang2=lua

And I'm not sure how i can interpret this chart.
If I'm right it basicly says that Lua is 10times faster than Mono.
But Mono's memory usage is  ~8 times better than Lua.

What does this say about perfomance ?



Comparing **Lua** with **Mono** -> *Advantages/Disadvantages*

",3
4150750,11/11/2010 01:41:54,386701,07/08/2010 13:36:20,82,3,Is it unnecessary to use ehcache over mongodb?,"1 for indexed object, such as user indexed by Id username
2 simple query, such as search by page, username
3 complex query, tag, geolocation query. 

Any suggestions?",performance,mongodb,ehcache,,,,open,0,26,8,"Is it unnecessary to use ehcache over mongodb? 1 for indexed object, such as user indexed by Id username
2 simple query, such as search by page, username
3 complex query, tag, geolocation query. 

Any suggestions?",3
2628164,04/13/2010 08:50:50,168642,09/04/2009 16:33:20,25,0,do console apps run faster than windows based app?,"i am reletivly new to world of programming
i have a few performance questions 
1. do console apps run faster than windows based app?
2.are languages like c and pascal faster than object oriented languages like c++ and delphi?i know language speed depends more on compiler than on language itself but do compilers for prcedural languages like c and pascal produce faster code than oo ones like  delphi,c++(including c++ compilers that can procuce c code)
sorry for my bad english",performance,c,c++,delphi,pascal,04/15/2010 04:13:07,not a real question,1,78,9,"do console apps run faster than windows based app? i am reletivly new to world of programming
i have a few performance questions 
1. do console apps run faster than windows based app?
2.are languages like c and pascal faster than object oriented languages like c++ and delphi?i know language speed depends more on compiler than on language itself but do compilers for prcedural languages like c and pascal produce faster code than oo ones like  delphi,c++(including c++ compilers that can procuce c code)
sorry for my bad english",5
8674440,12/29/2011 22:54:53,624143,02/19/2011 06:43:59,98,4,How to measure overhead,"I think http://www.webopedia.com/TERM/O/overhead.html gives a pretty good description of what programming ""overhead"" is. My question though, is how is the amount of overhead that an operation incurs measured? Is there an objective way of measuring this across all platforms (e.g. timing)? Or are there platform specific metrics that must be applied?",performance,,,,,12/31/2011 06:28:31,not a real question,1,51,4,"How to measure overhead I think http://www.webopedia.com/TERM/O/overhead.html gives a pretty good description of what programming ""overhead"" is. My question though, is how is the amount of overhead that an operation incurs measured? Is there an objective way of measuring this across all platforms (e.g. timing)? Or are there platform specific metrics that must be applied?",1
81268,09/17/2008 08:56:38,15395,09/17/2008 08:56:37,1,0,Case insensitive search on Sybase,"I have been sick and tired Googling the solution for doing case-insensitive search on Sybase ASE (Sybase data/column names are case sensitive). The Sybase documentation proudly says that there is only one way to do such search which is using the Upper and Lower functions, but the adage goes, it has performance problems. And believe me they are right, if your table has huge data the performance is so awkward you are never gonna use Upper and Lower again. My question to fellow developers is:  how do you guys tackle this? 

P.S. Don't advise to change the sort-order or move to any other Database please, in real world developers don't control the databases.",performance,sybase,case,casesensitive,ase,,open,0,114,5,"Case insensitive search on Sybase I have been sick and tired Googling the solution for doing case-insensitive search on Sybase ASE (Sybase data/column names are case sensitive). The Sybase documentation proudly says that there is only one way to do such search which is using the Upper and Lower functions, but the adage goes, it has performance problems. And believe me they are right, if your table has huge data the performance is so awkward you are never gonna use Upper and Lower again. My question to fellow developers is:  how do you guys tackle this? 

P.S. Don't advise to change the sort-order or move to any other Database please, in real world developers don't control the databases.",5
312003,11/23/2008 01:48:21,23903,09/16/2008 16:05:24,647,25,What is the most ridiculous pessimization you've seen?,"We all know that premature optimization is the root of all evil because it leads to unreadable/unmaintainable code.  Even worse is pessimization, when someone implements an ""optimization"" because they *think* it will be faster, but it ends up being slower, as well as being buggy, unmaintainable, etc.  What is the most ridiculous example of this that you've seen?",performance,optimization,fun,,,10/25/2011 15:30:27,not constructive,1,60,8,"What is the most ridiculous pessimization you've seen? We all know that premature optimization is the root of all evil because it leads to unreadable/unmaintainable code.  Even worse is pessimization, when someone implements an ""optimization"" because they *think* it will be faster, but it ends up being slower, as well as being buggy, unmaintainable, etc.  What is the most ridiculous example of this that you've seen?",3
9477934,02/28/2012 07:22:20,264179,02/02/2010 08:45:15,240,9,Async operations - Determining if an operation is long running or not,"How do you determine if an operation is long running or not?

i have heard somewhere that microsoft considers anything over 40 ms to be long running, and that means it should be async.

However other authors talk in term of hundreds of milliseconds.

What is your opinion?

Thanks",performance,asynchronous,long-running,,,02/29/2012 18:46:14,not constructive,1,45,12,"Async operations - Determining if an operation is long running or not How do you determine if an operation is long running or not?

i have heard somewhere that microsoft considers anything over 40 ms to be long running, and that means it should be async.

However other authors talk in term of hundreds of milliseconds.

What is your opinion?

Thanks",3
11014309,06/13/2012 11:54:51,1452872,06/13/2012 05:46:32,1,0,Slow broker response time in ActiveMQ 5.6,"We have a setup where user generates report using SOAP over ActiveMQ with Jboss 5.1.0 as the AS. Our project is migrating from ActiveMQ 4 to ActiveMQ 5.6 and we have noticed with the default configuration, the time of generation of one of the reports has increased by 50 secs. This particular report consists of the maximum amount of data among all others, and the other reports are all showing response times close to our earlier setup. So we are suspecting it has to do with some setting wherein the message queue is not able to handle larger data. We have checked and the time from database to return the query result is identical, and this extra 50 sec is coming in ActiveMQ. Here is the log file extract (DEBUG enabled) for our older implementation (ActiveMQ 4.0):


    2012-06-13 05:21:18,909 [iveMQ Scheduler] DEBUG InactivityMonitor              - No message sent since last write check, sending a KeepAliveInfo
    2012-06-13 05:21:18,909 [iveMQ Scheduler] DEBUG InactivityMonitor              - Message received since last read check, resetting flag: 
    2012-06-13 05:21:33,909 [iveMQ Scheduler] DEBUG InactivityMonitor              - No message sent since last write check, sending a KeepAliveInfo
    2012-06-13 05:21:48,909 [iveMQ Scheduler] DEBUG InactivityMonitor              - No message sent since last write check, sending a KeepAliveInfo
    2012-06-13 05:21:48,909 [iveMQ Scheduler] DEBUG InactivityMonitor              - Message received since last read check, resetting flag: 
    2012-06-13 05:21:58,561 [127.0.0.1:52937] DEBUG UsageManager                   - Memory usage change.  from: 0, to: 9
    2012-06-13 05:21:58,698 [127.0.0.1:52937] DEBUG JournalMessageStore            - Journalled transacted message add for: ID:dayrhevidq001.enterprisenet.org-48380-1339578768566-1:0:5:1:1, at: 917:5857443
    2012-06-13 05:21:58,733 [127.0.0.1:52937] DEBUG JournalMessageStore            - Transacted message add commit for: ID:dayrhevidq001.enterprisenet.org-48380-1339578768566-1:0:5:1:1, at: 917:5857443
    2012-06-13 05:21:58,738 [127.0.0.1:52937] DEBUG JournalMessageStore            - Journalled transacted message remove for: ID:dayrhevidq001.enterprisenet.org-48380-1339578768566-1:0:12:1:1, at: 917:6886127
    2012-06-13 05:21:59,126 [127.0.0.1:52937] DEBUG JournalMessageStore            - Transacted message remove commit for: ID:dayrhevidq001.enterprisenet.org-48380-1339578768566-1:0:12:1:1, at: 917:6886127
    2012-06-13 05:22:01,034 [127.0.0.1:52937] DEBUG JournalMessageStore            - Journalled transacted message remove for: ID:dayrhevidq001.enterprisenet.org-48380-1339578768566-1:0:5:1:1, at: 917:6886676
    2012-06-13 05:22:01,233 [127.0.0.1:52937] DEBUG JournalMessageStore            - Transacted message remove commit for: ID:dayrhevidq001.enterprisenet.org-48380-1339578768566-1:0:5:1:1, at: 917:6886676
    2012-06-13 05:22:01,233 [127.0.0.1:52937] DEBUG UsageManager                   - Memory usage change.  from: 9, to: 0
    2012-06-13 05:22:03,907 [iveMQ Scheduler] DEBUG InactivityMonitor              - Message sent since last write check, resetting flag
    2012-06-13 05:22:18,909 [iveMQ Scheduler] DEBUG InactivityMonitor              - No message sent since last write check, sending a KeepAliveInf

and here is the one from the migrated implementation (ActiveMQ 5.6):

    2012-06-11 02:04:18,755 | DEBUG | commit: TX:ID:nrisjctls01.dev.netratings.com-60906-1339405160583-1:1:4 syncCount: 2 | org.apache.activemq.transaction.LocalTransaction | ActiveMQ Transport: tcp:///127.0.0.1:60907
    2012-06-11 02:04:18,759 | TRACE | ID:nrisjctls01.dev.netratings.com-60906-1339405160583-1:1:19:1 dispatched: ID:nrisjctls01.dev.netratings.com-60906-1339405160583-1:1:5:1:1 - queue://dav/synchronizer, dispatched: 1, inflight: 1 | org.apache.activemq.broker.region.PrefetchSubscription | BrokerService[localhost] Task-6
    2012-06-11 02:04:19,105 | TRACE | ack:MessageAck {commandId = 75, responseRequired = false, ackType = 2, consumerId = ID:nrisjctls01.dev.netratings.com-60906-1339405160583-1:1:19:1, firstMessageId = ID:nrisjctls01.dev.netratings.com-60906-1339405160583-1:1:5:1:1, lastMessageId = ID:nrisjctls01.dev.netratings.com-60906-1339405160583-1:1:5:1:1, destination = queue://dav/synchronizer, transactionId = TX:ID:nrisjctls01.dev.netratings.com-60906-1339405160583-1:1:6, messageCount = 1, poisonCause = null} | org.apache.activemq.broker.region.PrefetchSubscription | ActiveMQ Transport: tcp:///127.0.0.1:60907
    2012-06-11 02:04:19,108 | DEBUG | commit: TX:ID:nrisjctls01.dev.netratings.com-60906-1339405160583-1:1:6 syncCount: 2 | org.apache.activemq.transaction.LocalTransaction | ActiveMQ Transport: tcp:///127.0.0.1:60907
    2012-06-11 02:04:19,110 | DEBUG | Main:memory:queue://dav/synchronizer:memory: usage change from: 1% of available memory, to: 0% of available memory | org.apache.activemq.usage.Usage | ActiveMQ Transport: tcp:///127.0.0.1:60907
    2012-06-11 02:04:19,110 | DEBUG | Main:memory: usage change from: 1% of available memory, to: 0% of available memory | org.apache.activemq.usage.Usage | ActiveMQ Transport: tcp:///127.0.0.1:60907
    2012-06-11 02:04:20,592 | DEBUG | Checkpoint started. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:04:20,700 | DEBUG | Checkpoint done. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:04:25,725 | DEBUG | Checkpoint started. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:04:25,734 | DEBUG | Checkpoint done. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:04:30,755 | DEBUG | Checkpoint started. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:04:30,762 | DEBUG | Checkpoint done. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:04:35,785 | DEBUG | Checkpoint started. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:04:35,795 | DEBUG | Checkpoint done. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:04:38,523 | DEBUG | queue://dav/validator expiring messages .. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:38,524 | DEBUG | dav/validator toPageIn: 0, Inflight: 0, pagedInMessages.size 0, enqueueCount: 0, dequeueCount: 0 | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:38,524 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@e0420b:dav/validator,batchResetNeeded=false,storeHasMessages=false,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:38,524 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@e0420b:dav/validator,batchResetNeeded=false,storeHasMessages=false,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:38,524 | DEBUG | queue://dav/validator expiring messages done. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,824 | DEBUG | queue://dav/logger expiring messages .. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,824 | DEBUG | dav/logger toPageIn: 0, Inflight: 0, pagedInMessages.size 0, enqueueCount: 1, dequeueCount: 1 | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,824 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@1c64ed8:dav/logger,batchResetNeeded=false,storeHasMessages=true,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,824 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@1c64ed8:dav/logger,batchResetNeeded=false,storeHasMessages=true,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,824 | DEBUG | queue://dav/logger expiring messages done. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,824 | DEBUG | queue://dav/synchronizer expiring messages .. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,824 | DEBUG | dav/synchronizer toPageIn: 0, Inflight: 0, pagedInMessages.size 0, enqueueCount: 1, dequeueCount: 1 | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,824 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@1549ceb:dav/synchronizer,batchResetNeeded=false,storeHasMessages=true,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,825 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@1549ceb:dav/synchronizer,batchResetNeeded=false,storeHasMessages=true,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,825 | DEBUG | queue://dav/synchronizer expiring messages done. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,825 | DEBUG | queue://dav/executor expiring messages .. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,825 | DEBUG | dav/executor toPageIn: 0, Inflight: 0, pagedInMessages.size 0, enqueueCount: 1, dequeueCount: 1 | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,825 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@9b777a:dav/executor,batchResetNeeded=false,storeHasMessages=true,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,825 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@9b777a:dav/executor,batchResetNeeded=false,storeHasMessages=true,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,825 | DEBUG | queue://dav/executor expiring messages done. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,825 | DEBUG | queue://dav/synchronizer expiring messages .. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,825 | DEBUG | dav/synchronizer toPageIn: 0, Inflight: 0, pagedInMessages.size 0, enqueueCount: 1, dequeueCount: 1 | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,825 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@1549ceb:dav/synchronizer,batchResetNeeded=false,storeHasMessages=true,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,825 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@1549ceb:dav/synchronizer,batchResetNeeded=false,storeHasMessages=true,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,825 | DEBUG | queue://dav/synchronizer expiring messages done. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,826 | DEBUG | queue://dav/validator expiring messages .. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,826 | DEBUG | dav/validator toPageIn: 0, Inflight: 0, pagedInMessages.size 0, enqueueCount: 0, dequeueCount: 0 | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,826 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@e0420b:dav/validator,batchResetNeeded=false,storeHasMessages=false,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,826 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@e0420b:dav/validator,batchResetNeeded=false,storeHasMessages=false,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,826 | DEBUG | queue://dav/validator expiring messages done. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,826 | DEBUG | queue://dav/cacher expiring messages .. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,826 | DEBUG | dav/cacher toPageIn: 0, Inflight: 0, pagedInMessages.size 0, enqueueCount: 0, dequeueCount: 0 | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,826 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@626fd2:dav/cacher,batchResetNeeded=false,storeHasMessages=false,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,826 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@626fd2:dav/cacher,batchResetNeeded=false,storeHasMessages=false,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,826 | DEBUG | queue://dav/cacher expiring messages done. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,826 | DEBUG | queue://dav/executor expiring messages .. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,826 | DEBUG | dav/executor toPageIn: 0, Inflight: 0, pagedInMessages.size 0, enqueueCount: 1, dequeueCount: 1 | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,827 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@9b777a:dav/executor,batchResetNeeded=false,storeHasMessages=true,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,827 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@9b777a:dav/executor,batchResetNeeded=false,storeHasMessages=true,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,827 | DEBUG | queue://dav/executor expiring messages done. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,827 | DEBUG | queue://dav/logger expiring messages .. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,827 | DEBUG | dav/logger toPageIn: 0, Inflight: 0, pagedInMessages.size 0, enqueueCount: 1, dequeueCount: 1 | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,827 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@1c64ed8:dav/logger,batchResetNeeded=false,storeHasMessages=true,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,827 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@1c64ed8:dav/logger,batchResetNeeded=false,storeHasMessages=true,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,827 | DEBUG | queue://dav/logger expiring messages done. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,827 | DEBUG | queue://dav/cacher expiring messages .. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,827 | DEBUG | dav/cacher toPageIn: 0, Inflight: 0, pagedInMessages.size 0, enqueueCount: 0, dequeueCount: 0 | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,827 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@626fd2:dav/cacher,batchResetNeeded=false,storeHasMessages=false,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,828 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@626fd2:dav/cacher,batchResetNeeded=false,storeHasMessages=false,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,828 | DEBUG | queue://dav/cacher expiring messages done. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:40,814 | DEBUG | Checkpoint started. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:04:40,831 | TRACE | Last update: 2:2257550, full gc candidates set: [2] | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:04:40,831 | TRACE | gc candidates after first tx:2:2257550, [] | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:04:40,831 | TRACE | gc candidates: [] | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:04:40,831 | DEBUG | Checkpoint done. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:04:45,851 | DEBUG | Checkpoint started. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:04:45,860 | DEBUG | Checkpoint done. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:04:50,880 | DEBUG | Checkpoint started. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:04:50,890 | DEBUG | Checkpoint done. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:04:55,910 | DEBUG | Checkpoint started. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:04:55,924 | DEBUG | Checkpoint done. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:05:00,943 | DEBUG | Checkpoint started. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:05:00,957 | DEBUG | Checkpoint done. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:05:05,979 | DEBUG | Checkpoint started. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:05:05,988 | DEBUG | Checkpoint done. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:05:08,524 | DEBUG | queue://dav/validator expiring messages .. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:05:08,525 | DEBUG | dav/validator toPageIn: 0, Inflight: 0, pagedInMessages.size 0, enqueueCount: 0, dequeueCount: 0 | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:05:08,525 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@e0420b:dav/validator,batchResetNeeded=false,storeHasMessages=false,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:05:08,525 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@e0420b:dav/validator,batchResetNeeded=false,storeHasMessages=false,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:05:08,525 | DEBUG | queue://dav/validator expiring messages done. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:05:09,825 | DEBUG | queue://dav/logger expiring messages .. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:05:09,825 | DEBUG | dav/logger toPageIn: 0, Inflight: 0, pagedInMessages.size 0, enqueueCount: 1, dequeueCount: 1 | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    <some lines removed to save space>
    2012-06-11 02:05:09,828 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@626fd2:dav/cacher,batchResetNeeded=false,storeHasMessages=false,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:05:09,828 | DEBUG | queue://dav/cacher expiring messages done. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:05:11,008 | DEBUG | Checkpoint started. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:05:11,017 | TRACE | Last update: 2:2279306, full gc candidates set: [2] | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:05:11,017 | TRACE | gc candidates after first tx:2:2279306, [] | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:05:11,017 | TRACE | gc candidates: [] | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:05:11,017 | DEBUG | Checkpoint done. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:05:14,368 | DEBUG | Cleaning up expired web clients. | org.apache.activemq.web.MessageListenerServlet | Timer-0
    2012-06-11 02:05:14,783 | DEBUG | Cleaning up expired web clients. | org.apache.activemq.web.MessageListenerServlet | Timer-1

Here is my ActiveMQ.xml file:

    <!--
        Licensed to the Apache Software Foundation (ASF) under one or more
        contributor license agreements.  See the NOTICE file distributed with
        this work for additional information regarding copyright ownership.
        The ASF licenses this file to You under the Apache License, Version 2.0
        (the ""License""); you may not use this file except in compliance with
        the License.  You may obtain a copy of the License at
    
        http://www.apache.org/licenses/LICENSE-2.0
    
        Unless required by applicable law or agreed to in writing, software
        distributed under the License is distributed on an ""AS IS"" BASIS,
        WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
        See the License for the specific language governing permissions and
        limitations under the License.
    -->
    <!-- START SNIPPET: example -->
    <beans
      xmlns=""http://www.springframework.org/schema/beans""
      xmlns:amq=""http://activemq.apache.org/schema/core""
      xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
      xsi:schemaLocation=""http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.0.xsd
      http://activemq.apache.org/schema/core http://activemq.apache.org/schema/core/activemq-core.xsd"">
    
        <!-- Allows us to use system properties as variables in this configuration file -->
        <bean class=""org.springframework.beans.factory.config.PropertyPlaceholderConfigurer"">
            <property name=""locations"">
                <value>file:${activemq.conf}/credentials.properties</value>
            </property>
        </bean>
    
        <!--
            The <broker> element is used to configure the ActiveMQ broker.
        -->
        <broker xmlns=""http://activemq.apache.org/schema/core"" brokerName=""localhost"" dataDirectory=""${activemq.data}"" 
    		useShutdownHook=""false"" useJmx=""true"" persistent=""false"">
    
            <!--
                For better performances use VM cursor and small memory limit.
                For more information, see:
    
                http://activemq.apache.org/message-cursors.html
    
                Also, if your producer is ""hanging"", it's probably due to producer flow control.
                For more information, see:
                http://activemq.apache.org/producer-flow-control.html
            -->
      <!--
            <destinationPolicy>
          <policyMap><policyEntries>
            
              <policyEntry topic=""FOO.>"">
                <dispatchPolicy>
                  <strictOrderDispatchPolicy />
                </dispatchPolicy>
                <subscriptionRecoveryPolicy>
                  <lastImageSubscriptionRecoveryPolicy />
                </subscriptionRecoveryPolicy>
              </policyEntry>
    
          </policyEntries></policyMap>
        </destinationPolicy>
      -->
    
    <destinationPolicy>
                <policyMap>
                  <policyEntries>
                    <policyEntry topic="">"" producerFlowControl=""false"" memoryLimit=""1mb"">
                      <pendingSubscriberPolicy>
                        <vmCursor />
                      </pendingSubscriberPolicy>
                    </policyEntry>
                    <policyEntry queue="">"" producerFlowControl=""false"" memoryLimit=""1mb"">
                      <!-- Use VM cursor for better latency
                           For more information, see:
    
                           http://activemq.apache.org/message-cursors.html
    
                      <pendingQueuePolicy>
                        <vmQueueCursor/>
                      </pendingQueuePolicy>
                      -->
                    </policyEntry>
                  </policyEntries>
                </policyMap>
            </destinationPolicy>
            <!--
                The managementContext is used to configure how ActiveMQ is exposed in
                JMX. By default, ActiveMQ uses the MBean server that is started by
                the JVM. For more information, see:
    
                http://activemq.apache.org/jmx.html
            -->
            <managementContext>
                <managementContext createConnector=""false""/>
            </managementContext>
    
            <!--
                Configure message persistence for the broker. The default persistence
                mechanism is the KahaDB store (identified by the kahaDB tag).
                For more information, see:
    
                http://activemq.apache.org/persistence.html
            -->
            <persistenceAdapter>
                <kahaDB directory=""${activemq.data}/kahadb""/>
            </persistenceAdapter>
    
    
              <!--
                The systemUsage controls the maximum amount of space the broker will
                use before slowing down producers. For more information, see:
                http://activemq.apache.org/producer-flow-control.html
                If using ActiveMQ embedded - the following limits could safely be used:
    
            <systemUsage>
                <systemUsage>
                    <memoryUsage>
                        <memoryUsage limit=""20 mb""/>
                    </memoryUsage>
                    <storeUsage>
                        <storeUsage limit=""1 gb""/>
                    </storeUsage>
                    <tempUsage>
                        <tempUsage limit=""100 mb""/>
                    </tempUsage>
                </systemUsage>
            </systemUsage>
            
              <systemUsage>
                <systemUsage>
                    <memoryUsage>
                        <memoryUsage limit=""64 mb""/>
                    </memoryUsage>
                    <storeUsage>
                        <storeUsage limit=""100 gb""/>
                    </storeUsage>
                    <tempUsage>
                        <tempUsage limit=""50 gb""/>
                    </tempUsage>
                </systemUsage>
            </systemUsage>-->
    
            <!--
                The transport connectors expose ActiveMQ over a given protocol to
                clients and other brokers. For more information, see:
    
                http://activemq.apache.org/configuring-transports.html
            -->
            <transportConnectors>
                <transportConnector name=""default"" uri=""tcp://localhost:61616?jms.useAsyncSend=true&amp;jms.prefetchPolicy.all=10&amp;wireFormat.maxInactivityDuration=0"" discoveryUri=""multicast://default""/>
            </transportConnectors>
    
        <",performance,jboss,response,activemq,messagebroker,06/13/2012 12:11:45,not a real question,1,4729,7,"Slow broker response time in ActiveMQ 5.6 We have a setup where user generates report using SOAP over ActiveMQ with Jboss 5.1.0 as the AS. Our project is migrating from ActiveMQ 4 to ActiveMQ 5.6 and we have noticed with the default configuration, the time of generation of one of the reports has increased by 50 secs. This particular report consists of the maximum amount of data among all others, and the other reports are all showing response times close to our earlier setup. So we are suspecting it has to do with some setting wherein the message queue is not able to handle larger data. We have checked and the time from database to return the query result is identical, and this extra 50 sec is coming in ActiveMQ. Here is the log file extract (DEBUG enabled) for our older implementation (ActiveMQ 4.0):


    2012-06-13 05:21:18,909 [iveMQ Scheduler] DEBUG InactivityMonitor              - No message sent since last write check, sending a KeepAliveInfo
    2012-06-13 05:21:18,909 [iveMQ Scheduler] DEBUG InactivityMonitor              - Message received since last read check, resetting flag: 
    2012-06-13 05:21:33,909 [iveMQ Scheduler] DEBUG InactivityMonitor              - No message sent since last write check, sending a KeepAliveInfo
    2012-06-13 05:21:48,909 [iveMQ Scheduler] DEBUG InactivityMonitor              - No message sent since last write check, sending a KeepAliveInfo
    2012-06-13 05:21:48,909 [iveMQ Scheduler] DEBUG InactivityMonitor              - Message received since last read check, resetting flag: 
    2012-06-13 05:21:58,561 [127.0.0.1:52937] DEBUG UsageManager                   - Memory usage change.  from: 0, to: 9
    2012-06-13 05:21:58,698 [127.0.0.1:52937] DEBUG JournalMessageStore            - Journalled transacted message add for: ID:dayrhevidq001.enterprisenet.org-48380-1339578768566-1:0:5:1:1, at: 917:5857443
    2012-06-13 05:21:58,733 [127.0.0.1:52937] DEBUG JournalMessageStore            - Transacted message add commit for: ID:dayrhevidq001.enterprisenet.org-48380-1339578768566-1:0:5:1:1, at: 917:5857443
    2012-06-13 05:21:58,738 [127.0.0.1:52937] DEBUG JournalMessageStore            - Journalled transacted message remove for: ID:dayrhevidq001.enterprisenet.org-48380-1339578768566-1:0:12:1:1, at: 917:6886127
    2012-06-13 05:21:59,126 [127.0.0.1:52937] DEBUG JournalMessageStore            - Transacted message remove commit for: ID:dayrhevidq001.enterprisenet.org-48380-1339578768566-1:0:12:1:1, at: 917:6886127
    2012-06-13 05:22:01,034 [127.0.0.1:52937] DEBUG JournalMessageStore            - Journalled transacted message remove for: ID:dayrhevidq001.enterprisenet.org-48380-1339578768566-1:0:5:1:1, at: 917:6886676
    2012-06-13 05:22:01,233 [127.0.0.1:52937] DEBUG JournalMessageStore            - Transacted message remove commit for: ID:dayrhevidq001.enterprisenet.org-48380-1339578768566-1:0:5:1:1, at: 917:6886676
    2012-06-13 05:22:01,233 [127.0.0.1:52937] DEBUG UsageManager                   - Memory usage change.  from: 9, to: 0
    2012-06-13 05:22:03,907 [iveMQ Scheduler] DEBUG InactivityMonitor              - Message sent since last write check, resetting flag
    2012-06-13 05:22:18,909 [iveMQ Scheduler] DEBUG InactivityMonitor              - No message sent since last write check, sending a KeepAliveInf

and here is the one from the migrated implementation (ActiveMQ 5.6):

    2012-06-11 02:04:18,755 | DEBUG | commit: TX:ID:nrisjctls01.dev.netratings.com-60906-1339405160583-1:1:4 syncCount: 2 | org.apache.activemq.transaction.LocalTransaction | ActiveMQ Transport: tcp:///127.0.0.1:60907
    2012-06-11 02:04:18,759 | TRACE | ID:nrisjctls01.dev.netratings.com-60906-1339405160583-1:1:19:1 dispatched: ID:nrisjctls01.dev.netratings.com-60906-1339405160583-1:1:5:1:1 - queue://dav/synchronizer, dispatched: 1, inflight: 1 | org.apache.activemq.broker.region.PrefetchSubscription | BrokerService[localhost] Task-6
    2012-06-11 02:04:19,105 | TRACE | ack:MessageAck {commandId = 75, responseRequired = false, ackType = 2, consumerId = ID:nrisjctls01.dev.netratings.com-60906-1339405160583-1:1:19:1, firstMessageId = ID:nrisjctls01.dev.netratings.com-60906-1339405160583-1:1:5:1:1, lastMessageId = ID:nrisjctls01.dev.netratings.com-60906-1339405160583-1:1:5:1:1, destination = queue://dav/synchronizer, transactionId = TX:ID:nrisjctls01.dev.netratings.com-60906-1339405160583-1:1:6, messageCount = 1, poisonCause = null} | org.apache.activemq.broker.region.PrefetchSubscription | ActiveMQ Transport: tcp:///127.0.0.1:60907
    2012-06-11 02:04:19,108 | DEBUG | commit: TX:ID:nrisjctls01.dev.netratings.com-60906-1339405160583-1:1:6 syncCount: 2 | org.apache.activemq.transaction.LocalTransaction | ActiveMQ Transport: tcp:///127.0.0.1:60907
    2012-06-11 02:04:19,110 | DEBUG | Main:memory:queue://dav/synchronizer:memory: usage change from: 1% of available memory, to: 0% of available memory | org.apache.activemq.usage.Usage | ActiveMQ Transport: tcp:///127.0.0.1:60907
    2012-06-11 02:04:19,110 | DEBUG | Main:memory: usage change from: 1% of available memory, to: 0% of available memory | org.apache.activemq.usage.Usage | ActiveMQ Transport: tcp:///127.0.0.1:60907
    2012-06-11 02:04:20,592 | DEBUG | Checkpoint started. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:04:20,700 | DEBUG | Checkpoint done. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:04:25,725 | DEBUG | Checkpoint started. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:04:25,734 | DEBUG | Checkpoint done. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:04:30,755 | DEBUG | Checkpoint started. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:04:30,762 | DEBUG | Checkpoint done. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:04:35,785 | DEBUG | Checkpoint started. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:04:35,795 | DEBUG | Checkpoint done. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:04:38,523 | DEBUG | queue://dav/validator expiring messages .. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:38,524 | DEBUG | dav/validator toPageIn: 0, Inflight: 0, pagedInMessages.size 0, enqueueCount: 0, dequeueCount: 0 | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:38,524 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@e0420b:dav/validator,batchResetNeeded=false,storeHasMessages=false,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:38,524 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@e0420b:dav/validator,batchResetNeeded=false,storeHasMessages=false,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:38,524 | DEBUG | queue://dav/validator expiring messages done. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,824 | DEBUG | queue://dav/logger expiring messages .. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,824 | DEBUG | dav/logger toPageIn: 0, Inflight: 0, pagedInMessages.size 0, enqueueCount: 1, dequeueCount: 1 | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,824 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@1c64ed8:dav/logger,batchResetNeeded=false,storeHasMessages=true,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,824 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@1c64ed8:dav/logger,batchResetNeeded=false,storeHasMessages=true,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,824 | DEBUG | queue://dav/logger expiring messages done. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,824 | DEBUG | queue://dav/synchronizer expiring messages .. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,824 | DEBUG | dav/synchronizer toPageIn: 0, Inflight: 0, pagedInMessages.size 0, enqueueCount: 1, dequeueCount: 1 | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,824 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@1549ceb:dav/synchronizer,batchResetNeeded=false,storeHasMessages=true,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,825 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@1549ceb:dav/synchronizer,batchResetNeeded=false,storeHasMessages=true,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,825 | DEBUG | queue://dav/synchronizer expiring messages done. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,825 | DEBUG | queue://dav/executor expiring messages .. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,825 | DEBUG | dav/executor toPageIn: 0, Inflight: 0, pagedInMessages.size 0, enqueueCount: 1, dequeueCount: 1 | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,825 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@9b777a:dav/executor,batchResetNeeded=false,storeHasMessages=true,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,825 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@9b777a:dav/executor,batchResetNeeded=false,storeHasMessages=true,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,825 | DEBUG | queue://dav/executor expiring messages done. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,825 | DEBUG | queue://dav/synchronizer expiring messages .. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,825 | DEBUG | dav/synchronizer toPageIn: 0, Inflight: 0, pagedInMessages.size 0, enqueueCount: 1, dequeueCount: 1 | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,825 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@1549ceb:dav/synchronizer,batchResetNeeded=false,storeHasMessages=true,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,825 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@1549ceb:dav/synchronizer,batchResetNeeded=false,storeHasMessages=true,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,825 | DEBUG | queue://dav/synchronizer expiring messages done. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,826 | DEBUG | queue://dav/validator expiring messages .. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,826 | DEBUG | dav/validator toPageIn: 0, Inflight: 0, pagedInMessages.size 0, enqueueCount: 0, dequeueCount: 0 | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,826 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@e0420b:dav/validator,batchResetNeeded=false,storeHasMessages=false,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,826 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@e0420b:dav/validator,batchResetNeeded=false,storeHasMessages=false,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,826 | DEBUG | queue://dav/validator expiring messages done. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,826 | DEBUG | queue://dav/cacher expiring messages .. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,826 | DEBUG | dav/cacher toPageIn: 0, Inflight: 0, pagedInMessages.size 0, enqueueCount: 0, dequeueCount: 0 | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,826 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@626fd2:dav/cacher,batchResetNeeded=false,storeHasMessages=false,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,826 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@626fd2:dav/cacher,batchResetNeeded=false,storeHasMessages=false,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,826 | DEBUG | queue://dav/cacher expiring messages done. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,826 | DEBUG | queue://dav/executor expiring messages .. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,826 | DEBUG | dav/executor toPageIn: 0, Inflight: 0, pagedInMessages.size 0, enqueueCount: 1, dequeueCount: 1 | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,827 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@9b777a:dav/executor,batchResetNeeded=false,storeHasMessages=true,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,827 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@9b777a:dav/executor,batchResetNeeded=false,storeHasMessages=true,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,827 | DEBUG | queue://dav/executor expiring messages done. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,827 | DEBUG | queue://dav/logger expiring messages .. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,827 | DEBUG | dav/logger toPageIn: 0, Inflight: 0, pagedInMessages.size 0, enqueueCount: 1, dequeueCount: 1 | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,827 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@1c64ed8:dav/logger,batchResetNeeded=false,storeHasMessages=true,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,827 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@1c64ed8:dav/logger,batchResetNeeded=false,storeHasMessages=true,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,827 | DEBUG | queue://dav/logger expiring messages done. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,827 | DEBUG | queue://dav/cacher expiring messages .. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,827 | DEBUG | dav/cacher toPageIn: 0, Inflight: 0, pagedInMessages.size 0, enqueueCount: 0, dequeueCount: 0 | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,827 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@626fd2:dav/cacher,batchResetNeeded=false,storeHasMessages=false,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,828 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@626fd2:dav/cacher,batchResetNeeded=false,storeHasMessages=false,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:39,828 | DEBUG | queue://dav/cacher expiring messages done. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:04:40,814 | DEBUG | Checkpoint started. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:04:40,831 | TRACE | Last update: 2:2257550, full gc candidates set: [2] | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:04:40,831 | TRACE | gc candidates after first tx:2:2257550, [] | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:04:40,831 | TRACE | gc candidates: [] | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:04:40,831 | DEBUG | Checkpoint done. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:04:45,851 | DEBUG | Checkpoint started. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:04:45,860 | DEBUG | Checkpoint done. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:04:50,880 | DEBUG | Checkpoint started. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:04:50,890 | DEBUG | Checkpoint done. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:04:55,910 | DEBUG | Checkpoint started. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:04:55,924 | DEBUG | Checkpoint done. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:05:00,943 | DEBUG | Checkpoint started. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:05:00,957 | DEBUG | Checkpoint done. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:05:05,979 | DEBUG | Checkpoint started. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:05:05,988 | DEBUG | Checkpoint done. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:05:08,524 | DEBUG | queue://dav/validator expiring messages .. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:05:08,525 | DEBUG | dav/validator toPageIn: 0, Inflight: 0, pagedInMessages.size 0, enqueueCount: 0, dequeueCount: 0 | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:05:08,525 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@e0420b:dav/validator,batchResetNeeded=false,storeHasMessages=false,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:05:08,525 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@e0420b:dav/validator,batchResetNeeded=false,storeHasMessages=false,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:05:08,525 | DEBUG | queue://dav/validator expiring messages done. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:05:09,825 | DEBUG | queue://dav/logger expiring messages .. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:05:09,825 | DEBUG | dav/logger toPageIn: 0, Inflight: 0, pagedInMessages.size 0, enqueueCount: 1, dequeueCount: 1 | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    <some lines removed to save space>
    2012-06-11 02:05:09,828 | TRACE | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@626fd2:dav/cacher,batchResetNeeded=false,storeHasMessages=false,size=0,cacheEnabled=true - fillBatch | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:05:09,828 | DEBUG | queue://dav/cacher expiring messages done. | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[localhost] Scheduler
    2012-06-11 02:05:11,008 | DEBUG | Checkpoint started. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:05:11,017 | TRACE | Last update: 2:2279306, full gc candidates set: [2] | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:05:11,017 | TRACE | gc candidates after first tx:2:2279306, [] | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:05:11,017 | TRACE | gc candidates: [] | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:05:11,017 | DEBUG | Checkpoint done. | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Journal Checkpoint Worker
    2012-06-11 02:05:14,368 | DEBUG | Cleaning up expired web clients. | org.apache.activemq.web.MessageListenerServlet | Timer-0
    2012-06-11 02:05:14,783 | DEBUG | Cleaning up expired web clients. | org.apache.activemq.web.MessageListenerServlet | Timer-1

Here is my ActiveMQ.xml file:

    <!--
        Licensed to the Apache Software Foundation (ASF) under one or more
        contributor license agreements.  See the NOTICE file distributed with
        this work for additional information regarding copyright ownership.
        The ASF licenses this file to You under the Apache License, Version 2.0
        (the ""License""); you may not use this file except in compliance with
        the License.  You may obtain a copy of the License at
    
        http://www.apache.org/licenses/LICENSE-2.0
    
        Unless required by applicable law or agreed to in writing, software
        distributed under the License is distributed on an ""AS IS"" BASIS,
        WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
        See the License for the specific language governing permissions and
        limitations under the License.
    -->
    <!-- START SNIPPET: example -->
    <beans
      xmlns=""http://www.springframework.org/schema/beans""
      xmlns:amq=""http://activemq.apache.org/schema/core""
      xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
      xsi:schemaLocation=""http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.0.xsd
      http://activemq.apache.org/schema/core http://activemq.apache.org/schema/core/activemq-core.xsd"">
    
        <!-- Allows us to use system properties as variables in this configuration file -->
        <bean class=""org.springframework.beans.factory.config.PropertyPlaceholderConfigurer"">
            <property name=""locations"">
                <value>file:${activemq.conf}/credentials.properties</value>
            </property>
        </bean>
    
        <!--
            The <broker> element is used to configure the ActiveMQ broker.
        -->
        <broker xmlns=""http://activemq.apache.org/schema/core"" brokerName=""localhost"" dataDirectory=""${activemq.data}"" 
    		useShutdownHook=""false"" useJmx=""true"" persistent=""false"">
    
            <!--
                For better performances use VM cursor and small memory limit.
                For more information, see:
    
                http://activemq.apache.org/message-cursors.html
    
                Also, if your producer is ""hanging"", it's probably due to producer flow control.
                For more information, see:
                http://activemq.apache.org/producer-flow-control.html
            -->
      <!--
            <destinationPolicy>
          <policyMap><policyEntries>
            
              <policyEntry topic=""FOO.>"">
                <dispatchPolicy>
                  <strictOrderDispatchPolicy />
                </dispatchPolicy>
                <subscriptionRecoveryPolicy>
                  <lastImageSubscriptionRecoveryPolicy />
                </subscriptionRecoveryPolicy>
              </policyEntry>
    
          </policyEntries></policyMap>
        </destinationPolicy>
      -->
    
    <destinationPolicy>
                <policyMap>
                  <policyEntries>
                    <policyEntry topic="">"" producerFlowControl=""false"" memoryLimit=""1mb"">
                      <pendingSubscriberPolicy>
                        <vmCursor />
                      </pendingSubscriberPolicy>
                    </policyEntry>
                    <policyEntry queue="">"" producerFlowControl=""false"" memoryLimit=""1mb"">
                      <!-- Use VM cursor for better latency
                           For more information, see:
    
                           http://activemq.apache.org/message-cursors.html
    
                      <pendingQueuePolicy>
                        <vmQueueCursor/>
                      </pendingQueuePolicy>
                      -->
                    </policyEntry>
                  </policyEntries>
                </policyMap>
            </destinationPolicy>
            <!--
                The managementContext is used to configure how ActiveMQ is exposed in
                JMX. By default, ActiveMQ uses the MBean server that is started by
                the JVM. For more information, see:
    
                http://activemq.apache.org/jmx.html
            -->
            <managementContext>
                <managementContext createConnector=""false""/>
            </managementContext>
    
            <!--
                Configure message persistence for the broker. The default persistence
                mechanism is the KahaDB store (identified by the kahaDB tag).
                For more information, see:
    
                http://activemq.apache.org/persistence.html
            -->
            <persistenceAdapter>
                <kahaDB directory=""${activemq.data}/kahadb""/>
            </persistenceAdapter>
    
    
              <!--
                The systemUsage controls the maximum amount of space the broker will
                use before slowing down producers. For more information, see:
                http://activemq.apache.org/producer-flow-control.html
                If using ActiveMQ embedded - the following limits could safely be used:
    
            <systemUsage>
                <systemUsage>
                    <memoryUsage>
                        <memoryUsage limit=""20 mb""/>
                    </memoryUsage>
                    <storeUsage>
                        <storeUsage limit=""1 gb""/>
                    </storeUsage>
                    <tempUsage>
                        <tempUsage limit=""100 mb""/>
                    </tempUsage>
                </systemUsage>
            </systemUsage>
            
              <systemUsage>
                <systemUsage>
                    <memoryUsage>
                        <memoryUsage limit=""64 mb""/>
                    </memoryUsage>
                    <storeUsage>
                        <storeUsage limit=""100 gb""/>
                    </storeUsage>
                    <tempUsage>
                        <tempUsage limit=""50 gb""/>
                    </tempUsage>
                </systemUsage>
            </systemUsage>-->
    
            <!--
                The transport connectors expose ActiveMQ over a given protocol to
                clients and other brokers. For more information, see:
    
                http://activemq.apache.org/configuring-transports.html
            -->
            <transportConnectors>
                <transportConnector name=""default"" uri=""tcp://localhost:61616?jms.useAsyncSend=true&amp;jms.prefetchPolicy.all=10&amp;wireFormat.maxInactivityDuration=0"" discoveryUri=""multicast://default""/>
            </transportConnectors>
    
        <",5
11207170,06/26/2012 12:17:59,588736,01/25/2011 08:59:18,166,4,Lazy dropdown in Grails,"im trying to implement in Grails autofilter like in Excel. My idea is to create a Taglib 

    class FilterPanelTagLib {
	static namespace = 'x'
	def filterPanel = {attrs, body ->
		
		StringBuffer content = new StringBuffer()
		List lines = attrs.lines
		def filter = attrs.filter
		out << ""<form>""
		long start = System.currentTimeMillis()
		attrs.columns.each{col->
			if(col != ''){
				List tmp = getColumnValues(lines,col)
				def value = filter?filter[col]:""""
				out << ""<th>"" + g.select(
					onchange:""submit()"",
					name:""filter.""+col,
					from:tmp, 
					value:value, 
					noSelection:['ALL':'ALL'])+""</th>""
			}else {
				out << ""<th> </th>""
			}
		}
		out << ""</form>""
		println ((System.currentTimeMillis()-start)/1000 + "" filterPanelTagLib"".center(40,""-""))

	}
	
	private List getColumnValues(List lines, String column){
		List result = lines.collect{it[column]}.unique().sort() << 'ALL'
		return result
	}


and use it in GSPs like this:

    <x:filterPanel filter=""${filter}"" lines=""${taskList}"" columns=""['id',
        	'label',
        	'activity',
        	'assignee'
        	'',
        	'']""/>

It works fine. I get the values for selected dropboxes as params.filter in controller and can use criteriaBuilder to get the filtered list.

The problem is the performance for creating the content of each dropdown. I have tested it for a list with 1000 items and it takes about 3 sec. The target performance is <2 sec for 30.000

Any suggestion is wellcome! ",performance,grails,,,,,open,0,196,4,"Lazy dropdown in Grails im trying to implement in Grails autofilter like in Excel. My idea is to create a Taglib 

    class FilterPanelTagLib {
	static namespace = 'x'
	def filterPanel = {attrs, body ->
		
		StringBuffer content = new StringBuffer()
		List lines = attrs.lines
		def filter = attrs.filter
		out << ""<form>""
		long start = System.currentTimeMillis()
		attrs.columns.each{col->
			if(col != ''){
				List tmp = getColumnValues(lines,col)
				def value = filter?filter[col]:""""
				out << ""<th>"" + g.select(
					onchange:""submit()"",
					name:""filter.""+col,
					from:tmp, 
					value:value, 
					noSelection:['ALL':'ALL'])+""</th>""
			}else {
				out << ""<th> </th>""
			}
		}
		out << ""</form>""
		println ((System.currentTimeMillis()-start)/1000 + "" filterPanelTagLib"".center(40,""-""))

	}
	
	private List getColumnValues(List lines, String column){
		List result = lines.collect{it[column]}.unique().sort() << 'ALL'
		return result
	}


and use it in GSPs like this:

    <x:filterPanel filter=""${filter}"" lines=""${taskList}"" columns=""['id',
        	'label',
        	'activity',
        	'assignee'
        	'',
        	'']""/>

It works fine. I get the values for selected dropboxes as params.filter in controller and can use criteriaBuilder to get the filtered list.

The problem is the performance for creating the content of each dropdown. I have tested it for a list with 1000 items and it takes about 3 sec. The target performance is <2 sec for 30.000

Any suggestion is wellcome! ",2
5668711,04/14/2011 19:38:44,606368,02/07/2011 11:44:07,16,0,help in how to write batch script for load testing,"I need to be able to write a batch script to do load testing on a server on the network. I need it to behave as multiple pcs polling the server. If I go into a little detail when I run the script it should ask me how many instances to create, server ip on the network, and the poll time. so e.g. if i enter 100 instances in the console and 2 mins poll time, it should create that many instances that all poll the server after every 2 mins. I have new idea how to write batch scripts and need some guidance into how to achieve it. Any help will be highly appreciated.",performance,testing,batch,,,04/14/2011 22:10:19,not a real question,1,115,10,"help in how to write batch script for load testing I need to be able to write a batch script to do load testing on a server on the network. I need it to behave as multiple pcs polling the server. If I go into a little detail when I run the script it should ask me how many instances to create, server ip on the network, and the poll time. so e.g. if i enter 100 instances in the console and 2 mins poll time, it should create that many instances that all poll the server after every 2 mins. I have new idea how to write batch scripts and need some guidance into how to achieve it. Any help will be highly appreciated.",3
7701032,10/09/2011 02:18:28,405055,07/28/2010 21:30:30,33,0,Web frameworks/languages,"I've recently found an interest in different web languages and frameworks. Anyways, which one is the most valuable to learn?

I've recently have found interest in JSP, but is there any reason to learn it (I know java already, but I do not know how to setup and program an application in JSP, etc) as opposed to PHP, which I already know? Or maybe I should learn python and use the django project? Or maybe haskell and use the snap framework? Or why not just perl?

Anyways, I would like some opinion on this!",performance,optimization,language,web,,10/09/2011 02:24:20,not constructive,1,92,2,"Web frameworks/languages I've recently found an interest in different web languages and frameworks. Anyways, which one is the most valuable to learn?

I've recently have found interest in JSP, but is there any reason to learn it (I know java already, but I do not know how to setup and program an application in JSP, etc) as opposed to PHP, which I already know? Or maybe I should learn python and use the django project? Or maybe haskell and use the snap framework? Or why not just perl?

Anyways, I would like some opinion on this!",4
10458805,05/05/2012 04:19:35,1202349,02/10/2012 15:40:43,331,4,Best practices / tools for high performance IIS?,"Recently I come across high performance web server nginx.

I would wonder, where does IIS stands as far as performance matters?

What are the tools/best practices to increase IIS 7 / 7.5 performance for a web application hosted on IIS.

Web Servers like Apache, nginx, or IIS plays an important role in performance and this would definitely help developers to understand how does it matters to choose correct web server for scalable web application.",performance,iis,webserver,scalability,,05/06/2012 11:08:02,not constructive,1,71,8,"Best practices / tools for high performance IIS? Recently I come across high performance web server nginx.

I would wonder, where does IIS stands as far as performance matters?

What are the tools/best practices to increase IIS 7 / 7.5 performance for a web application hosted on IIS.

Web Servers like Apache, nginx, or IIS plays an important role in performance and this would definitely help developers to understand how does it matters to choose correct web server for scalable web application.",4
6480184,06/25/2011 20:03:28,180862,09/29/2009 03:40:19,392,3,Is ORM (Object-Relational mapping) the root cause of performance problem on web sites use them?,"Most of MVC based web sites using ORM to map between the model and corresponding database entities. What's sitting in-between are the SQL/non-SQL queries that either auto-gen'd or provided by developers. The problem with that approach is that - 1. database I/O is the bottle-neck for real-time traffic 2. no easy to plug in mechanisms to do the caching. 3. scalability. etc

I would like to hear your opinions on that.",performance,mvc,orm,,,06/25/2011 22:27:29,not constructive,1,70,15,"Is ORM (Object-Relational mapping) the root cause of performance problem on web sites use them? Most of MVC based web sites using ORM to map between the model and corresponding database entities. What's sitting in-between are the SQL/non-SQL queries that either auto-gen'd or provided by developers. The problem with that approach is that - 1. database I/O is the bottle-neck for real-time traffic 2. no easy to plug in mechanisms to do the caching. 3. scalability. etc

I would like to hear your opinions on that.",3
11586730,07/20/2012 20:46:45,1539378,07/19/2012 23:42:52,1,0,Constructing a predictive algorithm (NN?) with various amounts of features for each training point in R,"I am relatively new to machine learning but have been trying to train an algorithm to predict if an account will close or not using thousands of data points and many features (I will post an example of data below).  I am using data from the month before the account closed but the problem is that accounts have been around for different amounts of time. So, whereas for one account I might only have performance data up to 1 year, for another account I might have 1 month, 3 month, 1 year, 3 year, 5 year and even 10 year.  I am looking for some help on how I can build an algorithm that incorporates the different amounts of data for each account.  I was thinking about using a neural net because I have always been interested in them but I am open to any suggestions. I can really just use help in any way on this problem. There were 94 features but I cut it down to 19 to start playing around with it. I will gladly accept any advice on how to approach this problem.



  [1]: http://i.stack.imgur.com/y1VVj.jpg",performance,r,machine-learning,classification,finance,07/21/2012 05:57:13,off topic,1,193,16,"Constructing a predictive algorithm (NN?) with various amounts of features for each training point in R I am relatively new to machine learning but have been trying to train an algorithm to predict if an account will close or not using thousands of data points and many features (I will post an example of data below).  I am using data from the month before the account closed but the problem is that accounts have been around for different amounts of time. So, whereas for one account I might only have performance data up to 1 year, for another account I might have 1 month, 3 month, 1 year, 3 year, 5 year and even 10 year.  I am looking for some help on how I can build an algorithm that incorporates the different amounts of data for each account.  I was thinking about using a neural net because I have always been interested in them but I am open to any suggestions. I can really just use help in any way on this problem. There were 94 features but I cut it down to 19 to start playing around with it. I will gladly accept any advice on how to approach this problem.



  [1]: http://i.stack.imgur.com/y1VVj.jpg",5
10336900,04/26/2012 15:48:07,1359113,04/26/2012 15:43:01,1,0,"external hdd really slow, and get error messages","i recently reinstalled windows7, and now my storejet 1.5TB external
hdd is much slower, i can view/open nearly all my data except for a
few folders.
these folders i cannot delete either.

can you please tell me what is wrong? virus, dying drive or what?

thanks duncan",performance,error-message,external,,,04/26/2012 16:00:55,off topic,1,42,8,"external hdd really slow, and get error messages i recently reinstalled windows7, and now my storejet 1.5TB external
hdd is much slower, i can view/open nearly all my data except for a
few folders.
these folders i cannot delete either.

can you please tell me what is wrong? virus, dying drive or what?

thanks duncan",3
10380421,04/30/2012 08:11:59,1235093,02/27/2012 08:02:22,1,0,How to use NAS differently?,"i know NAS can be serve to transfer data typically. Example Client server file sharing and etc. but i want to know are there any other ways to use NAS?

For example, use to on a PS3 to transfer files (just my assumption)?
What can Vendors do with this technology?
Or what u can do with this technology?",performance,homework,hardware,nas,,06/26/2012 14:40:49,off topic,1,55,5,"How to use NAS differently? i know NAS can be serve to transfer data typically. Example Client server file sharing and etc. but i want to know are there any other ways to use NAS?

For example, use to on a PS3 to transfer files (just my assumption)?
What can Vendors do with this technology?
Or what u can do with this technology?",4
8810465,01/10/2012 21:03:04,298336,03/21/2010 07:16:45,668,7,How to improve performance or simplify .NET code with linq,"Apart from database operation, how can I simplify or improve my code with LINQ ?

**Example**
    *To search in a string*
   

     string search = ""search in list"";
        IEnumerable<string> results = myList.Where(s => s == search);",performance,linq,.net-4.0,,,01/12/2012 03:38:32,not constructive,1,51,10,"How to improve performance or simplify .NET code with linq Apart from database operation, how can I simplify or improve my code with LINQ ?

**Example**
    *To search in a string*
   

     string search = ""search in list"";
        IEnumerable<string> results = myList.Where(s => s == search);",3
8829163,01/12/2012 01:51:39,1144479,01/12/2012 01:37:50,1,0,Bizarre Thrift/PHP Performance Performance,"I've got a bizarre performance issue with Thrift 0.8 for PHP for both server and client. I've got two tests using a simple echo() server call that just returns the string you send to it. 

  * Test 1: open() the transport, Call echo() 5 times and close() the transport. 
  * Test 2: Loop 5 times: open() transport, call echo(), close() transport. 

What? Test 2 is opening and closing the socket for each call? That must be really slow!

That's what is bizarre: Test 2 is 40x faster than Test 1, about 1ms per iteration vs 40ms per iteration. 

Here is the basic idea.

    $socket = new TSocket($host);
    $transport = new TBufferedTransport($socket, 1024, 1024);
    $protocol = new TBinaryProtocol($transport);

    // Test 2: Open and close the connection each time. 
    function testSingleCall($c, $transport, $protocol) {
        $client = new \thrift\LocationsClient($protocol, $protocol);
        $t1 = microtime(true);
        for ($i = 0; $i < $c; $i++) {
            $transport->open();
            $v = $client->echoMessage($i);              
           $transport->close();
       }
       $t2 = microtime(true);

        return (($t2 - $t1) * 1000) / $c; // miliseconds per call
    }

    // Test 1: multiple calls per transaction. 
    function testMultipleCall($c, $transport, $protocol) {
        $client = new \thrift\LocationsClient($protocol, $protocol);

        $t1 = microtime(true);

        $transport->open();
        for ($i = 0; $i < $c; $i++) {
            $v = $client->echoMessage($i);         
        }
        $transport->close();
       $t2 = microtime(true);

        return (($t2 - $t1) * 1000) / $c; // miliseconds per call
   }

Any ideas what would cause opening and closing sockets to be faster than re-using the open socket?
",performance,thrift,,,,,open,0,430,4,"Bizarre Thrift/PHP Performance Performance I've got a bizarre performance issue with Thrift 0.8 for PHP for both server and client. I've got two tests using a simple echo() server call that just returns the string you send to it. 

  * Test 1: open() the transport, Call echo() 5 times and close() the transport. 
  * Test 2: Loop 5 times: open() transport, call echo(), close() transport. 

What? Test 2 is opening and closing the socket for each call? That must be really slow!

That's what is bizarre: Test 2 is 40x faster than Test 1, about 1ms per iteration vs 40ms per iteration. 

Here is the basic idea.

    $socket = new TSocket($host);
    $transport = new TBufferedTransport($socket, 1024, 1024);
    $protocol = new TBinaryProtocol($transport);

    // Test 2: Open and close the connection each time. 
    function testSingleCall($c, $transport, $protocol) {
        $client = new \thrift\LocationsClient($protocol, $protocol);
        $t1 = microtime(true);
        for ($i = 0; $i < $c; $i++) {
            $transport->open();
            $v = $client->echoMessage($i);              
           $transport->close();
       }
       $t2 = microtime(true);

        return (($t2 - $t1) * 1000) / $c; // miliseconds per call
    }

    // Test 1: multiple calls per transaction. 
    function testMultipleCall($c, $transport, $protocol) {
        $client = new \thrift\LocationsClient($protocol, $protocol);

        $t1 = microtime(true);

        $transport->open();
        for ($i = 0; $i < $c; $i++) {
            $v = $client->echoMessage($i);         
        }
        $transport->close();
       $t2 = microtime(true);

        return (($t2 - $t1) * 1000) / $c; // miliseconds per call
   }

Any ideas what would cause opening and closing sockets to be faster than re-using the open socket?
",2
9206823,02/09/2012 07:15:30,1199042,02/09/2012 07:11:56,1,0,How-to monthly graph in Munin?,"I've ony daily (by day) and weekly (by week) graphs in Munin. **What I have to change/configure in Munin to showing the monthly (by month) graph?**

My Munin version is 1.4.6 on CentOS 6.2.

Thanks in advance!

Rgds,<br />
Scottie",performance,monitoring,munin,,,06/04/2012 16:36:48,off topic,1,36,5,"How-to monthly graph in Munin? I've ony daily (by day) and weekly (by week) graphs in Munin. **What I have to change/configure in Munin to showing the monthly (by month) graph?**

My Munin version is 1.4.6 on CentOS 6.2.

Thanks in advance!

Rgds,<br />
Scottie",3
11433566,07/11/2012 13:18:37,1300391,03/29/2012 09:33:19,1,0,how to find out the FreeTDS 's bottleneck vai performance,"i got a task to investigate the FreeTDS in order to want to know weather this FreeTDS is our project's bottleneck via performance testing
so ,i have two questions:
first : where can i got the FreeTDS.log?  does it configure in freetds.conf, and how to set?
Sectiond : what's the phenomenon when the FreeTDS is bottleneck indeed?  how can i confirm this?

many thanks for your reply.",performance,testing,freetds,,,,open,0,65,10,"how to find out the FreeTDS 's bottleneck vai performance i got a task to investigate the FreeTDS in order to want to know weather this FreeTDS is our project's bottleneck via performance testing
so ,i have two questions:
first : where can i got the FreeTDS.log?  does it configure in freetds.conf, and how to set?
Sectiond : what's the phenomenon when the FreeTDS is bottleneck indeed?  how can i confirm this?

many thanks for your reply.",3
11478788,07/13/2012 21:43:38,1108283,12/20/2011 16:28:09,18,1,Replacing Apache2.2 with lighttpd 1.4 to speedup. Fine tuning?,"I am trying to reduce memory usage and increase speed.
I am serving a small webservice that receives a max of 800 hits/sec and an average of 400 hits/sec.

The server is a 2 xeon dual core 3.0GHz with 8 Gb of ram.

2Gb is taken by the MySQL
6Gb are left for the rest, which I think that will allow me to take no less than 5Gb for the lighttpd.

How do I have to setup the lighttpd to handle that traffic, regardless of the HW being able, I am interested on optimizing it. I know that the real deal here might be the process involved on the webservice, but that is other story. But it is based on PHP5 and with eAccelerator (it really makes a difference).

I am trying to speed this up because the performace is not good enough. If I increse the number of MaxClients the server starts swapping which is terrible.

Should I user memcache?

I hope I have explained myself properly.

For the apache2 I was using this conf:

    <IfModule mpm_prefork_module>
        StartServers           5
        MinSpareServers        5
        MaxSpareServers      100
        ServerLimit         1000
        ListenBacklog       1000
        MaxClients           700
        MaxRequestsPerChild    0
    </IfModule>

Thanks!",performance,php5,lighttpd,,,07/14/2012 10:29:14,off topic,1,288,9,"Replacing Apache2.2 with lighttpd 1.4 to speedup. Fine tuning? I am trying to reduce memory usage and increase speed.
I am serving a small webservice that receives a max of 800 hits/sec and an average of 400 hits/sec.

The server is a 2 xeon dual core 3.0GHz with 8 Gb of ram.

2Gb is taken by the MySQL
6Gb are left for the rest, which I think that will allow me to take no less than 5Gb for the lighttpd.

How do I have to setup the lighttpd to handle that traffic, regardless of the HW being able, I am interested on optimizing it. I know that the real deal here might be the process involved on the webservice, but that is other story. But it is based on PHP5 and with eAccelerator (it really makes a difference).

I am trying to speed this up because the performace is not good enough. If I increse the number of MaxClients the server starts swapping which is terrible.

Should I user memcache?

I hope I have explained myself properly.

For the apache2 I was using this conf:

    <IfModule mpm_prefork_module>
        StartServers           5
        MinSpareServers        5
        MaxSpareServers      100
        ServerLimit         1000
        ListenBacklog       1000
        MaxClients           700
        MaxRequestsPerChild    0
    </IfModule>

Thanks!",3
9094760,02/01/2012 11:08:10,1053416,11/18/2011 08:44:03,7,1,How to save time while coding with database,"I spend lot of time on 
1. To fill dummy data into table to test on code result
2. While to increase performance in terms of result from data base 
3.etc",performance,coding-style,,,,02/02/2012 09:26:08,not constructive,1,30,8,"How to save time while coding with database I spend lot of time on 
1. To fill dummy data into table to test on code result
2. While to increase performance in terms of result from data base 
3.etc",2
4334367,12/02/2010 11:21:02,15985,09/17/2008 13:55:21,2336,77,How to tell when number of process page faults will affect performance?,There is a Windows performance counter for the number of page faults a process generates.  Is there a rule-of-thumb threshold at which performance will be affected?,performance,performancecounter,page-fault,,,,open,0,27,12,How to tell when number of process page faults will affect performance? There is a Windows performance counter for the number of page faults a process generates.  Is there a rule-of-thumb threshold at which performance will be affected?,3
3551786,08/23/2010 21:18:58,11344,09/16/2008 07:17:03,197,9,Is a touchscreen useful/effective while programming?,"Does anyone have experiences if working with a touch screen monitor is useful and effective while programming? I mean most things an effective programmer will do with the keyboard, but I wonder if the other things (clicking through menus, scrolling through compilation logs, and of course copy'n'paste) can be done more efficiently with a touch screen than with a mouse.",performance,touchscreen,programming,,,09/19/2011 05:23:44,not constructive,1,60,6,"Is a touchscreen useful/effective while programming? Does anyone have experiences if working with a touch screen monitor is useful and effective while programming? I mean most things an effective programmer will do with the keyboard, but I wonder if the other things (clicking through menus, scrolling through compilation logs, and of course copy'n'paste) can be done more efficiently with a touch screen than with a mouse.",3
10074973,04/09/2012 14:46:23,112882,05/27/2009 03:24:03,710,13,Is 300ms for encrypting and then decrypting a single 8 * 1K message an acceptable RSA performance?,"Hi I was just reading `PowerMod` in Mathematica 8's documentation and wanted to test the Haksell `RSA` package (`ghc --make -O2 -O3 -fllvm -optlo-O3 test.hs`):

    {-# LANGUAGE OverloadedStrings #-}
    
    module Main where
    
    import Control.Monad
    import System.Random
    import Codec.Crypto.RSA
    import Data.ByteString.Lazy
    import Data.ByteString.Char8
    
    import Criterion.Main
    import Criterion.Config

    main :: IO ()
    main = do
      print m1
      print m4
      print m8
      defaultMainWith defaultConfig (return ()) [
        bgroup ""RSA"" [
           bench ""1"" $ ed m1
         , bench ""4"" $ ed m4
         , bench ""8"" $ ed m8
         ]
       ]
    
    m1 = fromChunks [ Data.ByteString.Char8.replicate (1*1024) '0' ]
    m4 = fromChunks [ Data.ByteString.Char8.replicate (4*1024) '0' ]
    m8 = fromChunks [ Data.ByteString.Char8.replicate (8*1024) '0' ]
    
    ed m = do
      g1 <- newStdGen
      let (el,il,g2) = generateKeyPair g1 1024
      loop 1 g2 el il m
    
    loop :: RandomGen g => Int -> g -> PublicKey -> PrivateKey -> Data.ByteString.Lazy.ByteString -> IO ()
    loop n g e i m = do
      let   nn     = n-1
      let  (em,ng) = encrypt g e  m
      let   dm     = decrypt   i em
      when (m == dm) $ Data.ByteString.Char8.putStr ""1""
      when (nn > 0 ) $ loop nn ng e i m

Also tried this in Mathematica:

    {p, q} = Prime[RandomInteger[{10^4, 10^5}, {2}]];
    {p, q, n = p q}
    \[Lambda] = CarmichaelLambda[n]
    d = NestWhile[#1 + 1 & , Round[n/3], GCD[\[Lambda], #1] =!= 1 &]
    e = PowerMod[d, -1, \[Lambda]]
    enc = PowerMod[#, e, n] &;
    dec = PowerMod[#, d, n] &;
    c = ConstantArray[48, 8 1024];
    t = Table[c // enc // dec; // AbsoluteTiming, {10}][[All, 1]]

Timings both in Haskell (`m8`) and Mathematica cases are similar:

    {0.313015, 0.302337, 0.303766, 0.303321, 0.303018, 0.302574, \
    0.302511, 0.303958, 0.301411, 0.300820}

Is 300ms per 8192-bytes-long message an acceptable performance for RSA? How do OpenSSL or other implementations compare?

(Test rig: 64-bit linux; 4xCORE, Intel(R) Core(TM) i5 CPU       M 430  @ 2.27GHz)",performance,haskell,mathematica,openssl,rsa,04/10/2012 16:02:09,off topic,1,541,17,"Is 300ms for encrypting and then decrypting a single 8 * 1K message an acceptable RSA performance? Hi I was just reading `PowerMod` in Mathematica 8's documentation and wanted to test the Haksell `RSA` package (`ghc --make -O2 -O3 -fllvm -optlo-O3 test.hs`):

    {-# LANGUAGE OverloadedStrings #-}
    
    module Main where
    
    import Control.Monad
    import System.Random
    import Codec.Crypto.RSA
    import Data.ByteString.Lazy
    import Data.ByteString.Char8
    
    import Criterion.Main
    import Criterion.Config

    main :: IO ()
    main = do
      print m1
      print m4
      print m8
      defaultMainWith defaultConfig (return ()) [
        bgroup ""RSA"" [
           bench ""1"" $ ed m1
         , bench ""4"" $ ed m4
         , bench ""8"" $ ed m8
         ]
       ]
    
    m1 = fromChunks [ Data.ByteString.Char8.replicate (1*1024) '0' ]
    m4 = fromChunks [ Data.ByteString.Char8.replicate (4*1024) '0' ]
    m8 = fromChunks [ Data.ByteString.Char8.replicate (8*1024) '0' ]
    
    ed m = do
      g1 <- newStdGen
      let (el,il,g2) = generateKeyPair g1 1024
      loop 1 g2 el il m
    
    loop :: RandomGen g => Int -> g -> PublicKey -> PrivateKey -> Data.ByteString.Lazy.ByteString -> IO ()
    loop n g e i m = do
      let   nn     = n-1
      let  (em,ng) = encrypt g e  m
      let   dm     = decrypt   i em
      when (m == dm) $ Data.ByteString.Char8.putStr ""1""
      when (nn > 0 ) $ loop nn ng e i m

Also tried this in Mathematica:

    {p, q} = Prime[RandomInteger[{10^4, 10^5}, {2}]];
    {p, q, n = p q}
    \[Lambda] = CarmichaelLambda[n]
    d = NestWhile[#1 + 1 & , Round[n/3], GCD[\[Lambda], #1] =!= 1 &]
    e = PowerMod[d, -1, \[Lambda]]
    enc = PowerMod[#, e, n] &;
    dec = PowerMod[#, d, n] &;
    c = ConstantArray[48, 8 1024];
    t = Table[c // enc // dec; // AbsoluteTiming, {10}][[All, 1]]

Timings both in Haskell (`m8`) and Mathematica cases are similar:

    {0.313015, 0.302337, 0.303766, 0.303321, 0.303018, 0.302574, \
    0.302511, 0.303958, 0.301411, 0.300820}

Is 300ms per 8192-bytes-long message an acceptable performance for RSA? How do OpenSSL or other implementations compare?

(Test rig: 64-bit linux; 4xCORE, Intel(R) Core(TM) i5 CPU       M 430  @ 2.27GHz)",5
4120286,11/07/2010 23:29:02,455497,09/22/2010 20:07:00,55,0,Designing with performance in mind: how fast is the average computer in America?, how fast is the average computer in America?,performance,,,,,11/07/2010 23:36:23,not a real question,1,9,13,Designing with performance in mind: how fast is the average computer in America?  how fast is the average computer in America?,1
7086009,08/16/2011 22:46:39,816721,06/27/2011 03:24:50,1,0,Performance and weirness of HashMap,"Nested HashMap performance.

HashMap I have the following:

`HashMap <String, HashMap <String, HashMap <String,String>>> table = new HashMap <String,     HashMap <String, HashMap <String, String> >>();`


this is right?",performance,hashmap,,,,08/17/2011 04:00:06,not a real question,1,29,5,"Performance and weirness of HashMap Nested HashMap performance.

HashMap I have the following:

`HashMap <String, HashMap <String, HashMap <String,String>>> table = new HashMap <String,     HashMap <String, HashMap <String, String> >>();`


this is right?",2
8040532,11/07/2011 17:55:28,1034225,11/07/2011 17:43:04,1,0,exe file download speed from Tomcat on CentOS,"I've a VPS server with CentOS 5.6 and Tomcat 6.0.26 installed.

When I download an exe large file from Tomcat server, the download speed is very low (50Kb). From my computer I can download on other servers at 500Kb.

I'm thinking that is something limiting the connection speed because if I download the same file with two independent connections, each has the some speed (50Kb), so it seems that there are some limit for each connection.

Is there any configuration on CentOS or Tomcat that limits the download speed for each connection? How can increase the download speed?

And for the upload speed? I've downloaded and uploaded files with FileZilla with a SSH connection, and the speed is very low.

I've searched on Internet and posted an entry on centos forum https://www.centos.org/modules/newbb/viewtopic.php?topic_id=33979&forum=43&post_id=146411#forumpost146411, but I can't find any response.

Thank you very much.",performance,tomcat,centos,bandwidth,vps,11/18/2011 16:06:42,off topic,1,136,8,"exe file download speed from Tomcat on CentOS I've a VPS server with CentOS 5.6 and Tomcat 6.0.26 installed.

When I download an exe large file from Tomcat server, the download speed is very low (50Kb). From my computer I can download on other servers at 500Kb.

I'm thinking that is something limiting the connection speed because if I download the same file with two independent connections, each has the some speed (50Kb), so it seems that there are some limit for each connection.

Is there any configuration on CentOS or Tomcat that limits the download speed for each connection? How can increase the download speed?

And for the upload speed? I've downloaded and uploaded files with FileZilla with a SSH connection, and the speed is very low.

I've searched on Internet and posted an entry on centos forum https://www.centos.org/modules/newbb/viewtopic.php?topic_id=33979&forum=43&post_id=146411#forumpost146411, but I can't find any response.

Thank you very much.",5
11276899,06/30/2012 19:08:50,231624,12/14/2009 21:01:40,362,42,"Node.js memory, GC and performance","There are rumors that current Node.js (or, more exactly V8 GC) performs badly when there are lots of JS objects and memory used. 

Can You please explain what exatly is the problem - lots of objects or lots of properties on one object (or array)?

Maybe there are some benchmarks, would be interesting to see actual code and numbers.

As far as I know the main problem - lots of properties on one object, not lots of objects itself (although I'm not sure). 
If so - would be the in-memory graph database (about couple of hundreds of properties on each node at max) a good case?

Also I heard that latest versions of V8 has improved GC and that it solved some parts of this problems - is this true, and when it will be available in Node.js?",performance,node.js,memory,garbage-collection,,07/02/2012 15:21:15,not constructive,1,135,5,"Node.js memory, GC and performance There are rumors that current Node.js (or, more exactly V8 GC) performs badly when there are lots of JS objects and memory used. 

Can You please explain what exatly is the problem - lots of objects or lots of properties on one object (or array)?

Maybe there are some benchmarks, would be interesting to see actual code and numbers.

As far as I know the main problem - lots of properties on one object, not lots of objects itself (although I'm not sure). 
If so - would be the in-memory graph database (about couple of hundreds of properties on each node at max) a good case?

Also I heard that latest versions of V8 has improved GC and that it solved some parts of this problems - is this true, and when it will be available in Node.js?",4
6938697,08/04/2011 09:02:55,631973,02/24/2011 08:52:09,1,0,GWTupload performance,Is there anyone checked the performance of gwtupload. I want to switch from my normal fileUpload to gwtupload just scared about the performance as there will be lot of ping from client to server to check the status.,performance,gwt,file-upload,upload,,,open,0,38,2,GWTupload performance Is there anyone checked the performance of gwtupload. I want to switch from my normal fileUpload to gwtupload just scared about the performance as there will be lot of ping from client to server to check the status.,4
6856560,07/28/2011 09:10:14,66854,02/16/2009 08:26:11,67,0,Any free tool for testing RTSP Performance,"Is there a free tool available to test RTSP performance . Jmeter unfortunately does not support rtsp . 

so , if there is a free tool out there which can test rtsp . please let me know .

thanks in advance ",performance,rtsp,,,,,open,0,41,7,"Any free tool for testing RTSP Performance Is there a free tool available to test RTSP performance . Jmeter unfortunately does not support rtsp . 

so , if there is a free tool out there which can test rtsp . please let me know .

thanks in advance ",2
10942011,06/08/2012 01:29:08,1443488,06/08/2012 01:22:40,1,0,Speed of an object in pygame?,"I am writing a simple pygame program that only consists of moving a box around the screen. The box moves very fast and I want to know how to control the speed. In my code the updated position is moved by 1 and not smaller because if the number is not an integer it makes things more complicated.

    import os, sys
    import pygame
    from pygame.locals import *
    
    pygame.init()
    mainClock = pygame.time.Clock()
    
    WINDOWWIDTH = 400
    WINDOWHEIGHT = 400
    windowSurface = pygame.display.set_mode((WINDOWWIDTH, WINDOWHEIGHT), 0, 32)
    pygame.display.set_caption(""Box"")
    
    BLACK = (0, 0, 0)
    RED = (255, 0, 0)
    WHITE = (255, 255, 255)
    size1 = 20
    size2 = 2
    #character = pygame.Rect(30, 30, 20, 30)
    player = pygame.Surface((40,40))
    
    
    
    
    pos1 = 100
    pos2 = 100
    
    
    MOVESPEED = 6
    
    x = 1
    
    while True:
        if pos1 == WINDOWWIDTH - 40 and pos1 > 0:
            pos1 -= 1
            x += 1
        elif pos1 < WINDOWWIDTH - 40 and x == 1:
            pos1 += 1
        elif x ==2:
            pos1 -= 1
            
            
        for event in pygame.event.get():
            if event.type == QUIT:
                pygame.quit()
                sys.exit()
            if event.type == KEYDOWN:
                if event.key == K_LEFT:
                    
                    pos1 -= 5
                if event.key == K_RIGHT:
                    pos1 += 4
   
    
        windowSurface.fill(WHITE)
        
        #screen.blit(character)
    
        
    
        windowSurface.blit(player, (pos1, pos2))
        pygame.display.update()
",performance,object,pygame,,,,open,0,599,6,"Speed of an object in pygame? I am writing a simple pygame program that only consists of moving a box around the screen. The box moves very fast and I want to know how to control the speed. In my code the updated position is moved by 1 and not smaller because if the number is not an integer it makes things more complicated.

    import os, sys
    import pygame
    from pygame.locals import *
    
    pygame.init()
    mainClock = pygame.time.Clock()
    
    WINDOWWIDTH = 400
    WINDOWHEIGHT = 400
    windowSurface = pygame.display.set_mode((WINDOWWIDTH, WINDOWHEIGHT), 0, 32)
    pygame.display.set_caption(""Box"")
    
    BLACK = (0, 0, 0)
    RED = (255, 0, 0)
    WHITE = (255, 255, 255)
    size1 = 20
    size2 = 2
    #character = pygame.Rect(30, 30, 20, 30)
    player = pygame.Surface((40,40))
    
    
    
    
    pos1 = 100
    pos2 = 100
    
    
    MOVESPEED = 6
    
    x = 1
    
    while True:
        if pos1 == WINDOWWIDTH - 40 and pos1 > 0:
            pos1 -= 1
            x += 1
        elif pos1 < WINDOWWIDTH - 40 and x == 1:
            pos1 += 1
        elif x ==2:
            pos1 -= 1
            
            
        for event in pygame.event.get():
            if event.type == QUIT:
                pygame.quit()
                sys.exit()
            if event.type == KEYDOWN:
                if event.key == K_LEFT:
                    
                    pos1 -= 5
                if event.key == K_RIGHT:
                    pos1 += 4
   
    
        windowSurface.fill(WHITE)
        
        #screen.blit(character)
    
        
    
        windowSurface.blit(player, (pos1, pos2))
        pygame.display.update()
",3
6169458,05/29/2011 18:07:03,393406,07/16/2010 02:04:05,1271,78,Common causes for memory leaks,"What are the most common/frequent memory leaks programmers use to run into when developing applications?

Interested in all kind of languages, would be nice if this could evolve into a `community wiki` where to see fixes for common leaks in different languages.",performance,memory-leaks,,,,05/29/2011 19:08:55,not a real question,1,41,5,"Common causes for memory leaks What are the most common/frequent memory leaks programmers use to run into when developing applications?

Interested in all kind of languages, would be nice if this could evolve into a `community wiki` where to see fixes for common leaks in different languages.",2
6909,08/09/2008 20:46:07,527,08/06/2008 14:44:09,87,22,Improving Productivity of my Teams,"Last year we were focused on improve our technical capacities and leveraging our knowledge of various architectures and frameworks and methodologies. 

This year (August mark the start of a new accounting year) we need to improve our productivity. By this I don't means we need to get more KLOCs per month per programmer. What we need is to get more artifacts finished, stables, and returning in the promised market value. 

For artifacts I refer to every deliverable product that we produce as part of a given project: 
<ul>
<li>Software programs</li>
<li>Automatized Test Cases</li>
<li>User's Manuals<li>
<li>etc</li>
</ul>

Can you tell what things are improved your performance as solo programmer or as a member of a heterogeneous team?",performance,productivity,team,teamwork,,05/05/2012 19:03:22,off topic,1,111,5,"Improving Productivity of my Teams Last year we were focused on improve our technical capacities and leveraging our knowledge of various architectures and frameworks and methodologies. 

This year (August mark the start of a new accounting year) we need to improve our productivity. By this I don't means we need to get more KLOCs per month per programmer. What we need is to get more artifacts finished, stables, and returning in the promised market value. 

For artifacts I refer to every deliverable product that we produce as part of a given project: 
<ul>
<li>Software programs</li>
<li>Automatized Test Cases</li>
<li>User's Manuals<li>
<li>etc</li>
</ul>

Can you tell what things are improved your performance as solo programmer or as a member of a heterogeneous team?",4
7429835,09/15/2011 11:12:36,521836,11/26/2010 22:14:02,1,0,Best performance virtualisation - guests for development,"I'm looking for advice on performance of virtualisation platforms with guests being used for development. I need to shift workflows regularly between Windows XP, FreeBSD and Linux running simulators and dev platforms.

I have found VirtualBox running on a lightweight Debian host to suddenly crawl when running three VM's together. The simulators are very low resource but I have a low spec host which due to geographical constraints cannot be upgraded yet (4GB Ram, Core2Duo E6600).

I'm trying to decide whether to use XEN, KVM, VMware esxi, VirtualBox or VMWare Workstation. Ideally I will be able to just CTRL-Fx to switch VM's but really need the best performance system-wise. 3D Graphics are not a requirement.

Opinions, advice and constructive critisism all accepted.",performance,embedded,virtualization,,,09/15/2011 22:21:48,not constructive,1,119,7,"Best performance virtualisation - guests for development I'm looking for advice on performance of virtualisation platforms with guests being used for development. I need to shift workflows regularly between Windows XP, FreeBSD and Linux running simulators and dev platforms.

I have found VirtualBox running on a lightweight Debian host to suddenly crawl when running three VM's together. The simulators are very low resource but I have a low spec host which due to geographical constraints cannot be upgraded yet (4GB Ram, Core2Duo E6600).

I'm trying to decide whether to use XEN, KVM, VMware esxi, VirtualBox or VMWare Workstation. Ideally I will be able to just CTRL-Fx to switch VM's but really need the best performance system-wise. 3D Graphics are not a requirement.

Opinions, advice and constructive critisism all accepted.",3
9568991,03/05/2012 15:12:46,1152174,01/16/2012 15:44:09,3,0,high-performance WebGL framework,"I have a mesh with about 108000 triangles that should be rendered with WebGL.

At the moment I use no framework, just pure WebGL. I've already implemented object recognition via id mapping for picking with callback functionality as well as a basic camera manipulator. 

Now I want to switch over to a WebGL framework for maintenance issues.

I've already tried Three.js, but it was to slow for large meshes. Do you know a suitable WebGL framework for large meshes?",performance,frameworks,webgl,mesh,,,open,0,77,3,"high-performance WebGL framework I have a mesh with about 108000 triangles that should be rendered with WebGL.

At the moment I use no framework, just pure WebGL. I've already implemented object recognition via id mapping for picking with callback functionality as well as a basic camera manipulator. 

Now I want to switch over to a WebGL framework for maintenance issues.

I've already tried Three.js, but it was to slow for large meshes. Do you know a suitable WebGL framework for large meshes?",4
5706390,04/18/2011 17:11:35,23903,09/16/2008 16:05:24,18193,380,GPGPU: Still Bleeding Edge?,"Is GPGPU ready for production and prototyping use, or would you still consider it mostly a research/bleeding edge technology?  I work in the computational biology field and it's starting to attract attention from the more computer science oriented people in the field, but most of the work seems to be porting well-known algorithms.  The porting of the algorithm is itself the research project and the vast majority of people in the field don't know much about it.  

I do some pretty computationally intensive projects on conventional multicores.  I'm wondering how close GPGPU is to being usable enough for prototyping new algorithms, and for everyday production use.  From reading Wikipedia, I get the impression that the programming model is strange (heavily SIMD) and somewhat limited (no recursion or virtual functions, though these limitations are slowly being removed; no languages higher level than C or a limited subset of C++), and that there are several competing, incompatible standards.  I also get the impression that, unlike regular multicore, fine-grained parallelism is the only game in town.  Basic library functions would need to be rewritten.  Unlike with conventional multicore, you can't get huge speedups just by parallelizing the outer loop of your program and calling old-school serial library functions.

How severe are these limitations in practice?  Is GPGPU ready for serious use now?  If not, how long would you guess it will take?",performance,cuda,future,gpgpu,bleeding-edge,10/25/2011 02:27:15,not constructive,1,238,4,"GPGPU: Still Bleeding Edge? Is GPGPU ready for production and prototyping use, or would you still consider it mostly a research/bleeding edge technology?  I work in the computational biology field and it's starting to attract attention from the more computer science oriented people in the field, but most of the work seems to be porting well-known algorithms.  The porting of the algorithm is itself the research project and the vast majority of people in the field don't know much about it.  

I do some pretty computationally intensive projects on conventional multicores.  I'm wondering how close GPGPU is to being usable enough for prototyping new algorithms, and for everyday production use.  From reading Wikipedia, I get the impression that the programming model is strange (heavily SIMD) and somewhat limited (no recursion or virtual functions, though these limitations are slowly being removed; no languages higher level than C or a limited subset of C++), and that there are several competing, incompatible standards.  I also get the impression that, unlike regular multicore, fine-grained parallelism is the only game in town.  Basic library functions would need to be rewritten.  Unlike with conventional multicore, you can't get huge speedups just by parallelizing the outer loop of your program and calling old-school serial library functions.

How severe are these limitations in practice?  Is GPGPU ready for serious use now?  If not, how long would you guess it will take?",5
11179235,06/24/2012 16:40:10,1132904,01/05/2012 19:11:29,1,0,cost of == operator vs < or > operators,"This is really just sort of an academic question, I'm just curious to know which one is faster. I'm guessing the difference is negligible but still, I'd like to know.

    if( (x == 1) || (x == 2) )

vs

    if (x < 3)

thanks!",performance,operators,micro-optimization,,,06/25/2012 03:32:26,not constructive,1,49,9,"cost of == operator vs < or > operators This is really just sort of an academic question, I'm just curious to know which one is faster. I'm guessing the difference is negligible but still, I'd like to know.

    if( (x == 1) || (x == 2) )

vs

    if (x < 3)

thanks!",3
10344647,04/27/2012 04:15:57,626068,02/21/2011 06:04:55,18,0,how to decide the speed of the Animate Slide,"i want to know the  speed of animate slide  which give the user best experience, does any one have some suggestion ?  as a web developer we always use some slideshow  in  our products, but i am not sure the use like the slide animate , meanwhile , does the speed is so fast that they don't care  the animate .  so my question  is  how should we decide the speed of animate speed ",performance,animate,slide,user-experience,,05/04/2012 11:05:52,off topic,1,84,9,"how to decide the speed of the Animate Slide i want to know the  speed of animate slide  which give the user best experience, does any one have some suggestion ?  as a web developer we always use some slideshow  in  our products, but i am not sure the use like the slide animate , meanwhile , does the speed is so fast that they don't care  the animate .  so my question  is  how should we decide the speed of animate speed ",4
9045080,01/28/2012 12:00:32,1168610,01/25/2012 06:43:18,8,0,MaxSteps and Computing time issue for Solving Differential equation in Mathematica,"When we solve differential equation numerically using NDSolve then sometimes we get error like **NDSolve::mxst:** _Maximum steps reached_

According to mathematica doc the solution is to increase MaxSteps. For example if you used MaxSteps -> 100 and limit of x 0 to 100 but Mathematica calculated upto the x=50 then increasing steps MaxSteps -> 200  will solve your problem. 

My problem is what i got from mathematica is -

**During evaluation of In[212]:= NDSolve::mxst: Maximum number of 10000 steps reached at the point x == 2.1685790754404513`*^-14.**

So even if i want to got from limit 0 to 2 for x  then my steps should be 10^14 times larger . It will take a huge huge computing time. How much it may take for core i3 processor? **Is there any other way to compute quickly or solve this problem of maxsteps in any other way?**


",performance,math,mathematica,,,01/28/2012 15:47:59,off topic,1,143,11,"MaxSteps and Computing time issue for Solving Differential equation in Mathematica When we solve differential equation numerically using NDSolve then sometimes we get error like **NDSolve::mxst:** _Maximum steps reached_

According to mathematica doc the solution is to increase MaxSteps. For example if you used MaxSteps -> 100 and limit of x 0 to 100 but Mathematica calculated upto the x=50 then increasing steps MaxSteps -> 200  will solve your problem. 

My problem is what i got from mathematica is -

**During evaluation of In[212]:= NDSolve::mxst: Maximum number of 10000 steps reached at the point x == 2.1685790754404513`*^-14.**

So even if i want to got from limit 0 to 2 for x  then my steps should be 10^14 times larger . It will take a huge huge computing time. How much it may take for core i3 processor? **Is there any other way to compute quickly or solve this problem of maxsteps in any other way?**


",3
9938878,03/30/2012 07:44:55,1302702,03/30/2012 07:40:36,1,0,"What‘s do ""Performance isolation"" and ""Fairness isolation"" mean?",Are there any information about the two terms? Please let me know. Thanks!,performance,isolation,,,,03/31/2012 23:25:29,off topic,1,13,8,"What‘s do ""Performance isolation"" and ""Fairness isolation"" mean? Are there any information about the two terms? Please let me know. Thanks!",2
7625119,10/02/2011 07:33:32,371082,06/19/2010 15:58:11,78,1,How to perform Server Performance Audit,"Here is the situation. I am the CIO in my work place. The guy on the management team that get to take decision regarding Techy things. I dont do actual development, but we have an IT service provider that does this.

We have an internal web application we use for our internal process. The problem is the application does not perform at its best.

The application is web based and it runs on a Jboss installation and the DB is mysql

What i want to do is to make a request for a formal performance report from the IT service provider, so i can have an overview understanding of the architecture, how it is configured and maybe i can identify where we can make changes.

I have some indices i want to ask for, but am not sure the list is exhaustive. Right now this are the things i want to ask for:

1)Hardware specifications (Memory, processing speed, SWAP file size etc) 2)Details of how Application Server connects with DB server (i know this can affect performance but what exactly should i look for here? ) 3) Report on the present traffic i.e. how many requests to we have per sec etc

That is all. And i am not familiar with J2ee so i am not sure if i have the right questions :(

Please are there more questions to add to this list? General questions or ones specific to J2ee?

Your assistance would be appreciated.",performance,java-ee,database-performance,,,10/02/2011 10:08:08,off topic,1,238,6,"How to perform Server Performance Audit Here is the situation. I am the CIO in my work place. The guy on the management team that get to take decision regarding Techy things. I dont do actual development, but we have an IT service provider that does this.

We have an internal web application we use for our internal process. The problem is the application does not perform at its best.

The application is web based and it runs on a Jboss installation and the DB is mysql

What i want to do is to make a request for a formal performance report from the IT service provider, so i can have an overview understanding of the architecture, how it is configured and maybe i can identify where we can make changes.

I have some indices i want to ask for, but am not sure the list is exhaustive. Right now this are the things i want to ask for:

1)Hardware specifications (Memory, processing speed, SWAP file size etc) 2)Details of how Application Server connects with DB server (i know this can affect performance but what exactly should i look for here? ) 3) Report on the present traffic i.e. how many requests to we have per sec etc

That is all. And i am not familiar with J2ee so i am not sure if i have the right questions :(

Please are there more questions to add to this list? General questions or ones specific to J2ee?

Your assistance would be appreciated.",3
10448819,05/04/2012 12:29:40,1099631,12/15/2011 10:26:18,109,3,How large eCommerce sites are so fast?,"I have been working on asp.net mvc3 e-commerce application based on NopCommerce.

Recently we have been working on performance side to improve the performance of the site. The site have more than 200000 products and 1200 categories and large no. of brands. This efforts has resulted into 20 to 200 times the performance increment, but still when bench marked against the e-commerce majors like, amazon, flipkart, jabong, letsbuy, ebay, shopping.indiatimes.com,etc it is still slower.

Now looking at just few sites, like Amazon, Flipkart & Jabong... Their page starts loading with almost zero waiting time, and images and other resources load almost instantaneously.

Also for search, Nop Commerce is dyeing slow, just look at the FlipKart & Jabong or Amazon, they very fast. No clue how? What do they do? Do they perform a search in db or something else?

My question is what do they do to have this kind of superb performance? I know they have load balancing servers with mem cached like memory implementation implemented to cache may be the entire site. 

But what are the best practices for creating such large scalable web site? And how do they do it? Are any of them use any opensource e-commerce platform like nop commerce or magento as their base? Or do they all prefer custom made?

Would like to learn how to scale a web application like them with its best practices to implement. (Note that this is a general question and not a nop-commerce related one, it is one of the best e-commerce application we used till date.)

Thanks",performance,architecture,e-commerce,scalability,,05/09/2012 20:36:16,not a real question,1,255,7,"How large eCommerce sites are so fast? I have been working on asp.net mvc3 e-commerce application based on NopCommerce.

Recently we have been working on performance side to improve the performance of the site. The site have more than 200000 products and 1200 categories and large no. of brands. This efforts has resulted into 20 to 200 times the performance increment, but still when bench marked against the e-commerce majors like, amazon, flipkart, jabong, letsbuy, ebay, shopping.indiatimes.com,etc it is still slower.

Now looking at just few sites, like Amazon, Flipkart & Jabong... Their page starts loading with almost zero waiting time, and images and other resources load almost instantaneously.

Also for search, Nop Commerce is dyeing slow, just look at the FlipKart & Jabong or Amazon, they very fast. No clue how? What do they do? Do they perform a search in db or something else?

My question is what do they do to have this kind of superb performance? I know they have load balancing servers with mem cached like memory implementation implemented to cache may be the entire site. 

But what are the best practices for creating such large scalable web site? And how do they do it? Are any of them use any opensource e-commerce platform like nop commerce or magento as their base? Or do they all prefer custom made?

Would like to learn how to scale a web application like them with its best practices to implement. (Note that this is a general question and not a nop-commerce related one, it is one of the best e-commerce application we used till date.)

Thanks",4
6755813,07/20/2011 00:48:15,853039,07/20/2011 00:38:40,1,0,Why are numbers rounded in standard computer algorithms?,"I understand that this makes the algorithms faster and use less storage space, and that these would have been critical features for software to run on the hardware of previous decades, but is this still an important feature? If the calculations were done with exact rational arithmetic then there would be no rounding errors at all, which would simplify many algorithms as you would no longer have to worry about catastrophic cancellation or anything like that. ",performance,,,,,07/20/2011 09:31:54,off topic,1,77,8,"Why are numbers rounded in standard computer algorithms? I understand that this makes the algorithms faster and use less storage space, and that these would have been critical features for software to run on the hardware of previous decades, but is this still an important feature? If the calculations were done with exact rational arithmetic then there would be no rounding errors at all, which would simplify many algorithms as you would no longer have to worry about catastrophic cancellation or anything like that. ",1
7971638,11/01/2011 19:17:15,41540,11/28/2008 07:34:28,2433,171,What is the most performant way to get data of one rails application by another rails application?,"I have two rails applications (both now on Rails 3.1.1), and they work nicely. However, I have a dependence between the two. Application `A` uses data of application `B` by linking to it. These links are created automatically, but they have to be computed by doing a lookup to the data of application `B`. I'm working on Windows 7 with Ruby 1.9.2, and this will not be changed :-(

I have tried the following:

 * Use just a RESTful resource, so defined a controller, called its action (`get_xml_obj` with some params in it), read the needed values from the XML. Worked, but needs around 0.5s to 1s per call.
 * Replaced it by [ActiveResource#find][1] which worked as well, but with the same performance as the solution before.

So is there something I can do to tune the retrieval (without accessing the database directly)? I use a cache for the retrieved information, so it gets better over time, but 1 second is too much to wait for. And there may be more than 1 or 2 links in a page I want to render.


  [1]: http://api.rubyonrails.org/classes/ActiveResource/Base.html#method-c-find",performance,rest,ruby-on-rails-3.1,activeresource,,,open,0,184,17,"What is the most performant way to get data of one rails application by another rails application? I have two rails applications (both now on Rails 3.1.1), and they work nicely. However, I have a dependence between the two. Application `A` uses data of application `B` by linking to it. These links are created automatically, but they have to be computed by doing a lookup to the data of application `B`. I'm working on Windows 7 with Ruby 1.9.2, and this will not be changed :-(

I have tried the following:

 * Use just a RESTful resource, so defined a controller, called its action (`get_xml_obj` with some params in it), read the needed values from the XML. Worked, but needs around 0.5s to 1s per call.
 * Replaced it by [ActiveResource#find][1] which worked as well, but with the same performance as the solution before.

So is there something I can do to tune the retrieval (without accessing the database directly)? I use a cache for the retrieved information, so it gets better over time, but 1 second is too much to wait for. And there may be more than 1 or 2 links in a page I want to render.


  [1]: http://api.rubyonrails.org/classes/ActiveResource/Base.html#method-c-find",4
10722284,05/23/2012 14:41:17,593649,01/28/2011 10:39:50,10,1,Sending file via XmlHttp.Send is extremely slow,"I'm working on an Office Add-in, which is supposed to upload Outlook .msg files to a server. It's a VB6 project, and I'm using the MSXML2.xmlHTTP.Send method to send the data.

        
    Set xmlHTTP = New MSXML2.xmlHTTP
    Set adoStream = New ADODB.Stream
    adoStream.Type = adTypeBinary
    adoStream.Open
    adoStream.LoadFromFile filePath    
    Dim url As String

    '...(other stuff here)...
    
    xmlHTTP.Open ""POST"", url, False
    xmlHTTP.setRequestHeader ""Content-Length"", adoStream.size
    
    'Get the data and upload it:
    Dim dataObj As Variant
    dataObj = adoStream.Read(-1)
    xmlHTTP.Send dataObj
    adoStream.Close

It works OK with small files. However, it gets extremely slow with bigger files. For example, sending a .msg with two 4 MB attachments (so a total of 8 MB) takes 6-7 minutes.
The 

        xmlHTTP.Send dataObj

statement needs all that time to finish.

The server is a VM on the same PC, on the same network. I'm sure it isn't a network problem, since the VM can pull files from the network at ~20 MB/s.

On the server side, an ASP.NET application calls the Request.SaveAs(...) method to save the data.

When I set a watch on dataObj right after the 

        dataObj = adoStream.Read(-1)
statement, and expand the watch by clicking on the + sign near it, the variable seems to contain many thousands of 200-ish byte 'chunks'. Is that normal, or could that be the cause of the slowdown?

If we can't fix this, I'm willing to try different approaches for uploading the data. Please write some suggestions. Thank you!",performance,vb6,xmlhttprequest,httprequest,xmlhttp,06/04/2012 16:40:48,too localized,1,307,7,"Sending file via XmlHttp.Send is extremely slow I'm working on an Office Add-in, which is supposed to upload Outlook .msg files to a server. It's a VB6 project, and I'm using the MSXML2.xmlHTTP.Send method to send the data.

        
    Set xmlHTTP = New MSXML2.xmlHTTP
    Set adoStream = New ADODB.Stream
    adoStream.Type = adTypeBinary
    adoStream.Open
    adoStream.LoadFromFile filePath    
    Dim url As String

    '...(other stuff here)...
    
    xmlHTTP.Open ""POST"", url, False
    xmlHTTP.setRequestHeader ""Content-Length"", adoStream.size
    
    'Get the data and upload it:
    Dim dataObj As Variant
    dataObj = adoStream.Read(-1)
    xmlHTTP.Send dataObj
    adoStream.Close

It works OK with small files. However, it gets extremely slow with bigger files. For example, sending a .msg with two 4 MB attachments (so a total of 8 MB) takes 6-7 minutes.
The 

        xmlHTTP.Send dataObj

statement needs all that time to finish.

The server is a VM on the same PC, on the same network. I'm sure it isn't a network problem, since the VM can pull files from the network at ~20 MB/s.

On the server side, an ASP.NET application calls the Request.SaveAs(...) method to save the data.

When I set a watch on dataObj right after the 

        dataObj = adoStream.Read(-1)
statement, and expand the watch by clicking on the + sign near it, the variable seems to contain many thousands of 200-ish byte 'chunks'. Is that normal, or could that be the cause of the slowdown?

If we can't fix this, I'm willing to try different approaches for uploading the data. Please write some suggestions. Thank you!",5
123387,09/23/2008 19:50:46,22809,09/26/2008 18:51:08,1,0,IIS performance problem trying to implement an XMPP-like protocol,"we're have a client that needs to get interactive messages from a server, from clients that are distributed around the world behind all kinds of firewalls with all kinds of ports closed. The only thing we can rely on is HTTP port 80 (and HTTPS 443).

The design is basically modeled after XMPP (the Jabber protocol), using our client and IIS. The client issues GET requests to a .NET Handler; the handler holds the request open for a while looking for messages. If any messages arrive, they are immediately sent to the client; if not, after a timeout the connection is closed with a ""no-data"" response. The client immediately reopens the communication.

Well, theoretically.

What's actually happening is first, IIS can't handle more than about 100 simultaneous requests - others are all queued, and there can be a several minute lag between ""connected"" and IIS recognizing that the client called in. Second, about half the time the client times out without any response from the server (the client timeout is five minutes longer than the server's).

POST always works. Other data served on the same web server works. Web services on the same server work. This is an out-of-the-box installation on Windows 2K3 Server.

Is there a configuration option we're missing, or is there something else I should look at to address this?

Thanks.",performance,iis,xmpp,httphandler,,,open,0,218,9,"IIS performance problem trying to implement an XMPP-like protocol we're have a client that needs to get interactive messages from a server, from clients that are distributed around the world behind all kinds of firewalls with all kinds of ports closed. The only thing we can rely on is HTTP port 80 (and HTTPS 443).

The design is basically modeled after XMPP (the Jabber protocol), using our client and IIS. The client issues GET requests to a .NET Handler; the handler holds the request open for a while looking for messages. If any messages arrive, they are immediately sent to the client; if not, after a timeout the connection is closed with a ""no-data"" response. The client immediately reopens the communication.

Well, theoretically.

What's actually happening is first, IIS can't handle more than about 100 simultaneous requests - others are all queued, and there can be a several minute lag between ""connected"" and IIS recognizing that the client called in. Second, about half the time the client times out without any response from the server (the client timeout is five minutes longer than the server's).

POST always works. Other data served on the same web server works. Web services on the same server work. This is an out-of-the-box installation on Windows 2K3 Server.

Is there a configuration option we're missing, or is there something else I should look at to address this?

Thanks.",4
4324876,12/01/2010 13:36:32,353652,05/29/2010 15:41:20,199,12,How to find an element in a linked list of blocks (containing n elements) as fast as possible?,"My data structure is a linked list of blocks. A block contains 31 elements of 4 byte and one 4 byte pointer to the next block or NULL(in summary 128 bytes per block). I add elements from time to time. If the last block is full, I add another block via pointer.

One objective is to use as less memory (= blocks) as possible and having no free space between two elements in a block.

This setting is fix. All code runs on a 32-bit ARM Cortex-A8 CPU with NEON pipeline.

***Question:***
How to find a specific element in that data structure as quickly as possible?

***Approach (right now):***
I use sorted blocks and binary search to check for an element (9 bit of the 4 byte are the search criteria). If the desired element is not in the current block I jump to the next block. If the element is not in the last block and the last block is not yet full, I use the result of the binary search to insert the new element (if necessary I make space using memmove within this block). Thus all blocks are always sorted.

Do you have an idea to make that faster?",performance,linked-list,arm,bit-manipulation,,,open,0,195,18,"How to find an element in a linked list of blocks (containing n elements) as fast as possible? My data structure is a linked list of blocks. A block contains 31 elements of 4 byte and one 4 byte pointer to the next block or NULL(in summary 128 bytes per block). I add elements from time to time. If the last block is full, I add another block via pointer.

One objective is to use as less memory (= blocks) as possible and having no free space between two elements in a block.

This setting is fix. All code runs on a 32-bit ARM Cortex-A8 CPU with NEON pipeline.

***Question:***
How to find a specific element in that data structure as quickly as possible?

***Approach (right now):***
I use sorted blocks and binary search to check for an element (9 bit of the 4 byte are the search criteria). If the desired element is not in the current block I jump to the next block. If the element is not in the last block and the last block is not yet full, I use the result of the binary search to insert the new element (if necessary I make space using memmove within this block). Thus all blocks are always sorted.

Do you have an idea to make that faster?",4
11628335,07/24/2012 09:52:36,1166503,01/24/2012 07:34:38,11,1,Capacity planning for an enterprise software application,"If you had to do capacity planning and hardware sizing ""BEFORE"" you had a chance to actually code and test the application (typically while you are defining solution architecture), how would you do it? 

I know this can not be known accurately beforehand but the point is to present the approach at an early stage (including questions you need to ask, assumptions you need to make). 

All you konw, it will be an enterprise java application with App server, Web Server, Database. Business has given some number of concurrent ""USERES"" say 1000. Also assume that you will get a chance to fine tune your numbers after load testing the application but you can't be far off from the original estimates.",performance,sizing,capacity,,,07/24/2012 15:17:03,off topic,1,120,7,"Capacity planning for an enterprise software application If you had to do capacity planning and hardware sizing ""BEFORE"" you had a chance to actually code and test the application (typically while you are defining solution architecture), how would you do it? 

I know this can not be known accurately beforehand but the point is to present the approach at an early stage (including questions you need to ask, assumptions you need to make). 

All you konw, it will be an enterprise java application with App server, Web Server, Database. Business has given some number of concurrent ""USERES"" say 1000. Also assume that you will get a chance to fine tune your numbers after load testing the application but you can't be far off from the original estimates.",3
10972320,06/10/2012 21:00:54,68105,02/18/2009 22:50:24,7036,346,hard disk performance,"The page of MyDefrag states that

[On most harddisks the beginning of the harddisk is considerably faster than the end, sometimes by as much as 200 percent!][1] 

Is this true?  False?  What is this based on?  I'm looking for some explanation of why this might be true.  What does ""beginning of the harddisk"" even mean?  Inner cylinders?  Does the inner (smaller) cylinder actually contain less information that the outside longer cylinder?  


  [1]: http://www.mydefrag.com/",performance,hardware,harddrive,,,06/10/2012 21:38:29,off topic,1,81,3,"hard disk performance The page of MyDefrag states that

[On most harddisks the beginning of the harddisk is considerably faster than the end, sometimes by as much as 200 percent!][1] 

Is this true?  False?  What is this based on?  I'm looking for some explanation of why this might be true.  What does ""beginning of the harddisk"" even mean?  Inner cylinders?  Does the inner (smaller) cylinder actually contain less information that the outside longer cylinder?  


  [1]: http://www.mydefrag.com/",3
5276809,03/11/2011 18:23:28,519836,11/25/2010 08:15:45,753,16,What dpes it mean % Cpu Utilization in application runtime statistics?,"It might seem a stupid question but I really start thought about it and didn't get an immediate answer just by myself.

When I run an application and, at the same time, I use a runtime evaluator in order to profile my program, I get, at the end of the process, many statistics. One of this is the Cpu utilization time.

Typically what is the Cpu utilization time? Well you might tell me the percantage calculated dividing the global time that process spent in cpu by the overall simulation time. Well, unfortunately my statistics are very deep and the program I am using is very precise and gets me a chart about the cpu utilization time.

So in my chart I have on x axis the time and on y axis the cpu utilization time in %.

So something like this:

    cpu%
    ^
    |
    |
    |             * 
    |          *    *                           *
    |      *             *                   *
    |    *                    *          *   
    |   *                      *       * 
    | *                         *     *
    |*                             *
    -------------------------------------------------> time

So, what does it mean???????? How should I interpret the following sentence?

> ""The cpu utilization percentage for
> process 'MyProcess' at time '5.23 s'
> is 12%""

",performance,statistics,processes,cpu-usage,,,open,0,433,11,"What dpes it mean % Cpu Utilization in application runtime statistics? It might seem a stupid question but I really start thought about it and didn't get an immediate answer just by myself.

When I run an application and, at the same time, I use a runtime evaluator in order to profile my program, I get, at the end of the process, many statistics. One of this is the Cpu utilization time.

Typically what is the Cpu utilization time? Well you might tell me the percantage calculated dividing the global time that process spent in cpu by the overall simulation time. Well, unfortunately my statistics are very deep and the program I am using is very precise and gets me a chart about the cpu utilization time.

So in my chart I have on x axis the time and on y axis the cpu utilization time in %.

So something like this:

    cpu%
    ^
    |
    |
    |             * 
    |          *    *                           *
    |      *             *                   *
    |    *                    *          *   
    |   *                      *       * 
    | *                         *     *
    |*                             *
    -------------------------------------------------> time

So, what does it mean???????? How should I interpret the following sentence?

> ""The cpu utilization percentage for
> process 'MyProcess' at time '5.23 s'
> is 12%""

",4
1390745,09/07/2009 19:48:59,22471,09/25/2008 23:40:51,1458,33,What is a simple example of replacing c code with assembly to improve performance?,"I've heard that game developers will sometimes replace parts of inner loops w/ assembly code to improve the performance.  

What is a simple example of this?

Where does the assembly go?  Just inline w/ the c code?

Thanks!",performance,c,,,,,open,0,38,14,"What is a simple example of replacing c code with assembly to improve performance? I've heard that game developers will sometimes replace parts of inner loops w/ assembly code to improve the performance.  

What is a simple example of this?

Where does the assembly go?  Just inline w/ the c code?

Thanks!",2
10232825,04/19/2012 16:45:20,334911,05/06/2010 22:31:09,907,20,Does jumping around in a program hurt performance due to caching issues,"I know there are memory caches so that using values from a lot of disparate points in mameroy causes cache misses and hurts performance. As a program is executed is it actually loaded from memory into the cpu in small blocks similar to an L1 cache? I ask because presumably then jumping around in memory frequently would cause misses on this cache and hurt performance. So I guess it is two questions: is there such an ""execution cache"" and does jumping around frequently hurt performance.

P.S. Not sure what appropriate tags are for this other than performance and caching. Feel free to re-tag.",performance,caching,,,,,open,0,102,12,"Does jumping around in a program hurt performance due to caching issues I know there are memory caches so that using values from a lot of disparate points in mameroy causes cache misses and hurts performance. As a program is executed is it actually loaded from memory into the cpu in small blocks similar to an L1 cache? I ask because presumably then jumping around in memory frequently would cause misses on this cache and hurt performance. So I guess it is two questions: is there such an ""execution cache"" and does jumping around frequently hurt performance.

P.S. Not sure what appropriate tags are for this other than performance and caching. Feel free to re-tag.",2
9770369,03/19/2012 12:39:53,1253460,03/06/2012 23:23:27,3,0,Making localhost slower,"It might look stupid but I want to make my localhost slower, I'm working on a project and I want to test it on slow connections too, is there anyway to actually do this?",performance,localhost,,,,03/19/2012 16:12:46,not a real question,1,34,3,"Making localhost slower It might look stupid but I want to make my localhost slower, I'm working on a project and I want to test it on slow connections too, is there anyway to actually do this?",2
4673706,01/12/2011 20:31:01,118218,06/05/2009 18:42:23,742,36,Slow down CPU to simulate slower computers in browser testing,"I'm trying to see how our web pages behave on an average customer's computer. We have not yet pinned down this configuration, but it's likely to be slower than what our developers and testers will have. 

I've seen answers to similar questions that suggest throttling bandwidth and using a VM where the memory has been limited, but do I also need to slow down the CPU? I am under the impression that the CPU will run fairly close to full speed, even in a VM. Are there virtual machine platforms that allow you to limit the CPU cycles? I saw one suggestion to run something like Folding @ Home, but I would welcome other suggestions to throttle the CPU speed.

I've seen this question: http://stackoverflow.com/questions/1997089/how-to-slow-down-the-browser, and others that talk about limiting bandwidth.",performance,testing,virtualization,,,,open,0,131,10,"Slow down CPU to simulate slower computers in browser testing I'm trying to see how our web pages behave on an average customer's computer. We have not yet pinned down this configuration, but it's likely to be slower than what our developers and testers will have. 

I've seen answers to similar questions that suggest throttling bandwidth and using a VM where the memory has been limited, but do I also need to slow down the CPU? I am under the impression that the CPU will run fairly close to full speed, even in a VM. Are there virtual machine platforms that allow you to limit the CPU cycles? I saw one suggestion to run something like Folding @ Home, but I would welcome other suggestions to throttle the CPU speed.

I've seen this question: http://stackoverflow.com/questions/1997089/how-to-slow-down-the-browser, and others that talk about limiting bandwidth.",3
3439840,08/09/2010 12:10:38,415041,08/09/2010 11:57:54,1,0,Is it possible to measure the performance of the language ?,"Do most languages have many different execution engines ?
",performance,programming-languages,,,,08/09/2010 19:50:44,not a real question,1,9,11,"Is it possible to measure the performance of the language ? Do most languages have many different execution engines ?
",2
8628733,12/25/2011 07:50:42,21501,09/24/2008 05:54:22,5588,102,Benchmarking CPU-bound algorithms/implementations,"Let's say I'm writing my own `StringBuilder` in a compiled language (e.g. C++).

What is the best way to measure the performance of various implementations? Simply timing a few hundred thousand runs yields highly inconsistent results: the timings from one batch to the other can differ by as much as 15%, making it impossible to accurately assess potential performance improvements that yield performance gains smaller than that.

I've done the following:

1. Disable SpeedStep
2. Use RDTSC for timing
3. Run the process with realtime priority
4. Set the affinity to a single CPU core

This stabilizied the results somewhat. Any other ideas?",performance,benchmarking,cpu,,,,open,0,96,3,"Benchmarking CPU-bound algorithms/implementations Let's say I'm writing my own `StringBuilder` in a compiled language (e.g. C++).

What is the best way to measure the performance of various implementations? Simply timing a few hundred thousand runs yields highly inconsistent results: the timings from one batch to the other can differ by as much as 15%, making it impossible to accurately assess potential performance improvements that yield performance gains smaller than that.

I've done the following:

1. Disable SpeedStep
2. Use RDTSC for timing
3. Run the process with realtime priority
4. Set the affinity to a single CPU core

This stabilizied the results somewhat. Any other ideas?",3
6607078,07/07/2011 07:08:38,783842,06/04/2011 10:35:01,6,1,"How to judge my procedure bottleneck in CPU, the memory or the network card?","How to judge my procedure bottleneck in CPU, the memory or the network card? which tools do you used? Thanks!",performance,,,,,07/07/2011 13:20:06,not a real question,1,20,14,"How to judge my procedure bottleneck in CPU, the memory or the network card? How to judge my procedure bottleneck in CPU, the memory or the network card? which tools do you used? Thanks!",1
11189745,06/25/2012 12:52:25,290082,03/09/2010 23:31:23,1479,1,Real time data processing,"I am parsing keywords several times per second. Every second i have 1000 - 5000 keywords. So i want to find outlier, growing and other stuff which called technical analysis. One of the problem is how to store data. 
I will be able to do someting like:

             20-01 20-02 20-03 
    brother    0      3     4
    table      1      0     0
    cup        34     54    78


But it might be a lot of keywords. For every new part of data i need to look is this word exists? If donnt then i must to add new words and add new rows for them. What is right way to organize store? Should i use key\value database, NoSQL or something else? ",performance,database-design,real-time,,,,open,0,177,4,"Real time data processing I am parsing keywords several times per second. Every second i have 1000 - 5000 keywords. So i want to find outlier, growing and other stuff which called technical analysis. One of the problem is how to store data. 
I will be able to do someting like:

             20-01 20-02 20-03 
    brother    0      3     4
    table      1      0     0
    cup        34     54    78


But it might be a lot of keywords. For every new part of data i need to look is this word exists? If donnt then i must to add new words and add new rows for them. What is right way to organize store? Should i use key\value database, NoSQL or something else? ",3
6180515,05/30/2011 20:28:29,721759,04/23/2011 13:24:58,1,0,I don't understand this performance problem,"Hope you can help me to clarify my doubts! 

I'm running a process with grails to load info from a spreadsheet into a data base.

My local machine has 4GB RAM and iCore7 1.73GHZ processor
The server machine has 2GB RAM and a Intel E7400 2.8GHZ Dual Core
Both with a 500GB hard drive

Below you can see the time in seconds to load different information from the spread sheet into the DB.


SERVER UBUNTU 9.04 64BIT

LOAD DICTIONARY TABLES STARTING...
LOAD DICTIONARY TABLES : TOTAL Processing time = 5.31
2011-05-30 11:49:39,210 [main] DEBUG dataImport.CatalogueDataLoader - LOADING CATALOGUE...

2011-05-30 11:49:39,582 [main] DEBUG dataImport.CatalogueDataLoader - CATALOGUE LOAD : TOTAL Processing time 0.371 


LOCAL UBUNTU 10.10 64BIT

LOAD DICTIONARY TABLES STARTING...
LOAD DICTIONARY TABLES : TOTAL Processing time = 32.641
2011-05-30 12:36:38,875 [main] DEBUG dataImport.CatalogueDataLoader - LOADING CATALOGUE...

2011-05-30 12:36:40,214 [main] DEBUG dataImport.CatalogueDataLoader - CATALOGUE LOAD : TOTAL Processing time 1.338 



The CATALOGUE LOAD : TOTAL is just for 1 line of the spreadsheet but I have to load around 7K lines, so the time difference is important. In my machine is taking more than 1 hour and in the server is just 10min. It does not make sense I think.


Can someone help me to understand this and maybe suggest some solutions to that?

Thanks!",performance,grails,hardware,intel,ubuntu-10.10,05/31/2011 09:23:53,not a real question,1,199,6,"I don't understand this performance problem Hope you can help me to clarify my doubts! 

I'm running a process with grails to load info from a spreadsheet into a data base.

My local machine has 4GB RAM and iCore7 1.73GHZ processor
The server machine has 2GB RAM and a Intel E7400 2.8GHZ Dual Core
Both with a 500GB hard drive

Below you can see the time in seconds to load different information from the spread sheet into the DB.


SERVER UBUNTU 9.04 64BIT

LOAD DICTIONARY TABLES STARTING...
LOAD DICTIONARY TABLES : TOTAL Processing time = 5.31
2011-05-30 11:49:39,210 [main] DEBUG dataImport.CatalogueDataLoader - LOADING CATALOGUE...

2011-05-30 11:49:39,582 [main] DEBUG dataImport.CatalogueDataLoader - CATALOGUE LOAD : TOTAL Processing time 0.371 


LOCAL UBUNTU 10.10 64BIT

LOAD DICTIONARY TABLES STARTING...
LOAD DICTIONARY TABLES : TOTAL Processing time = 32.641
2011-05-30 12:36:38,875 [main] DEBUG dataImport.CatalogueDataLoader - LOADING CATALOGUE...

2011-05-30 12:36:40,214 [main] DEBUG dataImport.CatalogueDataLoader - CATALOGUE LOAD : TOTAL Processing time 1.338 



The CATALOGUE LOAD : TOTAL is just for 1 line of the spreadsheet but I have to load around 7K lines, so the time difference is important. In my machine is taking more than 1 hour and in the server is just 10min. It does not make sense I think.


Can someone help me to understand this and maybe suggest some solutions to that?

Thanks!",5
5092211,02/23/2011 14:29:47,419872,08/13/2010 18:24:13,1,2,Session optimization in Jetty + Wicket,"Is there a way to swap sessions onto disk with jetty if they are idle for n.n. minutes?

It is under investigation, but we suspect that we have many idle users logged in with large session size. So, while they are doing nothing, their session could be pushed onto disk.

Is there a setting or utility or way to achieve this? 

    We are using wicket+jetty.

Simply shortening session destroy timeout is not an option.",performance,optimization,jetty,,,,open,0,75,6,"Session optimization in Jetty + Wicket Is there a way to swap sessions onto disk with jetty if they are idle for n.n. minutes?

It is under investigation, but we suspect that we have many idle users logged in with large session size. So, while they are doing nothing, their session could be pushed onto disk.

Is there a setting or utility or way to achieve this? 

    We are using wicket+jetty.

Simply shortening session destroy timeout is not an option.",3
7901814,10/26/2011 11:11:16,795160,06/12/2011 22:38:53,1,0,Debugging Entity Framework SQL statements,"I am using the shared webshop nopcommerce and I have tried asking this question on their forums without any luck. Hoping to get a more general answer, I am asking it here as well.

I am having a weird pattern of response time when using the Entity Framework for SQL communication.

This is from my web host - http://i55.tinypic.com/263uiag.png

This is from my local server - http://i53.tinypic.com/102t4xe.png

It's the increase in response time I am worried about.
I have narrowed the problem down to one single line in code
Nop.Data > EfRepository.cs > public void Insert(T entity) > _entities.Add(entity);
Yes I know this very specific for the NopCommerce, but the point is really that I am looking her for help on how to debug this.

Are there some events I can catch that display the SQL being executed?
Or what other things can I do to find out more what is actually happening in the Entity Framework in that above command.
",performance,entity-framework,nopcommerce,,,,open,0,152,5,"Debugging Entity Framework SQL statements I am using the shared webshop nopcommerce and I have tried asking this question on their forums without any luck. Hoping to get a more general answer, I am asking it here as well.

I am having a weird pattern of response time when using the Entity Framework for SQL communication.

This is from my web host - http://i55.tinypic.com/263uiag.png

This is from my local server - http://i53.tinypic.com/102t4xe.png

It's the increase in response time I am worried about.
I have narrowed the problem down to one single line in code
Nop.Data > EfRepository.cs > public void Insert(T entity) > _entities.Add(entity);
Yes I know this very specific for the NopCommerce, but the point is really that I am looking her for help on how to debug this.

Are there some events I can catch that display the SQL being executed?
Or what other things can I do to find out more what is actually happening in the Entity Framework in that above command.
",3
10595171,05/15/2012 06:23:08,1117127,12/27/2011 05:46:24,131,8,Does Number of commits can affect svn performance,"I was about to commit about 1000 files at once after few refactoring stuff. Is it advisable to commit such huge number of files or I should commit them in batches. I am trying to look at pros and cons sort of. 

One of the pros is that I will have same entry in the SVN for all my changes and will be easy to navigate. ",performance,svn,advice,,,,open,0,67,8,"Does Number of commits can affect svn performance I was about to commit about 1000 files at once after few refactoring stuff. Is it advisable to commit such huge number of files or I should commit them in batches. I am trying to look at pros and cons sort of. 

One of the pros is that I will have same entry in the SVN for all my changes and will be easy to navigate. ",3
10511477,05/09/2012 07:18:51,355715,06/01/2010 18:13:43,1,0,Play Framework: Rare Slow Requests,"I'm testing Play! Framework (2.0, using Scala) to evaluate it for a project. It's pleasantly simple to develop in and I'm pretty happy with it, but while doing a stress test I was a little surprised by some results. 

With an application that takes a POST request, parses the body, and then just returns an OK, latency was usually very low (99% <20ms). But using siege (like ab), over five minutes with 100 concurrents a few requests (~.1%) were taking 3-5 seconds. All the requests were identical.

I was running the application in production mode. The server has six cores and over 10GB of RAM and the machine running the stress tester is on the same network, so I don't think it's a system bottleneck or issue.

That's a pretty low rate of long requests, but is it to be expected? When I tried logging the time taken to complete requests inside Play! I didn't find any requests taking a long time in my code, so I'm not sure how to diagnose the cause of this. ",performance,scala,playframework,playframework-2.x,,05/09/2012 15:50:09,not constructive,1,175,5,"Play Framework: Rare Slow Requests I'm testing Play! Framework (2.0, using Scala) to evaluate it for a project. It's pleasantly simple to develop in and I'm pretty happy with it, but while doing a stress test I was a little surprised by some results. 

With an application that takes a POST request, parses the body, and then just returns an OK, latency was usually very low (99% <20ms). But using siege (like ab), over five minutes with 100 concurrents a few requests (~.1%) were taking 3-5 seconds. All the requests were identical.

I was running the application in production mode. The server has six cores and over 10GB of RAM and the machine running the stress tester is on the same network, so I don't think it's a system bottleneck or issue.

That's a pretty low rate of long requests, but is it to be expected? When I tried logging the time taken to complete requests inside Play! I didn't find any requests taking a long time in my code, so I'm not sure how to diagnose the cause of this. ",4
10063212,04/08/2012 13:37:27,867814,07/28/2011 15:30:16,141,8,Would C/C++/Pascal/etc. be substantially slower if it was a stackless language?,"And a related question: How would stack traces/similar debugging features look like in it?

And please excuse me if this is a stupid question, but I don't know much about low-level programming. I know most CPUs have instructions related to the stack, but would a properly optimized stackless language really be that much slower?",performance,language-design,stackless,,,04/10/2012 04:38:23,not constructive,1,53,11,"Would C/C++/Pascal/etc. be substantially slower if it was a stackless language? And a related question: How would stack traces/similar debugging features look like in it?

And please excuse me if this is a stupid question, but I don't know much about low-level programming. I know most CPUs have instructions related to the stack, but would a properly optimized stackless language really be that much slower?",3
3290391,07/20/2010 13:26:27,278470,02/22/2010 05:31:46,250,6,improving site performance..,"I have a site [http://www.ratingscorner.com/colleges][1]


  [1]: http://www.ratingscorner.com/colleges


i have an issue like performance. I think the site is slow. Please see if the speed is feasible . Its on a shared server. what things should i do to increase site/DB performance.",performance,,,,,07/20/2010 13:33:36,off topic,1,41,3,"improving site performance.. I have a site [http://www.ratingscorner.com/colleges][1]


  [1]: http://www.ratingscorner.com/colleges


i have an issue like performance. I think the site is slow. Please see if the speed is feasible . Its on a shared server. what things should i do to increase site/DB performance.",1
71199,09/16/2008 10:57:17,11530,09/16/2008 08:57:05,31,6,What makes you lose motivation?,What environmental factors make you lose your motivation?,performance,productivity,motivation,,,07/10/2012 21:44:31,off topic,1,8,5,What makes you lose motivation? What environmental factors make you lose your motivation?,3
3858288,10/04/2010 19:04:08,305644,03/31/2010 02:40:33,3034,227,node.js misconceptions?,"From the node.js page

""Almost no function in Node directly performs I/O, so the process never blocks. Because nothing blocks, less-than-expert programmers are able to develop fast systems.""

so if a less than expert programmer does something like start an infinite loop in the callback, it doesn't kill the system (eventually)?",performance,node.js,misconceptions,,,10/05/2010 23:06:32,not a real question,1,49,2,"node.js misconceptions? From the node.js page

""Almost no function in Node directly performs I/O, so the process never blocks. Because nothing blocks, less-than-expert programmers are able to develop fast systems.""

so if a less than expert programmer does something like start an infinite loop in the callback, it doesn't kill the system (eventually)?",3
10269198,04/22/2012 15:23:06,242933,01/04/2010 02:28:02,6694,191,When to index a Core Data attribute?,"In which cases should I index a Core Data attribute?

1. When I'm sorting by it?

        fetchRequest.sortDescriptors =
        [NSArray arrayWithObject:[NSSortDescriptor
                                  sortDescriptorWithKey:@""name"" ascending:YES
                                  selector:@selector(localizedStandardCompare:)]];

2. When I'm filtering by it?

        fetchRequest.predicate = [NSPredicate predicateWithFormat:
                                  @""name BEGINSWITH[cd] %@"", searchString];

3. Any other cases?",performance,core-data,indexing,nspredicate,nssortdescriptor,,open,0,158,7,"When to index a Core Data attribute? In which cases should I index a Core Data attribute?

1. When I'm sorting by it?

        fetchRequest.sortDescriptors =
        [NSArray arrayWithObject:[NSSortDescriptor
                                  sortDescriptorWithKey:@""name"" ascending:YES
                                  selector:@selector(localizedStandardCompare:)]];

2. When I'm filtering by it?

        fetchRequest.predicate = [NSPredicate predicateWithFormat:
                                  @""name BEGINSWITH[cd] %@"", searchString];

3. Any other cases?",5
4062019,10/31/2010 05:57:21,178033,09/23/2009 19:37:42,47,1,good resources/books on profiling software,"Can anyone suggest some good resources and/or books on profiling, importance of profiling, profiling techniques?

thanks",performance,profiling,measurement,,,09/24/2011 14:55:22,not constructive,1,15,5,"good resources/books on profiling software Can anyone suggest some good resources and/or books on profiling, importance of profiling, profiling techniques?

thanks",3
708670,04/02/2009 07:33:43,77184,03/12/2009 12:31:28,500,41,How many requests per second does your webapp serve?,"I'am interested in some statistical data gathering about requests per second (the definition of requests per second i use is 'a user requests a certain functionality provided by an URL' (this means requests != hits (one request may consist of multiple hits))). So how many requests per second and server do you serve? If you dont maintain your own app, perhaps you know values from other apps.

Please provide us with a short description of the application (lots of reads, lots of write, Web2.0 stuff ...) and perhaps a source (if you correspond to a foreign app).

Thanks a lot, i'am really interested in this.",performance,web-applications,request,,,05/07/2012 11:18:25,not constructive,1,103,9,"How many requests per second does your webapp serve? I'am interested in some statistical data gathering about requests per second (the definition of requests per second i use is 'a user requests a certain functionality provided by an URL' (this means requests != hits (one request may consist of multiple hits))). So how many requests per second and server do you serve? If you dont maintain your own app, perhaps you know values from other apps.

Please provide us with a short description of the application (lots of reads, lots of write, Web2.0 stuff ...) and perhaps a source (if you correspond to a foreign app).

Thanks a lot, i'am really interested in this.",3
8368924,12/03/2011 16:05:26,35696,11/08/2008 03:38:52,2856,145,PostgreSQL performance on Windows using latest versions,"An old [question][1] brought up the performance issues of PostgreSQL on Windows.  There's been some major improvements to PostgreSQL since that time... has the performance gap improved between Windows and the Linux varieties?

  [1]: http://stackoverflow.com/questions/1162206/why-is-postgresql-so-slow-on-windows",performance,postgresql,,,,12/04/2011 09:04:32,off topic,1,37,7,"PostgreSQL performance on Windows using latest versions An old [question][1] brought up the performance issues of PostgreSQL on Windows.  There's been some major improvements to PostgreSQL since that time... has the performance gap improved between Windows and the Linux varieties?

  [1]: http://stackoverflow.com/questions/1162206/why-is-postgresql-so-slow-on-windows",2
6885059,07/30/2011 18:13:38,688332,04/01/2011 21:47:36,25,0,Why is deleting TinyMCE files from my disk so slow?,"Does anybody know why deleting tinyMCE direcory is so slow? It drives me nuts. No matter if in Joomla, Drupal or standalone unzipped archive. I have 7200rpm standard disc i3 intel proc. 6 months old machine. Is it fast, howevwer deleting TinyMCE is very slow.

Any explanation why?",performance,tinymce,harddisk,,,07/30/2011 18:56:28,off topic,1,47,10,"Why is deleting TinyMCE files from my disk so slow? Does anybody know why deleting tinyMCE direcory is so slow? It drives me nuts. No matter if in Joomla, Drupal or standalone unzipped archive. I have 7200rpm standard disc i3 intel proc. 6 months old machine. Is it fast, howevwer deleting TinyMCE is very slow.

Any explanation why?",3
942941,06/03/2009 03:28:34,47637,12/19/2008 03:31:07,22,3,how to do performance and scalability testing without clear requirments?,"Guys, any idea how to do performance and scalability testing if there is no clear performance requirements has been defined? Thanks!",performance,scalability,testing,,,,open,0,21,10,"how to do performance and scalability testing without clear requirments? Guys, any idea how to do performance and scalability testing if there is no clear performance requirements has been defined? Thanks!",3
7454814,09/17/2011 12:42:20,847064,07/15/2011 19:34:20,22,1,Papers/ Books about load management (for DBs and OSs),"I am looking for books or papers about **load management** (is that the correct English term for it?). Especially related with database systems and operating systems.

I want to answer questions like:

 - How does a database manages the queue of requests?
 - How can a system be aware, that an application consumes only a defined percentage of a resource (e.g. maximum 50% of the CPU)?
 - How can load management be implemented on the client-side instead of the server? Advantages? Disadvantages? 

I am looking for theoretical material, like general design patterns or best practices.

Any ideas? Thanx. ",performance,books,management,patterns,,10/02/2011 22:24:53,off topic,1,97,9,"Papers/ Books about load management (for DBs and OSs) I am looking for books or papers about **load management** (is that the correct English term for it?). Especially related with database systems and operating systems.

I want to answer questions like:

 - How does a database manages the queue of requests?
 - How can a system be aware, that an application consumes only a defined percentage of a resource (e.g. maximum 50% of the CPU)?
 - How can load management be implemented on the client-side instead of the server? Advantages? Disadvantages? 

I am looking for theoretical material, like general design patterns or best practices.

Any ideas? Thanx. ",4
6723640,07/17/2011 11:50:40,673726,03/23/2011 19:18:20,603,53,database access VS lan access,"In a program, there are times when I can retrieve the same data either on a shared database or on a server connected in a lan. Which one should I choose regarding speed (and other considerations maybe) ?

An opened connection to the database AND an opened socket are already available when I need to make that decision.
",performance,,,,,07/17/2011 12:04:48,not a real question,1,57,5,"database access VS lan access In a program, there are times when I can retrieve the same data either on a shared database or on a server connected in a lan. Which one should I choose regarding speed (and other considerations maybe) ?

An opened connection to the database AND an opened socket are already available when I need to make that decision.
",1
7606721,09/30/2011 06:32:28,210342,11/13/2009 10:35:11,1242,40,Why JVM-based applications are slow -- IntelliJ vs. VisualStudio?,"The good reading for start:
http://stackoverflow.com/questions/2163411/why-did-java-have-the-reputation-of-being-slow

However I am not interested what people THINK, or what was state of the art 10 years ago. I am interested about facts -- what I see now.

### Piece of background

Currently my primary language is C#, VisualStudio is my IDE, but since the limitations of C# are more and more painful, and since I prefer Linux to Windows, I am not even the user of my own programs, I tried/try to learn Scala. The IDE in this case is IntelliJ IDEA (but I tried Eclipse too).

### Visual comparison

What is striking me, that for example:

* on the same machine (Windows 7, Core 2 Duo, 2.66 GHz, 3GB RAM, 32-bit) Visual Studio is slow, IntelliJ is dead, dead slow, I type faster than it shows the letter, it is visually obvious that every tooltip is shown with some struggle

* at home, with VS running on Windows 7 (8GB RAM, 2 cores, 64-Bit, Quad CPU, 2.96 GHz something like this) is on par with IntelliJ (16GB RAM, 4 cores, 64-bit, same clock, running openSUSE 11.4) -- both are slow; however the trick is, VS is running within VirtualBox installed on top of this Linux box

Now I scratch my head and I start wonder, if switching from C# to Scala is really a good idea, because even on mechanical side, I can write (hit the keys) program faster in C# than in Scala (to be exact: using VS vs. IntelliJ). Another point -- every program using technology X stands for that technology (it is kind of advertisement). So in this respect, IntelliJ does a very poor work.

*Of course, it is not only problem with IntelliJ, I didn't see **any** GUI JVM-based program that feels snappy.*

### Question

If you could make an educated guess, what are the reasons Visual Studio is a magnitude faster than IntelliJ, what would it be:

* it is fault of JetBrains,
* it is fault of Swing,
* Visual Studio uses undocumented C ""for"" loops (little conspiracy is not bad -- just kidding),
* JVM is not that mature after all,
* GC is a useful thing, but cannot match with manual allocating/freeing memory,
* ?

**There has to a be a reason for such difference in application speeds, so what it is?** Please let's focus on Visual Studio vs. IntelliJ IDEA.",performance,visual-studio-2010,jvm,intellij-idea,,09/30/2011 09:20:07,not constructive,1,378,9,"Why JVM-based applications are slow -- IntelliJ vs. VisualStudio? The good reading for start:
http://stackoverflow.com/questions/2163411/why-did-java-have-the-reputation-of-being-slow

However I am not interested what people THINK, or what was state of the art 10 years ago. I am interested about facts -- what I see now.

### Piece of background

Currently my primary language is C#, VisualStudio is my IDE, but since the limitations of C# are more and more painful, and since I prefer Linux to Windows, I am not even the user of my own programs, I tried/try to learn Scala. The IDE in this case is IntelliJ IDEA (but I tried Eclipse too).

### Visual comparison

What is striking me, that for example:

* on the same machine (Windows 7, Core 2 Duo, 2.66 GHz, 3GB RAM, 32-bit) Visual Studio is slow, IntelliJ is dead, dead slow, I type faster than it shows the letter, it is visually obvious that every tooltip is shown with some struggle

* at home, with VS running on Windows 7 (8GB RAM, 2 cores, 64-Bit, Quad CPU, 2.96 GHz something like this) is on par with IntelliJ (16GB RAM, 4 cores, 64-bit, same clock, running openSUSE 11.4) -- both are slow; however the trick is, VS is running within VirtualBox installed on top of this Linux box

Now I scratch my head and I start wonder, if switching from C# to Scala is really a good idea, because even on mechanical side, I can write (hit the keys) program faster in C# than in Scala (to be exact: using VS vs. IntelliJ). Another point -- every program using technology X stands for that technology (it is kind of advertisement). So in this respect, IntelliJ does a very poor work.

*Of course, it is not only problem with IntelliJ, I didn't see **any** GUI JVM-based program that feels snappy.*

### Question

If you could make an educated guess, what are the reasons Visual Studio is a magnitude faster than IntelliJ, what would it be:

* it is fault of JetBrains,
* it is fault of Swing,
* Visual Studio uses undocumented C ""for"" loops (little conspiracy is not bad -- just kidding),
* JVM is not that mature after all,
* GC is a useful thing, but cannot match with manual allocating/freeing memory,
* ?

**There has to a be a reason for such difference in application speeds, so what it is?** Please let's focus on Visual Studio vs. IntelliJ IDEA.",4
9236052,02/10/2012 23:04:30,487113,10/26/2010 01:38:07,201,3,shortcomings/limitations of wordpress?,I've read a few comments of people on here alluding to specific performance/security issues with using performance. I've also read comments of people 'having to' switch to drupal or something else. I myself find it confusing to have a page that's generated from the database. Just curious if people can share some of their specific problems. ,performance,security,wordpress,content-management-system,,02/12/2012 08:11:45,not constructive,1,57,3,shortcomings/limitations of wordpress? I've read a few comments of people on here alluding to specific performance/security issues with using performance. I've also read comments of people 'having to' switch to drupal or something else. I myself find it confusing to have a page that's generated from the database. Just curious if people can share some of their specific problems. ,4
9529155,03/02/2012 06:43:48,390452,07/13/2010 12:52:43,681,35,Site slowness issue,"I am juggling with the problem from past week, My site http://demo.cupidlies.com/ is too slow to load on browser. I have applied all the possible solution like

1)gZip
2) compress css/js
3) minimize queries

but still its taking to much time to load, I am not able to figure it out, even some sites that check sites' speeds can not figure how much time its taking to load.
Its a very weird problem, expecting your help to find any solution to make its speed at least average.

Thanks",performance,zend-framework,website,,,03/02/2012 08:13:45,not a real question,1,82,3,"Site slowness issue I am juggling with the problem from past week, My site http://demo.cupidlies.com/ is too slow to load on browser. I have applied all the possible solution like

1)gZip
2) compress css/js
3) minimize queries

but still its taking to much time to load, I am not able to figure it out, even some sites that check sites' speeds can not figure how much time its taking to load.
Its a very weird problem, expecting your help to find any solution to make its speed at least average.

Thanks",3
11296885,07/02/2012 15:41:14,1496492,07/02/2012 15:34:58,1,0,why do Performance testing at protocol level,I was told that it is better to performance test at protocol level. Can anyone explain why this is?,performance,performance-testing,loadrunner,,,07/02/2012 17:14:30,not a real question,1,19,7,why do Performance testing at protocol level I was told that it is better to performance test at protocol level. Can anyone explain why this is?,3
7309554,09/05/2011 14:38:28,440410,09/06/2010 07:03:52,233,9,A chess board representation in Haskell,"I am primarily a C++/Java/Python programmer and have recently started diving into Haskell. The only other functional languages that I have ever dabbled in are Scheme and Ocaml (both of which are ""impure"").

I think I have some understanding of the purely functional programming paradigm and why it is a good thing. But some things bother me when trying to visualize implementing certain real world applications using Haskell. Also, some [posts][1] on stackoverflow seem to suggest that for certain applications, Haskell can only provide tedious work-arounds at best (which may be elegantly solved with C++ for example). This is kind of unexpected, and makes me feel that the language is not really flexible enough no matter how powerful it is in its expressiveness.

So, here is my question:

How would I go about designing a chess Board in Haskell? The requirement for the Board is that operations on it (such as Move, UnmakeLastMove, etc) need to be really fast as they are repeatedly invoked by a search function (alpha-beta for example) in a tight loop. The engine's strength depends on how deep you can look in the game tree. How would I model such a thing in a purely functional way (where state changes are disallowed) and yet have efficiency comparable to that of C++? In C++, one could implement a board by having a mutable 8x8 array. In Haskell, however, it seems like the only way is to recreate the entire board with updated state every time operations are invoked. How else can it be implemented? Is using monads necessary here? Can laziness be leveraged in some interesting manner?

  [1]: http://stackoverflow.com/questions/1255018/n-queens-in-haskell-without-list-traversal",performance,haskell,chess,,,08/01/2012 01:56:44,not constructive,1,270,6,"A chess board representation in Haskell I am primarily a C++/Java/Python programmer and have recently started diving into Haskell. The only other functional languages that I have ever dabbled in are Scheme and Ocaml (both of which are ""impure"").

I think I have some understanding of the purely functional programming paradigm and why it is a good thing. But some things bother me when trying to visualize implementing certain real world applications using Haskell. Also, some [posts][1] on stackoverflow seem to suggest that for certain applications, Haskell can only provide tedious work-arounds at best (which may be elegantly solved with C++ for example). This is kind of unexpected, and makes me feel that the language is not really flexible enough no matter how powerful it is in its expressiveness.

So, here is my question:

How would I go about designing a chess Board in Haskell? The requirement for the Board is that operations on it (such as Move, UnmakeLastMove, etc) need to be really fast as they are repeatedly invoked by a search function (alpha-beta for example) in a tight loop. The engine's strength depends on how deep you can look in the game tree. How would I model such a thing in a purely functional way (where state changes are disallowed) and yet have efficiency comparable to that of C++? In C++, one could implement a board by having a mutable 8x8 array. In Haskell, however, it seems like the only way is to recreate the entire board with updated state every time operations are invoked. How else can it be implemented? Is using monads necessary here? Can laziness be leveraged in some interesting manner?

  [1]: http://stackoverflow.com/questions/1255018/n-queens-in-haskell-without-list-traversal",3
6921700,08/03/2011 04:38:25,875872,08/03/2011 04:38:25,1,0,drupal vs. joomla which one's performance is better?,"i have a project need to start in a week, joomla and drupal are both in my candidates list. performance is key to my project, so which one you think is better for me?",performance,drupal,joomla,,,08/04/2011 22:13:26,not constructive,1,34,8,"drupal vs. joomla which one's performance is better? i have a project need to start in a week, joomla and drupal are both in my candidates list. performance is key to my project, so which one you think is better for me?",3
6812227,07/25/2011 05:53:45,777052,05/31/2011 04:04:50,81,1,Speed Advantage of Symmetric Key Encryption,We've all read that a symmetric or shared key is faster than public-private keys.  But what's the exact reason for this?  It seems to me in either case encryption and decryption logic against a key of some kind must be performed.,performance,encryption,key,advantage,symmetric,,open,0,43,6,Speed Advantage of Symmetric Key Encryption We've all read that a symmetric or shared key is faster than public-private keys.  But what's the exact reason for this?  It seems to me in either case encryption and decryption logic against a key of some kind must be performed.,5
6700052,07/14/2011 21:19:00,680877,03/28/2011 19:47:08,6,1,Understandability vs Performance,"I was having a discussion at work today about code around weather it is more important for people(especially juniors) to understand or to have high performance code.

I am all for having less efficient as long as it is easy for anyone to understand, as it code is always handed off to junior developers to support.",performance,,,,,07/14/2011 21:44:42,off topic,1,55,3,"Understandability vs Performance I was having a discussion at work today about code around weather it is more important for people(especially juniors) to understand or to have high performance code.

I am all for having less efficient as long as it is easy for anyone to understand, as it code is always handed off to junior developers to support.",1
10338774,04/26/2012 17:53:47,1359284,04/26/2012 16:59:18,1,0,Large Couch DB with slow views,"Our team is all Couch newbs.  Our views are very slow.  This view below takes a very long time or does not return a response.  We use a large doc id instead of fields, this helps us directly pull back a document which happens very fast.  Is there something we are doing wrong with our views that they are so slow?  I read it has to query every document but gets faster the more you use it?  Should we be storing information in fields to make a view that type of information faster?  Thanks.

Example query
http://server_name:5984/html_logs/_design/api/_view/getFilesByKeyword?key=[""1000""]

The PHP that creates the view
$design_document->views[""getFilesByKeyword""] = array(""map"" => ""function(doc) {\n\tvar key;\n\tif (doc._attachments) {\n\t\tvar key = doc._id.split(\""-\"");\n\t\tvar keyword = key[1];\n\t\tkey = [keyword];\n\t\tvar attachments = new Array();\n\t\tfor (var att in doc._attachments) attachments.push(att);\n\t\temit(key, {filenames:attachments});\n\t}\n}"");

Addional info
We use CouchDB on a c1.xlarge ubuntu aws server(pretty big).  We store a half a million 100k html files in couch a day.  The first thing we noticed is that 1.1 didn't work so we moved to .10(we didn't really spend too much time troubleshooting this), then we had root fill up quite fast so we moved the logging to /vol.",performance,view,couchdb,,,,open,0,196,6,"Large Couch DB with slow views Our team is all Couch newbs.  Our views are very slow.  This view below takes a very long time or does not return a response.  We use a large doc id instead of fields, this helps us directly pull back a document which happens very fast.  Is there something we are doing wrong with our views that they are so slow?  I read it has to query every document but gets faster the more you use it?  Should we be storing information in fields to make a view that type of information faster?  Thanks.

Example query
http://server_name:5984/html_logs/_design/api/_view/getFilesByKeyword?key=[""1000""]

The PHP that creates the view
$design_document->views[""getFilesByKeyword""] = array(""map"" => ""function(doc) {\n\tvar key;\n\tif (doc._attachments) {\n\t\tvar key = doc._id.split(\""-\"");\n\t\tvar keyword = key[1];\n\t\tkey = [keyword];\n\t\tvar attachments = new Array();\n\t\tfor (var att in doc._attachments) attachments.push(att);\n\t\temit(key, {filenames:attachments});\n\t}\n}"");

Addional info
We use CouchDB on a c1.xlarge ubuntu aws server(pretty big).  We store a half a million 100k html files in couch a day.  The first thing we noticed is that 1.1 didn't work so we moved to .10(we didn't really spend too much time troubleshooting this), then we had root fill up quite fast so we moved the logging to /vol.",3
7602930,09/29/2011 20:17:47,187956,10/11/2009 12:32:14,1,0,Inefficient CPU usage by Tomcat,"I am using tomcat to convert pdf to png. I was using Jmeter to load/throughput test the app using a Intel(R) Xeon(R) CPU X5550  @ 2.67GHz server which has 16 hyper-threaded cores. Tomcat6 had 2gigs of heap memory allocated, and the connector was setup to handle 1000 concurrent connections. During my load testing I discovered that as I increased users the response time degraded which is normal. The load average, however, was very low (around 1 for a 16 core machine), and cpu usage was around 60%. I was monitoring the tomcat instance with jconsole through JMX, and found that memory usage was not an issue. In order to keep the response time low, I created around 8 instances of tomcat on 8 different ports, and have a load balancer balance the load between all 8 ports during my load testing. This time I found that the server was able to handle many more concurrent connections without degraded response time. The load average was also high at 7. So, it seems like tomcat is not using the CPU efficiently when setup with only one instance (I even tried creating multiple connectors on different ports).

It seems like we need to run multiple tomcat instances on a single machine to extract the most performance. Doing this setup is not optimal since creating multiple instances on a pool of servers, and maintaining it will be a pain. 

My question is that is it possible to configure tomcat such a way so that a single instance will use as much cpu as possible with a high load average? 

Please comment if you have any question about the setup.

I'd appreciate any help/pointer.
",performance,tomcat6,load-testing,java-server,,09/30/2011 19:37:38,off topic,1,278,5,"Inefficient CPU usage by Tomcat I am using tomcat to convert pdf to png. I was using Jmeter to load/throughput test the app using a Intel(R) Xeon(R) CPU X5550  @ 2.67GHz server which has 16 hyper-threaded cores. Tomcat6 had 2gigs of heap memory allocated, and the connector was setup to handle 1000 concurrent connections. During my load testing I discovered that as I increased users the response time degraded which is normal. The load average, however, was very low (around 1 for a 16 core machine), and cpu usage was around 60%. I was monitoring the tomcat instance with jconsole through JMX, and found that memory usage was not an issue. In order to keep the response time low, I created around 8 instances of tomcat on 8 different ports, and have a load balancer balance the load between all 8 ports during my load testing. This time I found that the server was able to handle many more concurrent connections without degraded response time. The load average was also high at 7. So, it seems like tomcat is not using the CPU efficiently when setup with only one instance (I even tried creating multiple connectors on different ports).

It seems like we need to run multiple tomcat instances on a single machine to extract the most performance. Doing this setup is not optimal since creating multiple instances on a pool of servers, and maintaining it will be a pain. 

My question is that is it possible to configure tomcat such a way so that a single instance will use as much cpu as possible with a high load average? 

Please comment if you have any question about the setup.

I'd appreciate any help/pointer.
",4
7472968,09/19/2011 15:01:02,463959,10/01/2010 14:20:54,146,5,"Web development on a super fast machine... Pros/Cons, Testing for slower systems","I'm not even sure that such question is for StackOverflow, but that's the only good place that I know for asking this stuff, so I apologize in advance and I hope that it'll be useful for others as well.

I am sick of working on a computer that does not always bring me Intellisense on time, that takes a long time to switch between open windows (like to check e-mail, or sneak into facebook for a second, or switch to Photoshop CS5 from Visual Studio) - in short, a computer that is delaying my professional work and reducing my productivity... Until something opens I could even forget why I clicked that thing...

And it doesn't matter what kind of a computer I have at the moment, but my question is the following - If I'd build a very expensive, beast desktop machine with latest tech that is available today - I know for certain that it will be up to 20 times faster than any of our web-servers... - ie, ""Production"" environment... 

I can see already that I might be very disappointed in observing my web sites, web applications performing slower than on my development machine, etc..

Has any of you met with such problems ? 
Is there any way to simulate a slower environment (some Virtualization trick) ?

Please share your experience on this...",performance,virtual-machine,,,,09/19/2011 15:32:39,off topic,1,221,12,"Web development on a super fast machine... Pros/Cons, Testing for slower systems I'm not even sure that such question is for StackOverflow, but that's the only good place that I know for asking this stuff, so I apologize in advance and I hope that it'll be useful for others as well.

I am sick of working on a computer that does not always bring me Intellisense on time, that takes a long time to switch between open windows (like to check e-mail, or sneak into facebook for a second, or switch to Photoshop CS5 from Visual Studio) - in short, a computer that is delaying my professional work and reducing my productivity... Until something opens I could even forget why I clicked that thing...

And it doesn't matter what kind of a computer I have at the moment, but my question is the following - If I'd build a very expensive, beast desktop machine with latest tech that is available today - I know for certain that it will be up to 20 times faster than any of our web-servers... - ie, ""Production"" environment... 

I can see already that I might be very disappointed in observing my web sites, web applications performing slower than on my development machine, etc..

Has any of you met with such problems ? 
Is there any way to simulate a slower environment (some Virtualization trick) ?

Please share your experience on this...",2
11279788,07/01/2012 05:23:56,1124893,01/01/2012 06:14:45,11,1,"What assembler would be best fit to program an OS from scratch, with execution performance prioritized?","**I am not fit to program an OS under any circumstances, but this is hypothetical.**

I want to create a simple operating system from almost absolute scratch in assembly language. With great ambitions in mind, I want it to be as optimized as possible for execution. I also want the code to be fairly portable for multiple architectures (especially x86 and x64 interchangeably). I feel a bit minimalist, so lets toss out software ""bloat"".

What I am looking for is an assembler that can fit this criteria. Obviously, I want one that is decently documented officially or unofficially, and I need it to be decently well-known (I.E. I don't want ""ΣASM"" found in the middle of the internet Sahara.)",performance,assembly,operating-system,language,portability,07/01/2012 08:04:01,not constructive,1,117,16,"What assembler would be best fit to program an OS from scratch, with execution performance prioritized? **I am not fit to program an OS under any circumstances, but this is hypothetical.**

I want to create a simple operating system from almost absolute scratch in assembly language. With great ambitions in mind, I want it to be as optimized as possible for execution. I also want the code to be fairly portable for multiple architectures (especially x86 and x64 interchangeably). I feel a bit minimalist, so lets toss out software ""bloat"".

What I am looking for is an assembler that can fit this criteria. Obviously, I want one that is decently documented officially or unofficially, and I need it to be decently well-known (I.E. I don't want ""ΣASM"" found in the middle of the internet Sahara.)",5
10225097,04/19/2012 09:19:28,962411,09/24/2011 08:14:14,1,0,How can I check if too many / poorly written mod_rewrite rules are using up a lot of CPU?,"httpd processes are using a lot of CPU on my server. I have about 70 mode_rewrite rules without special RewriteCond directives and id evyerthing fails, I have this:

    RewriteCond %{REQUEST_FILENAME} !-f
    RewriteCond %{REQUEST_FILENAME} !-d
    RewriteRule ^([a-zA-Z0-9\-]+)(\/(.*))?$ index.php?module=appindex&list_identifier=$1&raw_query=$3 [L,QSA]

I have about 100.000 hits / day. 

Is there anyway I can log the CPU usage caused by mod_rewrite?

Thank you.",performance,mod-rewrite,httpd,,,04/19/2012 11:46:19,off topic,1,66,19,"How can I check if too many / poorly written mod_rewrite rules are using up a lot of CPU? httpd processes are using a lot of CPU on my server. I have about 70 mode_rewrite rules without special RewriteCond directives and id evyerthing fails, I have this:

    RewriteCond %{REQUEST_FILENAME} !-f
    RewriteCond %{REQUEST_FILENAME} !-d
    RewriteRule ^([a-zA-Z0-9\-]+)(\/(.*))?$ index.php?module=appindex&list_identifier=$1&raw_query=$3 [L,QSA]

I have about 100.000 hits / day. 

Is there anyway I can log the CPU usage caused by mod_rewrite?

Thank you.",3
7457016,09/17/2011 18:45:39,54783,01/13/2009 21:44:40,893,62,Fastest machine for data migration using Oracle for Windows?,"We are currently migrating our database from an in-house developed software to a vendor solution. Both softwares run under Windows and use an UNIX-based database.

There is about 25 GB of data and a lot of somewhat complex transformations. All in all, a single run requires a little more then 100 hours on a mighty PowerEdge database server and more then a week on an HP Super Dome.

More then 100 hours is very, very slow ! Each new version of the database take up to 2 weeks to produce (modify code, generate data, test, modify code again sometime, generate data again). Since we would like to spend upcoming weekends in family, we have to speed things up a little bit :)

My guest is that, since the disk *is* the bottleneck, a powerful workstation would do better then a database server. I think that we should have a maximum of RAM (64 GB, say), an two separate RAID-0 arrays of SSD : one for the database, the other for the log and redo files.

But his is just a guest. Any advice about this ? Any experience with Oracle on SSD ? Should we use different disks for the database and the others files ?

Many thanks in advance !

Note : some may suggest us to optimize our PL/SQL but it has been done. Thanks again.",performance,oracle,hardware,data-migration,,09/17/2011 22:20:37,off topic,1,222,9,"Fastest machine for data migration using Oracle for Windows? We are currently migrating our database from an in-house developed software to a vendor solution. Both softwares run under Windows and use an UNIX-based database.

There is about 25 GB of data and a lot of somewhat complex transformations. All in all, a single run requires a little more then 100 hours on a mighty PowerEdge database server and more then a week on an HP Super Dome.

More then 100 hours is very, very slow ! Each new version of the database take up to 2 weeks to produce (modify code, generate data, test, modify code again sometime, generate data again). Since we would like to spend upcoming weekends in family, we have to speed things up a little bit :)

My guest is that, since the disk *is* the bottleneck, a powerful workstation would do better then a database server. I think that we should have a maximum of RAM (64 GB, say), an two separate RAID-0 arrays of SSD : one for the database, the other for the log and redo files.

But his is just a guest. Any advice about this ? Any experience with Oracle on SSD ? Should we use different disks for the database and the others files ?

Many thanks in advance !

Note : some may suggest us to optimize our PL/SQL but it has been done. Thanks again.",4
6803876,07/23/2011 23:13:50,542687,12/14/2010 23:22:38,258,0,speed vs size vs readability,"Premise: i is some number more than 0, and you want to loop until i equals 0.

    --i; // to include or not to include?
    while ( i > 0 )
        --i;

If the first line is included, `i > 0` is checked one less time. If the first line is not included, there is one less line of code. Readability might be a factor, too.

Would you include the first line or not?

Working example: http://codepad.org/OD8ywqxY",performance,design,size,readability,,07/24/2011 04:58:26,not a real question,1,87,5,"speed vs size vs readability Premise: i is some number more than 0, and you want to loop until i equals 0.

    --i; // to include or not to include?
    while ( i > 0 )
        --i;

If the first line is included, `i > 0` is checked one less time. If the first line is not included, there is one less line of code. Readability might be a factor, too.

Would you include the first line or not?

Working example: http://codepad.org/OD8ywqxY",4
3076925,06/19/2010 19:27:15,371198,06/19/2010 19:23:55,1,0,nginx better than apache for dynamic content?,"i have searched for this around the web and i can't find the right answer for my question.

basically i want to know if i can get better performance with nginx than with apache (in php apps), and i'm not involving static content (where i know nginx is better).

the sites are a widely collection of scripts with a lot of variables, using old not OOP oriented code and new websites using classes and smarty. the sites are very dynamic, changes parts in each request. 

i want to avoid suing nginx for static content and apache for php, so for that i;m asking, if it worth the transition in performance terms.

my main confusion comes where i have seen benchmarks using wordpress and wp-supercache plugin, that could make it better for nginx than a custom sites with the features i already described. i have seen other benchmarks that just not show a big difference between them (around 5%)

thanks in advance for any help :D

Regards,
Shadow.",performance,apache,webserver,nginx,,12/06/2011 15:57:32,off topic,1,161,7,"nginx better than apache for dynamic content? i have searched for this around the web and i can't find the right answer for my question.

basically i want to know if i can get better performance with nginx than with apache (in php apps), and i'm not involving static content (where i know nginx is better).

the sites are a widely collection of scripts with a lot of variables, using old not OOP oriented code and new websites using classes and smarty. the sites are very dynamic, changes parts in each request. 

i want to avoid suing nginx for static content and apache for php, so for that i;m asking, if it worth the transition in performance terms.

my main confusion comes where i have seen benchmarks using wordpress and wp-supercache plugin, that could make it better for nginx than a custom sites with the features i already described. i have seen other benchmarks that just not show a big difference between them (around 5%)

thanks in advance for any help :D

Regards,
Shadow.",4
3121,08/06/2008 06:10:10,404,08/05/2008 13:34:59,63,7,Why are SYSTEM and taskmgr.exe taking up 100% of my CPU?,"I'm running 32-bit Windows 2003 R2 in VMware 6.0.3 on my Windows XP 32-bit host OS.  I have run all the Windows updates, but haven't done any other tweaking.  About once every hour (but not quite on a schedule) my CPU usage goes to 100%, slowing everything down for about 1 minute.  When I can get taskmgr up, I see that usually a process called ""SYSTEM"" is using most of the CPU.  Occasionally taskmgr itself is using up almost all of it.  What is causing this, and how can I avoid it?

The host machine is an older Pentium D with 3 gigs RAM.  The VM is running from a relatively fast SATA drive.  Memory usage in either the host or the guest doesn't seem to be a factor in whether or not these slowdowns will occur.

thanks!
Chris",performance,vmware,hanging,,,05/22/2012 19:21:34,off topic,1,143,11,"Why are SYSTEM and taskmgr.exe taking up 100% of my CPU? I'm running 32-bit Windows 2003 R2 in VMware 6.0.3 on my Windows XP 32-bit host OS.  I have run all the Windows updates, but haven't done any other tweaking.  About once every hour (but not quite on a schedule) my CPU usage goes to 100%, slowing everything down for about 1 minute.  When I can get taskmgr up, I see that usually a process called ""SYSTEM"" is using most of the CPU.  Occasionally taskmgr itself is using up almost all of it.  What is causing this, and how can I avoid it?

The host machine is an older Pentium D with 3 gigs RAM.  The VM is running from a relatively fast SATA drive.  Memory usage in either the host or the guest doesn't seem to be a factor in whether or not these slowdowns will occur.

thanks!
Chris",3
96977,09/18/2008 21:06:32,2577,08/23/2008 03:18:09,605,37,How can I find out what's thrashing my hard drive in XP?,"This is a programming question in the sense that I'm experiencing a hardware issue which is keeping me from being able to develop effectively...

Anyway, my PC is running insanely slow, but it doesn't appear to be a memory consumption issue or a CPU issue - no programs are running away with too much of either. But something is thrashing my hard drive - the hard drive light just stays on nearly solidly, barely flickering at all, and it never stops.

In Vista there would be a likely culprit (superfetch) as well as intrinsic tools to see what the issue is (resource manager, I believe) - but I'm running Windows XP.

Is there a way, through a utility or programmatically - to see what it is that's murdering my hard drive performance in Windows XP?",performance,windows-xp,harddrive,,,03/15/2011 17:05:08,off topic,1,132,12,"How can I find out what's thrashing my hard drive in XP? This is a programming question in the sense that I'm experiencing a hardware issue which is keeping me from being able to develop effectively...

Anyway, my PC is running insanely slow, but it doesn't appear to be a memory consumption issue or a CPU issue - no programs are running away with too much of either. But something is thrashing my hard drive - the hard drive light just stays on nearly solidly, barely flickering at all, and it never stops.

In Vista there would be a likely culprit (superfetch) as well as intrinsic tools to see what the issue is (resource manager, I believe) - but I'm running Windows XP.

Is there a way, through a utility or programmatically - to see what it is that's murdering my hard drive performance in Windows XP?",3
9862877,03/25/2012 18:38:25,1233160,02/25/2012 23:17:18,30,1,Classify points according to euclidean distance,"I have a matrix A consisting of 200 vectors of size d.

I want that a matrix B consisting of 4096 vectors gets classified to these points according to the nearest distance rule.

Thus the result should have rows of size B having the id number ( from 1 to 200 ) to which it belongs.

I have written this code via `2 for` loops and it takes lots of time for calculation.

    for i = 1:4096
            counter = 1;
            vector1 = FaceImage(:,i)';
            vector2 = Centroids(1,:);
            distance = pdist( [ vector1 ; vector2] , 'euclidean' );
            for j = 2:200
                  vector2 = Centroids(j,:);
                  temp = pdist( [ vector1 ; vector2] , 'euclidean' );
                  if temp < distance
                        distance = temp;
                        counter = j;
                  end
            end
            Histogram( i ) = counter;
    end

Can somebody help me out increasing the efficiency of the above code ... or perhaps suggest me an inbuilt function ?

Thanks",performance,matlab,,,,,open,0,345,6,"Classify points according to euclidean distance I have a matrix A consisting of 200 vectors of size d.

I want that a matrix B consisting of 4096 vectors gets classified to these points according to the nearest distance rule.

Thus the result should have rows of size B having the id number ( from 1 to 200 ) to which it belongs.

I have written this code via `2 for` loops and it takes lots of time for calculation.

    for i = 1:4096
            counter = 1;
            vector1 = FaceImage(:,i)';
            vector2 = Centroids(1,:);
            distance = pdist( [ vector1 ; vector2] , 'euclidean' );
            for j = 2:200
                  vector2 = Centroids(j,:);
                  temp = pdist( [ vector1 ; vector2] , 'euclidean' );
                  if temp < distance
                        distance = temp;
                        counter = j;
                  end
            end
            Histogram( i ) = counter;
    end

Can somebody help me out increasing the efficiency of the above code ... or perhaps suggest me an inbuilt function ?

Thanks",2
9217074,02/09/2012 19:02:51,1050659,11/16/2011 22:57:25,10,0,Guide and Resources on Modern Web Development,"I was wondering if you guys here could point me in the direction of some good guides,books,tutorials or resources regarding modern day large scale web application development.

I have some experience creating very small scale websites and playing around with wordpress.
My limited knowledge base includes.

 - HTML
 - CSS
 - Basic Javascript
 - jQuery
 - PHP

Although I know how to use these tools I have never been able to put them together very well and streamlined in a large project.

Particularly I would like information on how to build a web application that will

 - Have scalabilty
 - Maintain good performance and reliability
 - Make it easy to add in new features (modular design?)

I found this presentation 
http://www.slideshare.net/nzakas/scalable-javascript-application-architecture 

and it hit on some of the points I wanted to learn. I would like more info that builds on that.

I am open to learning new languages if that is needed and working with any sort of good frameworks out there.

Any help is appreciated! Thank You!
",performance,web-applications,architecture,scalability,,02/12/2012 08:00:09,not constructive,1,162,7,"Guide and Resources on Modern Web Development I was wondering if you guys here could point me in the direction of some good guides,books,tutorials or resources regarding modern day large scale web application development.

I have some experience creating very small scale websites and playing around with wordpress.
My limited knowledge base includes.

 - HTML
 - CSS
 - Basic Javascript
 - jQuery
 - PHP

Although I know how to use these tools I have never been able to put them together very well and streamlined in a large project.

Particularly I would like information on how to build a web application that will

 - Have scalabilty
 - Maintain good performance and reliability
 - Make it easy to add in new features (modular design?)

I found this presentation 
http://www.slideshare.net/nzakas/scalable-javascript-application-architecture 

and it hit on some of the points I wanted to learn. I would like more info that builds on that.

I am open to learning new languages if that is needed and working with any sort of good frameworks out there.

Any help is appreciated! Thank You!
",4
7878819,10/24/2011 16:32:58,15985,09/17/2008 13:55:21,3179,85,NServiceBus: How to stop distributor acting as a processing bottleneck (reduces rate 65%),"We have an event processing system that will process events sent directly from the source to handler process at 200 eps (events per second). The queues and message sends are transactional. Adding the NSB distributor between the event generator and the handler process reduces this rate from 200 eps to 70 eps. The disk usage and CPU on the distributor box become significantly higher as well.

Seen with commercial build of NServiceBus, version 2.6.0.1505.

Has anyone else seen this behaviour or have any advice?",performance,transactions,msmq,nservicebus,distributor,,open,0,82,13,"NServiceBus: How to stop distributor acting as a processing bottleneck (reduces rate 65%) We have an event processing system that will process events sent directly from the source to handler process at 200 eps (events per second). The queues and message sends are transactional. Adding the NSB distributor between the event generator and the handler process reduces this rate from 200 eps to 70 eps. The disk usage and CPU on the distributor box become significantly higher as well.

Seen with commercial build of NServiceBus, version 2.6.0.1505.

Has anyone else seen this behaviour or have any advice?",5
11434423,07/11/2012 14:03:56,623423,02/18/2011 16:00:32,25,1,Resin 2 to Tomcat 7 performance issues,"We develop and maintain a high volume web application that runs under a legacy version of Resin (2.1.17).  As part of a significant infrastructure upgrade, we are in the midst of re-hosting this application to run under Tomcat 7.0.28.   We have completed the development and configuration work, and are currently load testing the new configuration in our performance lab using Load Runner.  Despite running the load test with a wide variety of tuning changes, we are unable to complete a load test that matches the performance profile of previous Resin tests.

The main issues that we see with the failing Tomcat load tests are:
1.	Very high CPU usage by Tomcat (close to 100%, vs Resin which averages 60%)
2.	High thread usage in Tomcat.  The number of active threads used by Tomcat spikes to the maximum configured and does not go down until the test is done (kind of like a square wave).  Resin uses far fewer threads and never gets close to the maximum.

Here are a few architectural/background notes:

1.	We run Apache 2.2 on our web servers.  They serve up static content and proxy dynamic requests to the application server, which runs Tomcat.
2.	The web application uses Java 1.6, Spring, Hibernate, and a legacy version of Tapestry.
3.	We are running the Tomcat performance pack (APR).
4.	The load runner stress test runs approximately 8 hours.  At the peak of ramp-up, it creates 1600 concurrent virtual users to drive the load.
5.	In order to decouple the application from Resin SSO, we integrated CAS into our login/authorization process.  CAS runs on a separate server and does not appear to be under stress.  Outside of this change, the web application code is essentially unchanged.

My main question is:
1.	Any advice on how to troubleshoot and tune Apache/Tomcat in order to reduce thread and CPU usage?


Please let me know if you have any questions or need additional information.
",performance,tomcat7,resin,,,07/12/2012 17:07:45,off topic,1,313,7,"Resin 2 to Tomcat 7 performance issues We develop and maintain a high volume web application that runs under a legacy version of Resin (2.1.17).  As part of a significant infrastructure upgrade, we are in the midst of re-hosting this application to run under Tomcat 7.0.28.   We have completed the development and configuration work, and are currently load testing the new configuration in our performance lab using Load Runner.  Despite running the load test with a wide variety of tuning changes, we are unable to complete a load test that matches the performance profile of previous Resin tests.

The main issues that we see with the failing Tomcat load tests are:
1.	Very high CPU usage by Tomcat (close to 100%, vs Resin which averages 60%)
2.	High thread usage in Tomcat.  The number of active threads used by Tomcat spikes to the maximum configured and does not go down until the test is done (kind of like a square wave).  Resin uses far fewer threads and never gets close to the maximum.

Here are a few architectural/background notes:

1.	We run Apache 2.2 on our web servers.  They serve up static content and proxy dynamic requests to the application server, which runs Tomcat.
2.	The web application uses Java 1.6, Spring, Hibernate, and a legacy version of Tapestry.
3.	We are running the Tomcat performance pack (APR).
4.	The load runner stress test runs approximately 8 hours.  At the peak of ramp-up, it creates 1600 concurrent virtual users to drive the load.
5.	In order to decouple the application from Resin SSO, we integrated CAS into our login/authorization process.  CAS runs on a separate server and does not appear to be under stress.  Outside of this change, the web application code is essentially unchanged.

My main question is:
1.	Any advice on how to troubleshoot and tune Apache/Tomcat in order to reduce thread and CPU usage?


Please let me know if you have any questions or need additional information.
",3
7032279,08/11/2011 20:18:52,890661,08/11/2011 20:12:45,1,0,Building a High Performance Computing System,"Hypothetical Question:  

Suppose you have mountains of data and an algorithmic process that is offline and you want to optimize your hardware.

Suppose you have 50k to dispose of.

What is the best hardware setup for building an in house stack that maximizes computation power and gives you flexible memory to store large sparse matrices?

project background:  scientific research , performing machine learning on large data sets and I want to get the biggest bang for my buck.",performance,memory,stack,hardware,setup,08/12/2011 00:33:54,not constructive,1,77,6,"Building a High Performance Computing System Hypothetical Question:  

Suppose you have mountains of data and an algorithmic process that is offline and you want to optimize your hardware.

Suppose you have 50k to dispose of.

What is the best hardware setup for building an in house stack that maximizes computation power and gives you flexible memory to store large sparse matrices?

project background:  scientific research , performing machine learning on large data sets and I want to get the biggest bang for my buck.",5
11489482,07/15/2012 04:42:08,663447,03/16/2011 23:32:42,933,14,Does scp block server functions?,"I have a node.js application server. It serves my api and dynamic web pages. The database is not on the server.

I want to continuously scp files into the server. Would that cause slow downs or block web requests or would it only have a minor impact?

Thanks.",performance,node.js,scp,,,07/16/2012 12:13:01,off topic,1,46,5,"Does scp block server functions? I have a node.js application server. It serves my api and dynamic web pages. The database is not on the server.

I want to continuously scp files into the server. Would that cause slow downs or block web requests or would it only have a minor impact?

Thanks.",3
6408545,06/20/2011 08:27:10,217089,11/23/2009 15:04:10,18,1,Automated performance test for web application,"I'm currently developing a webapp with JBoss Seam, the webapp uses EJB to connect to the backend layer (J2EE + Oracle).

We are planning to use Selenium to create the functional tests, but I'd also like to have some automated performance tests for the client layer. Would it be possible to use Selenium for this or is it a bad idea?

I know tools like YSlow, PageSpeed or Firebug can be used for measuring the web performance, but is there anyway to automate this?

Thanks",performance,testing,,,,,open,0,82,6,"Automated performance test for web application I'm currently developing a webapp with JBoss Seam, the webapp uses EJB to connect to the backend layer (J2EE + Oracle).

We are planning to use Selenium to create the functional tests, but I'd also like to have some automated performance tests for the client layer. Would it be possible to use Selenium for this or is it a bad idea?

I know tools like YSlow, PageSpeed or Firebug can be used for measuring the web performance, but is there anyway to automate this?

Thanks",2
9409374,02/23/2012 08:22:21,1091061,12/10/2011 09:40:29,77,6,How can I speedtest my website?,"Are there any services (preferably free) which allow you to enter a URL then proceed to download the file from multiple locations and give you the speeds?

Searching has turned up untold numbers of speedtesters for my connection but for a remote website.

Thanks",performance,download,internet,,,,open,0,42,6,"How can I speedtest my website? Are there any services (preferably free) which allow you to enter a URL then proceed to download the file from multiple locations and give you the speeds?

Searching has turned up untold numbers of speedtesters for my connection but for a remote website.

Thanks",3
